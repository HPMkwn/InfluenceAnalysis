{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllCF_Sent_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllCF_Sent_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx-B1Nnp1Xm5",
        "outputId": "f542d5f9-940b-4ef9-d87a-2053054b0c4f"
      },
      "source": [
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7xUQbhdpm55",
        "outputId": "e85c1028-acfe-4e95-b4d7-d5e5bb74682b"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from gensim.models import Word2Vec\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import sentiwordnet as swn\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "print(device_name)\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/crowdFlower/cF_clean_our_combined.csv')\r\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "v-3YRgz91vOB",
        "outputId": "7d43637c-e8a8-4ca9-90b2-cb1348fdb25d"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35975</th>\n",
              "      <td>1</td>\n",
              "      <td>Praying for love in a lap dance and paying in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17590</th>\n",
              "      <td>2</td>\n",
              "      <td>trying to fix my internet connection      gues...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38071</th>\n",
              "      <td>0</td>\n",
              "      <td>starting an account here on twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23304</th>\n",
              "      <td>2</td>\n",
              "      <td>hmmm wrong link ignore my tweet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26990</th>\n",
              "      <td>0</td>\n",
              "      <td>thanks   before the major chop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion                                             tweets\n",
              "35975        1   Praying for love in a lap dance and paying in...\n",
              "17590        2  trying to fix my internet connection      gues...\n",
              "38071        0                starting an account here on twitter\n",
              "23304        2                    hmmm wrong link ignore my tweet\n",
              "26990        0                   thanks   before the major chop  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z18Mari1xuy",
        "outputId": "5608052e-abd4-40a3-e67e-445512aba3ad"
      },
      "source": [
        "df.emotion.unique()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NsmUUgQp1il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5b910b-7437-49cf-a1ee-116934564f0c"
      },
      "source": [
        "no_labels=4\r\n",
        "category_dict = {}\r\n",
        "for i in range(no_labels):\r\n",
        "  category_dict[i]=[0 for j in range(no_labels)]\r\n",
        "  category_dict[i][i]=1\r\n",
        "data_tweet = [str(x).lower().split() for x in df['tweets']]\r\n",
        "data_cat = np.array([np.array(category_dict[x]) for x in df['emotion']])\r\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMNiDUVVp5UK",
        "outputId": "77871b58-3707-45a3-83ea-24248a407bca"
      },
      "source": [
        "print(data_tweet[:5])\r\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['praying', 'for', 'love', 'in', 'a', 'lap', 'dance', 'and', 'paying', 'in', 'naivety', 'less', 'than', 'one', 'of', 'my', 'fav', 'songs'], ['trying', 'to', 'fix', 'my', 'internet', 'connection', 'guess', 'my', 'prayers', 'have', 'been', 'answered', 'and', 'i', 'wont', 'have', 'any', 'study', 'distractions', 'ugh'], ['starting', 'an', 'account', 'here', 'on', 'twitter'], ['hmmm', 'wrong', 'link', 'ignore', 'my', 'tweet'], ['thanks', 'before', 'the', 'major', 'chop']]\n",
            "[[0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q37vZ3dKp5SB"
      },
      "source": [
        "# Parameters\r\n",
        "Min_count = 0\r\n",
        "Embedding_size = 200\r\n",
        "Window_size = 5\r\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K-8Bji5p5OU"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\r\n",
        "                     window=Window_size,\r\n",
        "                     size=Embedding_size,\r\n",
        "                     negative=Negative_sampling,sg=1)\r\n",
        "w2v_sg.build_vocab(data_tweet)\r\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\r\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\r\n",
        "                     window=Window_size,\r\n",
        "                     size=Embedding_size,\r\n",
        "                     negative=Negative_sampling,sg=0)\r\n",
        "w2v_cbow.build_vocab(data_tweet)\r\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\r\n",
        "w2v_sg.wv.init_sims(True)\r\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs8MuQvjp5K6"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\r\n",
        "vocab_sg = [x for x in vocab_sg]\r\n",
        "vocab_cbow = w2v_cbow.wv.vocab\r\n",
        "vocab_cbow = [x for x in vocab_cbow] \r\n",
        "vocab_glove = {}\r\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\r\n",
        "  for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      vector = np.asarray(values[1:], \"float32\")\r\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaxTpKUQp1g0"
      },
      "source": [
        "batch_size = 500\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "data_tweet = [nltk.pos_tag(x) for x in data_tweet]\r\n",
        "def getsent(word,tag):\r\n",
        "  res=0\r\n",
        "  try:\r\n",
        "    x = swn.senti_synset(word+'.'+tag[0].lower()+'.01')\r\n",
        "    res =  (x.pos_score()-x.neg_score())\r\n",
        "  finally:\r\n",
        "    return res \r\n",
        "data_tweet = [[(i[0],i[1],getsent(i[0],i[1])) for i in x] for x in data_tweet]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo55tJBvp1eG"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][2]<0:\r\n",
        "            tweet[i]=((tweet[i][0]*(-1)),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = np.array(data_cat[k*(batch_size):(k+1)*(batch_size)])\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][2]<0:\r\n",
        "            tweet[i]=((tweet[i][0]*(-1)),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = np.array(data_cat[k*(batch_size):(k+1)*(batch_size)])\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2iIzjQg7J7g",
        "outputId": "6de4c029-dbb9-48f2-aa6a-c8e97bc4e05b"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\r\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\r\n",
        "sss.get_n_splits(data_tweet, data_cat)\r\n",
        "X_train=[]\r\n",
        "X_test=[]\r\n",
        "y_train=[]\r\n",
        "y_test=[]\r\n",
        "counter=0\r\n",
        "for train_index, test_index in sss.split(data_tweet, data_cat):\r\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\r\n",
        "  for i in train_index:\r\n",
        "    X_train+=[data_tweet[i]]\r\n",
        "    y_train+=[data_cat[i]]\r\n",
        "\r\n",
        "  for j in test_index:\r\n",
        "    X_test+=[data_tweet[j]]\r\n",
        "    y_test+=[data_cat[j]]\r\n",
        "\r\n",
        " # X_train, X_test = data_tweet[train_index], data_tweet[test_index]\r\n",
        " # y_train, y_test = data_cat[train_index], data_cat[test_index]\r\n",
        "print(X_train[:5])\r\n",
        "print(y_train[:5])\r\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\r\n",
        "sss.get_n_splits(X_train, y_train)\r\n",
        "x_train=[]\r\n",
        "X_val=[]\r\n",
        "Y_train=[]\r\n",
        "y_val=[]\r\n",
        "for train_index1, test_index1 in sss.split(X_train, y_train):\r\n",
        "  print(\"TRAIN:\", train_index1, \"TEST:\", test_index1)\r\n",
        "  for i in train_index1:\r\n",
        "    x_train+=[X_train[i]]\r\n",
        "    Y_train+=[y_train[i]]\r\n",
        "\r\n",
        "  for j in test_index1:\r\n",
        "    X_val+=[X_train[j]]\r\n",
        "    y_val+=[y_train[j]]\r\n",
        "X_train=x_train\r\n",
        "y_train=Y_train\r\n",
        "print(len(X_train))\r\n",
        "print(len(y_train))\r\n",
        "print(len(X_test))\r\n",
        "print(len(y_test))\r\n",
        "print(len(X_val))\r\n",
        "print(len(y_val))\r\n",
        "print(X_train[:5])\r\n",
        "print(y_train[:5])\r\n",
        "result_table = [0,0,0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [10470  6321 13813 ... 24250 39308  6334] TEST: [22946 37404 11668 ...  6796  7895 14585]\n",
            "[[('the', 'DT', 0), ('goonies', 'NNS', 0), ('projected', 'VBN', 0), ('on', 'IN', 0), ('a', 'DT', 0), ('garage', 'NN', 0.0), ('door', 'NN', 0.0), ('with', 'IN', 0), ('my', 'PRP$', 0), ('friends', 'NNS', 0), ('amazing', 'VBG', 0)], [('17', 'CD', 0), ('again', 'RB', 0.0), ('tonight', 'NN', 0.0)], [('we', 'PRP', 0), ('finally', 'RB', 0.0), ('hit', 'VBD', 0.0), ('1000', 'CD', 0), ('friends', 'NNS', 0), ('thanks', 'NNS', 0.125)], [('lolzz', 'JJ', 0), ('public', 'JJ', 0), ('script', 'NN', 0.0), ('wait', 'NN', 0.0), ('n', 'NN', 0.0), ('watch', 'NN', 0.0)], [('night', 'NN', 0.0)]]\n",
            "[array([0, 1, 0, 0]), array([1, 0, 0, 0]), array([0, 1, 0, 0]), array([0, 1, 0, 0]), array([1, 0, 0, 0])]\n",
            "TRAIN: [28318 14444 15946 ... 11043 21820 15291] TEST: [23742   966  6886 ... 25081 24593 26654]\n",
            "25600\n",
            "25600\n",
            "8000\n",
            "8000\n",
            "6400\n",
            "6400\n",
            "[[('geez', 'JJ', 0), ('15', 'CD', 0), ('years', 'NNS', 0.0), ('you', 'PRP', 0), ('d', 'VBP', 0), ('think', 'VBP', 0.0), ('they', 'PRP', 0), ('would', 'MD', 0), ('have', 'VB', 0.25), ('had', 'VBN', 0), ('that', 'IN', 0), ('checked', 'VBN', 0), ('out', 'RP', 0.0), ('years', 'NNS', 0.0), ('ago', 'RB', 0.0)], [('oh', 'NN', 0.0), ('that', 'WDT', 0), ('is', 'VBZ', 0), ('bad', 'JJ', 0), ('i', 'NN', 0.0), ('made', 'VBD', 0), ('that', 'IN', 0), ('mistake', 'NN', -0.625), ('once', 'RB', 0.0), ('before', 'RB', 0.0), ('but', 'CC', 0), ('still', 'RB', -0.125), ('managed', 'VBD', 0), ('to', 'TO', 0), ('pass', 'VB', 0.0), ('though', 'IN', 0), ('so', 'RB', 0.0), ('maybe', 'RB', 0.0), ('it', 'PRP', 0), ('is', 'VBZ', 0), ('not', 'RB', -0.625), ('as', 'RB', -0.125), ('bad', 'JJ', 0), ('as', 'IN', 0), ('he', 'PRP', 0), ('thinks', 'VBZ', 0)], [('leather', 'NN', 0.0), ('shoes', 'NNS', 0.0), ('and', 'CC', 0), ('bags', 'NNS', 0), ('are', 'VBP', 0), ('so', 'RB', 0.0), ('hard', 'JJ', 0), ('to', 'TO', 0), ('resist', 'VB', 0.0)], [('haha', 'NN', 0), ('my', 'PRP$', 0), ('rooommate', 'NN', 0), ('are', 'VBP', 0), ('watching', 'VBG', 0), ('that', 'IN', 0), ('i', 'NN', 0.0), ('have', 'VBP', 0.25), ('a', 'DT', 0), ('huge', 'JJ', 0), ('crush', 'NN', 0.0), ('on', 'IN', 0), ('landon', 'NN', 0)], [('i', 'NN', 0.0), ('should', 'MD', 0), ('post', 'VB', 0.0), ('some', 'DT', 0), ('photos', 'NN', 0), ('of', 'IN', 0), ('my', 'PRP$', 0), ('robots', 'NNS', 0), ('ok', 'VBP', 0), ('offline', 'NN', 0), ('for', 'IN', 0), ('4', 'CD', 0), ('hours', 'NNS', 0.0), ('now', 'RB', 0.0), ('ttfn', 'VBP', 0), ('oh', 'UH', 0), ('so', 'RB', 0.0), ('im', 'JJ', 0), ('confusion', 'NN', -0.375)]]\n",
            "[array([1, 0, 0, 0]), array([0, 0, 1, 0]), array([0, 1, 0, 0]), array([1, 0, 0, 0]), array([1, 0, 0, 0])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGcegqQC73VR"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N68btQXPp1YY"
      },
      "source": [
        "epochs = 30"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESbfjsopp1TI",
        "outputId": "055c8a01-f5fb-4082-8934-a1433a20144c"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "52/52 [==============================] - 71s 1s/step - loss: 1.2949 - accuracy: 0.3619 - f1: 0.0135 - precision: 0.1250 - recall: 0.0080 - val_loss: 1.1811 - val_accuracy: 0.4148 - val_f1: 0.0018 - val_precision: 0.3077 - val_recall: 9.2308e-04\n",
            "Epoch 2/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1760 - accuracy: 0.4264 - f1: 0.0964 - precision: 0.5676 - recall: 0.0553 - val_loss: 1.1514 - val_accuracy: 0.4423 - val_f1: 0.2065 - val_precision: 0.5895 - val_recall: 0.1254\n",
            "Epoch 3/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1424 - accuracy: 0.4594 - f1: 0.2324 - precision: 0.5881 - recall: 0.1458 - val_loss: 1.1402 - val_accuracy: 0.4589 - val_f1: 0.2900 - val_precision: 0.5729 - val_recall: 0.1943\n",
            "Epoch 4/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1279 - accuracy: 0.4721 - f1: 0.2707 - precision: 0.6052 - recall: 0.1754 - val_loss: 1.1309 - val_accuracy: 0.4665 - val_f1: 0.3169 - val_precision: 0.5853 - val_recall: 0.2174\n",
            "Epoch 5/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.1117 - accuracy: 0.4861 - f1: 0.2968 - precision: 0.6089 - recall: 0.1971 - val_loss: 1.1250 - val_accuracy: 0.4668 - val_f1: 0.3214 - val_precision: 0.5814 - val_recall: 0.2223\n",
            "Epoch 6/30\n",
            "52/52 [==============================] - 60s 1s/step - loss: 1.1077 - accuracy: 0.4845 - f1: 0.3072 - precision: 0.6079 - recall: 0.2063 - val_loss: 1.1294 - val_accuracy: 0.4634 - val_f1: 0.3265 - val_precision: 0.5813 - val_recall: 0.2272\n",
            "Epoch 7/30\n",
            "52/52 [==============================] - 60s 1s/step - loss: 1.0981 - accuracy: 0.4960 - f1: 0.3208 - precision: 0.6130 - recall: 0.2183 - val_loss: 1.1227 - val_accuracy: 0.4643 - val_f1: 0.3309 - val_precision: 0.5753 - val_recall: 0.2325\n",
            "Epoch 8/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0897 - accuracy: 0.4992 - f1: 0.3336 - precision: 0.6153 - recall: 0.2298 - val_loss: 1.1189 - val_accuracy: 0.4711 - val_f1: 0.3433 - val_precision: 0.5811 - val_recall: 0.2438\n",
            "Epoch 9/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0813 - accuracy: 0.5058 - f1: 0.3479 - precision: 0.6234 - recall: 0.2421 - val_loss: 1.1230 - val_accuracy: 0.4695 - val_f1: 0.3444 - val_precision: 0.5772 - val_recall: 0.2457\n",
            "Epoch 10/30\n",
            "52/52 [==============================] - 60s 1s/step - loss: 1.0726 - accuracy: 0.5076 - f1: 0.3578 - precision: 0.6291 - recall: 0.2509 - val_loss: 1.1108 - val_accuracy: 0.4766 - val_f1: 0.3430 - val_precision: 0.5911 - val_recall: 0.2418\n",
            "Epoch 11/30\n",
            "52/52 [==============================] - 65s 1s/step - loss: 1.0642 - accuracy: 0.5169 - f1: 0.3685 - precision: 0.6353 - recall: 0.2604 - val_loss: 1.1146 - val_accuracy: 0.4752 - val_f1: 0.3488 - val_precision: 0.5804 - val_recall: 0.2495\n",
            "Epoch 12/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0562 - accuracy: 0.5189 - f1: 0.3747 - precision: 0.6361 - recall: 0.2665 - val_loss: 1.1141 - val_accuracy: 0.4754 - val_f1: 0.3367 - val_precision: 0.5869 - val_recall: 0.2363\n",
            "Epoch 13/30\n",
            "52/52 [==============================] - 63s 1s/step - loss: 1.0538 - accuracy: 0.5222 - f1: 0.3827 - precision: 0.6573 - recall: 0.2710 - val_loss: 1.1131 - val_accuracy: 0.4765 - val_f1: 0.3464 - val_precision: 0.5784 - val_recall: 0.2475\n",
            "Epoch 14/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0443 - accuracy: 0.5260 - f1: 0.3940 - precision: 0.6529 - recall: 0.2829 - val_loss: 1.1134 - val_accuracy: 0.4751 - val_f1: 0.3541 - val_precision: 0.5691 - val_recall: 0.2572\n",
            "Epoch 15/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0325 - accuracy: 0.5356 - f1: 0.4095 - precision: 0.6593 - recall: 0.2981 - val_loss: 1.1151 - val_accuracy: 0.4777 - val_f1: 0.3486 - val_precision: 0.5833 - val_recall: 0.2488\n",
            "Epoch 16/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0196 - accuracy: 0.5434 - f1: 0.4209 - precision: 0.6607 - recall: 0.3097 - val_loss: 1.1104 - val_accuracy: 0.4785 - val_f1: 0.3609 - val_precision: 0.5834 - val_recall: 0.2614\n",
            "Epoch 17/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0129 - accuracy: 0.5483 - f1: 0.4343 - precision: 0.6725 - recall: 0.3219 - val_loss: 1.1086 - val_accuracy: 0.4789 - val_f1: 0.3616 - val_precision: 0.5774 - val_recall: 0.2634\n",
            "Epoch 18/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0045 - accuracy: 0.5542 - f1: 0.4485 - precision: 0.6806 - recall: 0.3354 - val_loss: 1.1129 - val_accuracy: 0.4780 - val_f1: 0.3683 - val_precision: 0.5695 - val_recall: 0.2723\n",
            "Epoch 19/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9906 - accuracy: 0.5649 - f1: 0.4640 - precision: 0.6830 - recall: 0.3521 - val_loss: 1.1094 - val_accuracy: 0.4811 - val_f1: 0.3871 - val_precision: 0.5786 - val_recall: 0.2911\n",
            "Epoch 20/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9835 - accuracy: 0.5692 - f1: 0.4731 - precision: 0.6816 - recall: 0.3628 - val_loss: 1.1116 - val_accuracy: 0.4883 - val_f1: 0.4026 - val_precision: 0.5732 - val_recall: 0.3103\n",
            "Epoch 21/30\n",
            "52/52 [==============================] - 68s 1s/step - loss: 0.9753 - accuracy: 0.5745 - f1: 0.4849 - precision: 0.6815 - recall: 0.3768 - val_loss: 1.1181 - val_accuracy: 0.4900 - val_f1: 0.4054 - val_precision: 0.5751 - val_recall: 0.3132\n",
            "Epoch 22/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9610 - accuracy: 0.5819 - f1: 0.4979 - precision: 0.6878 - recall: 0.3905 - val_loss: 1.1169 - val_accuracy: 0.4966 - val_f1: 0.4217 - val_precision: 0.5767 - val_recall: 0.3325\n",
            "Epoch 23/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.9543 - accuracy: 0.5878 - f1: 0.5047 - precision: 0.6844 - recall: 0.4001 - val_loss: 1.1274 - val_accuracy: 0.4968 - val_f1: 0.4272 - val_precision: 0.5739 - val_recall: 0.3403\n",
            "Epoch 24/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9488 - accuracy: 0.5900 - f1: 0.5150 - precision: 0.6851 - recall: 0.4128 - val_loss: 1.1220 - val_accuracy: 0.4995 - val_f1: 0.4280 - val_precision: 0.5735 - val_recall: 0.3415\n",
            "Epoch 25/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9326 - accuracy: 0.5980 - f1: 0.5286 - precision: 0.6954 - recall: 0.4266 - val_loss: 1.1312 - val_accuracy: 0.4972 - val_f1: 0.4331 - val_precision: 0.5716 - val_recall: 0.3488\n",
            "Epoch 26/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9175 - accuracy: 0.6086 - f1: 0.5416 - precision: 0.7021 - recall: 0.4411 - val_loss: 1.1355 - val_accuracy: 0.4994 - val_f1: 0.4348 - val_precision: 0.5694 - val_recall: 0.3518\n",
            "Epoch 27/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9085 - accuracy: 0.6123 - f1: 0.5497 - precision: 0.7056 - recall: 0.4506 - val_loss: 1.1444 - val_accuracy: 0.4934 - val_f1: 0.4266 - val_precision: 0.5682 - val_recall: 0.3417\n",
            "Epoch 28/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9017 - accuracy: 0.6140 - f1: 0.5549 - precision: 0.7084 - recall: 0.4565 - val_loss: 1.1361 - val_accuracy: 0.5002 - val_f1: 0.4398 - val_precision: 0.5688 - val_recall: 0.3586\n",
            "Epoch 29/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.8868 - accuracy: 0.6243 - f1: 0.5698 - precision: 0.7180 - recall: 0.4727 - val_loss: 1.1489 - val_accuracy: 0.5008 - val_f1: 0.4402 - val_precision: 0.5620 - val_recall: 0.3620\n",
            "Epoch 30/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.8786 - accuracy: 0.6289 - f1: 0.5743 - precision: 0.7187 - recall: 0.4787 - val_loss: 1.1606 - val_accuracy: 0.4975 - val_f1: 0.4359 - val_precision: 0.5531 - val_recall: 0.3598\n",
            "16/16 [==============================] - 11s 696ms/step - loss: 1.1330 - accuracy: 0.5222 - f1: 0.4629 - precision: 0.5898 - recall: 0.3810\n",
            "[1.1330294609069824, 0.5222499966621399, 0.46285539865493774, 0.5897719860076904, 0.38100001215934753]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix4waUDRqQ-s",
        "outputId": "6fbbe755-2de9-4286-effa-636b5ac81ecf"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "52/52 [==============================] - 67s 1s/step - loss: 1.2994 - accuracy: 0.3379 - f1: 0.0087 - precision: 0.1122 - recall: 0.0047 - val_loss: 1.1839 - val_accuracy: 0.4118 - val_f1: 6.1416e-04 - val_precision: 0.1538 - val_recall: 3.0769e-04\n",
            "Epoch 2/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1824 - accuracy: 0.4159 - f1: 0.0595 - precision: 0.6048 - recall: 0.0335 - val_loss: 1.1549 - val_accuracy: 0.4380 - val_f1: 0.1920 - val_precision: 0.5876 - val_recall: 0.1148\n",
            "Epoch 3/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1459 - accuracy: 0.4521 - f1: 0.2079 - precision: 0.6120 - recall: 0.1267 - val_loss: 1.1427 - val_accuracy: 0.4515 - val_f1: 0.2774 - val_precision: 0.5750 - val_recall: 0.1829\n",
            "Epoch 4/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1303 - accuracy: 0.4700 - f1: 0.2466 - precision: 0.5967 - recall: 0.1566 - val_loss: 1.1283 - val_accuracy: 0.4686 - val_f1: 0.3166 - val_precision: 0.5820 - val_recall: 0.2177\n",
            "Epoch 5/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.1141 - accuracy: 0.4780 - f1: 0.2848 - precision: 0.6025 - recall: 0.1874 - val_loss: 1.1266 - val_accuracy: 0.4657 - val_f1: 0.3231 - val_precision: 0.5759 - val_recall: 0.2248\n",
            "Epoch 6/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.1049 - accuracy: 0.4839 - f1: 0.3050 - precision: 0.6060 - recall: 0.2045 - val_loss: 1.1277 - val_accuracy: 0.4632 - val_f1: 0.3263 - val_precision: 0.5759 - val_recall: 0.2278\n",
            "Epoch 7/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0973 - accuracy: 0.4937 - f1: 0.3231 - precision: 0.6144 - recall: 0.2198 - val_loss: 1.1234 - val_accuracy: 0.4712 - val_f1: 0.3433 - val_precision: 0.5671 - val_recall: 0.2465\n",
            "Epoch 8/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0901 - accuracy: 0.5001 - f1: 0.3352 - precision: 0.6122 - recall: 0.2319 - val_loss: 1.1336 - val_accuracy: 0.4655 - val_f1: 0.3317 - val_precision: 0.5675 - val_recall: 0.2346\n",
            "Epoch 9/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0853 - accuracy: 0.5031 - f1: 0.3440 - precision: 0.6230 - recall: 0.2383 - val_loss: 1.1315 - val_accuracy: 0.4685 - val_f1: 0.3401 - val_precision: 0.5622 - val_recall: 0.2440\n",
            "Epoch 10/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0762 - accuracy: 0.5035 - f1: 0.3534 - precision: 0.6296 - recall: 0.2464 - val_loss: 1.1147 - val_accuracy: 0.4785 - val_f1: 0.3417 - val_precision: 0.5762 - val_recall: 0.2432\n",
            "Epoch 11/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0660 - accuracy: 0.5137 - f1: 0.3640 - precision: 0.6368 - recall: 0.2555 - val_loss: 1.1133 - val_accuracy: 0.4746 - val_f1: 0.3289 - val_precision: 0.5826 - val_recall: 0.2294\n",
            "Epoch 12/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0589 - accuracy: 0.5186 - f1: 0.3731 - precision: 0.6386 - recall: 0.2642 - val_loss: 1.1187 - val_accuracy: 0.4712 - val_f1: 0.3434 - val_precision: 0.5654 - val_recall: 0.2468\n",
            "Epoch 13/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0529 - accuracy: 0.5224 - f1: 0.3823 - precision: 0.6415 - recall: 0.2729 - val_loss: 1.1211 - val_accuracy: 0.4703 - val_f1: 0.3365 - val_precision: 0.5577 - val_recall: 0.2411\n",
            "Epoch 14/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0446 - accuracy: 0.5283 - f1: 0.3946 - precision: 0.6452 - recall: 0.2851 - val_loss: 1.1169 - val_accuracy: 0.4751 - val_f1: 0.3362 - val_precision: 0.5687 - val_recall: 0.2388\n",
            "Epoch 15/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0336 - accuracy: 0.5345 - f1: 0.4014 - precision: 0.6523 - recall: 0.2910 - val_loss: 1.1291 - val_accuracy: 0.4712 - val_f1: 0.3433 - val_precision: 0.5579 - val_recall: 0.2480\n",
            "Epoch 16/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0250 - accuracy: 0.5399 - f1: 0.4151 - precision: 0.6565 - recall: 0.3046 - val_loss: 1.1194 - val_accuracy: 0.4760 - val_f1: 0.3559 - val_precision: 0.5668 - val_recall: 0.2595\n",
            "Epoch 17/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0170 - accuracy: 0.5464 - f1: 0.4271 - precision: 0.6576 - recall: 0.3170 - val_loss: 1.1219 - val_accuracy: 0.4754 - val_f1: 0.3517 - val_precision: 0.5680 - val_recall: 0.2548\n",
            "Epoch 18/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 1.0086 - accuracy: 0.5538 - f1: 0.4392 - precision: 0.6597 - recall: 0.3298 - val_loss: 1.1271 - val_accuracy: 0.4752 - val_f1: 0.3645 - val_precision: 0.5597 - val_recall: 0.2703\n",
            "Epoch 19/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9951 - accuracy: 0.5604 - f1: 0.4621 - precision: 0.6736 - recall: 0.3522 - val_loss: 1.1351 - val_accuracy: 0.4775 - val_f1: 0.3806 - val_precision: 0.5546 - val_recall: 0.2898\n",
            "Epoch 20/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.9888 - accuracy: 0.5680 - f1: 0.4735 - precision: 0.6744 - recall: 0.3651 - val_loss: 1.1371 - val_accuracy: 0.4829 - val_f1: 0.3865 - val_precision: 0.5522 - val_recall: 0.2974\n",
            "Epoch 21/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9839 - accuracy: 0.5759 - f1: 0.4806 - precision: 0.6736 - recall: 0.3739 - val_loss: 1.1432 - val_accuracy: 0.4892 - val_f1: 0.3950 - val_precision: 0.5544 - val_recall: 0.3071\n",
            "Epoch 22/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.9693 - accuracy: 0.5775 - f1: 0.4949 - precision: 0.6798 - recall: 0.3893 - val_loss: 1.1374 - val_accuracy: 0.4966 - val_f1: 0.4093 - val_precision: 0.5575 - val_recall: 0.3235\n",
            "Epoch 23/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.9656 - accuracy: 0.5787 - f1: 0.4976 - precision: 0.6782 - recall: 0.3933 - val_loss: 1.1535 - val_accuracy: 0.5015 - val_f1: 0.4231 - val_precision: 0.5571 - val_recall: 0.3412\n",
            "Epoch 24/30\n",
            "52/52 [==============================] - 63s 1s/step - loss: 0.9548 - accuracy: 0.5884 - f1: 0.5127 - precision: 0.6828 - recall: 0.4108 - val_loss: 1.1541 - val_accuracy: 0.4932 - val_f1: 0.4156 - val_precision: 0.5546 - val_recall: 0.3325\n",
            "Epoch 25/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.9598 - accuracy: 0.5837 - f1: 0.5035 - precision: 0.6779 - recall: 0.4011 - val_loss: 1.1551 - val_accuracy: 0.4980 - val_f1: 0.4358 - val_precision: 0.5518 - val_recall: 0.3603\n",
            "Epoch 26/30\n",
            "52/52 [==============================] - 65s 1s/step - loss: 0.9301 - accuracy: 0.6031 - f1: 0.5389 - precision: 0.6930 - recall: 0.4412 - val_loss: 1.1573 - val_accuracy: 0.5025 - val_f1: 0.4354 - val_precision: 0.5502 - val_recall: 0.3603\n",
            "Epoch 27/30\n",
            "52/52 [==============================] - 63s 1s/step - loss: 0.9237 - accuracy: 0.6047 - f1: 0.5399 - precision: 0.6946 - recall: 0.4417 - val_loss: 1.1489 - val_accuracy: 0.4974 - val_f1: 0.4284 - val_precision: 0.5553 - val_recall: 0.3489\n",
            "Epoch 28/30\n",
            "52/52 [==============================] - 63s 1s/step - loss: 0.9102 - accuracy: 0.6123 - f1: 0.5534 - precision: 0.7095 - recall: 0.4540 - val_loss: 1.1600 - val_accuracy: 0.4954 - val_f1: 0.4337 - val_precision: 0.5484 - val_recall: 0.3588\n",
            "Epoch 29/30\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.8926 - accuracy: 0.6222 - f1: 0.5619 - precision: 0.7070 - recall: 0.4665 - val_loss: 1.1747 - val_accuracy: 0.5000 - val_f1: 0.4421 - val_precision: 0.5524 - val_recall: 0.3686\n",
            "Epoch 30/30\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.8901 - accuracy: 0.6265 - f1: 0.5754 - precision: 0.7180 - recall: 0.4804 - val_loss: 1.1793 - val_accuracy: 0.4957 - val_f1: 0.4424 - val_precision: 0.5439 - val_recall: 0.3729\n",
            "16/16 [==============================] - 11s 707ms/step - loss: 1.1618 - accuracy: 0.5066 - f1: 0.4609 - precision: 0.5694 - recall: 0.3872\n",
            "[1.1618074178695679, 0.5066249966621399, 0.4609472155570984, 0.569401741027832, 0.3872499167919159]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rYX8zohqQ2h",
        "outputId": "95ed6f5d-5692-4622-9194-164be5735dae"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[0]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "52/52 [==============================] - 34s 592ms/step - loss: 1.2424 - accuracy: 0.4041 - f1: 0.0843 - precision: 0.3122 - recall: 0.0503 - val_loss: 1.0921 - val_accuracy: 0.5146 - val_f1: 0.3679 - val_precision: 0.6114 - val_recall: 0.2632\n",
            "Epoch 2/30\n",
            "52/52 [==============================] - 29s 566ms/step - loss: 1.0849 - accuracy: 0.5260 - f1: 0.3903 - precision: 0.6253 - recall: 0.2846 - val_loss: 1.0466 - val_accuracy: 0.5423 - val_f1: 0.4420 - val_precision: 0.6399 - val_recall: 0.3377\n",
            "Epoch 3/30\n",
            "52/52 [==============================] - 29s 567ms/step - loss: 1.0386 - accuracy: 0.5572 - f1: 0.4503 - precision: 0.6524 - recall: 0.3444 - val_loss: 1.0297 - val_accuracy: 0.5518 - val_f1: 0.4816 - val_precision: 0.6394 - val_recall: 0.3866\n",
            "Epoch 4/30\n",
            "52/52 [==============================] - 34s 658ms/step - loss: 1.0039 - accuracy: 0.5693 - f1: 0.4942 - precision: 0.6696 - recall: 0.3922 - val_loss: 1.0080 - val_accuracy: 0.5608 - val_f1: 0.5027 - val_precision: 0.6422 - val_recall: 0.4132\n",
            "Epoch 5/30\n",
            "52/52 [==============================] - 33s 618ms/step - loss: 0.9839 - accuracy: 0.5868 - f1: 0.5039 - precision: 0.6709 - recall: 0.4040 - val_loss: 1.0053 - val_accuracy: 0.5626 - val_f1: 0.4968 - val_precision: 0.6506 - val_recall: 0.4020\n",
            "Epoch 6/30\n",
            "52/52 [==============================] - 29s 566ms/step - loss: 0.9652 - accuracy: 0.5903 - f1: 0.5245 - precision: 0.6815 - recall: 0.4268 - val_loss: 1.0044 - val_accuracy: 0.5643 - val_f1: 0.4970 - val_precision: 0.6549 - val_recall: 0.4006\n",
            "Epoch 7/30\n",
            "52/52 [==============================] - 29s 567ms/step - loss: 0.9481 - accuracy: 0.5995 - f1: 0.5376 - precision: 0.6855 - recall: 0.4426 - val_loss: 1.0046 - val_accuracy: 0.5669 - val_f1: 0.5032 - val_precision: 0.6415 - val_recall: 0.4142\n",
            "Epoch 8/30\n",
            "52/52 [==============================] - 30s 570ms/step - loss: 0.9278 - accuracy: 0.6076 - f1: 0.5475 - precision: 0.6929 - recall: 0.4530 - val_loss: 1.0014 - val_accuracy: 0.5635 - val_f1: 0.5012 - val_precision: 0.6502 - val_recall: 0.4080\n",
            "Epoch 9/30\n",
            "52/52 [==============================] - 29s 563ms/step - loss: 0.9078 - accuracy: 0.6174 - f1: 0.5641 - precision: 0.7026 - recall: 0.4717 - val_loss: 1.0139 - val_accuracy: 0.5600 - val_f1: 0.4995 - val_precision: 0.6397 - val_recall: 0.4098\n",
            "Epoch 10/30\n",
            "52/52 [==============================] - 29s 565ms/step - loss: 0.8906 - accuracy: 0.6271 - f1: 0.5778 - precision: 0.7091 - recall: 0.4880 - val_loss: 1.0163 - val_accuracy: 0.5625 - val_f1: 0.5233 - val_precision: 0.6245 - val_recall: 0.4505\n",
            "Epoch 11/30\n",
            "52/52 [==============================] - 29s 565ms/step - loss: 0.8751 - accuracy: 0.6365 - f1: 0.5928 - precision: 0.7092 - recall: 0.5095 - val_loss: 1.0296 - val_accuracy: 0.5552 - val_f1: 0.5203 - val_precision: 0.6102 - val_recall: 0.4537\n",
            "Epoch 12/30\n",
            "52/52 [==============================] - 29s 562ms/step - loss: 0.8623 - accuracy: 0.6425 - f1: 0.6038 - precision: 0.7154 - recall: 0.5227 - val_loss: 1.0494 - val_accuracy: 0.5480 - val_f1: 0.5171 - val_precision: 0.6004 - val_recall: 0.4543\n",
            "Epoch 13/30\n",
            "52/52 [==============================] - 29s 561ms/step - loss: 0.8464 - accuracy: 0.6530 - f1: 0.6147 - precision: 0.7198 - recall: 0.5369 - val_loss: 1.0487 - val_accuracy: 0.5455 - val_f1: 0.5084 - val_precision: 0.6014 - val_recall: 0.4405\n",
            "Epoch 14/30\n",
            "52/52 [==============================] - 29s 562ms/step - loss: 0.8281 - accuracy: 0.6600 - f1: 0.6241 - precision: 0.7304 - recall: 0.5450 - val_loss: 1.0635 - val_accuracy: 0.5438 - val_f1: 0.5140 - val_precision: 0.5996 - val_recall: 0.4498\n",
            "Epoch 15/30\n",
            "52/52 [==============================] - 29s 561ms/step - loss: 0.8067 - accuracy: 0.6694 - f1: 0.6398 - precision: 0.7388 - recall: 0.5644 - val_loss: 1.0937 - val_accuracy: 0.5489 - val_f1: 0.5245 - val_precision: 0.5954 - val_recall: 0.4688\n",
            "Epoch 16/30\n",
            "52/52 [==============================] - 29s 563ms/step - loss: 0.7873 - accuracy: 0.6842 - f1: 0.6578 - precision: 0.7480 - recall: 0.5873 - val_loss: 1.1120 - val_accuracy: 0.5449 - val_f1: 0.5225 - val_precision: 0.5845 - val_recall: 0.4726\n",
            "Epoch 17/30\n",
            "52/52 [==============================] - 29s 561ms/step - loss: 0.7608 - accuracy: 0.6925 - f1: 0.6672 - precision: 0.7512 - recall: 0.6003 - val_loss: 1.1326 - val_accuracy: 0.5466 - val_f1: 0.5258 - val_precision: 0.5820 - val_recall: 0.4795\n",
            "Epoch 18/30\n",
            "52/52 [==============================] - 29s 562ms/step - loss: 0.7501 - accuracy: 0.6959 - f1: 0.6773 - precision: 0.7582 - recall: 0.6123 - val_loss: 1.1533 - val_accuracy: 0.5438 - val_f1: 0.5202 - val_precision: 0.5794 - val_recall: 0.4720\n",
            "Epoch 19/30\n",
            "52/52 [==============================] - 29s 568ms/step - loss: 0.7342 - accuracy: 0.7069 - f1: 0.6875 - precision: 0.7688 - recall: 0.6220 - val_loss: 1.2109 - val_accuracy: 0.5358 - val_f1: 0.5165 - val_precision: 0.5691 - val_recall: 0.4729\n",
            "Epoch 20/30\n",
            "52/52 [==============================] - 29s 562ms/step - loss: 0.7297 - accuracy: 0.7104 - f1: 0.6857 - precision: 0.7704 - recall: 0.6183 - val_loss: 1.2322 - val_accuracy: 0.5398 - val_f1: 0.5240 - val_precision: 0.5678 - val_recall: 0.4865\n",
            "Epoch 21/30\n",
            "52/52 [==============================] - 29s 564ms/step - loss: 0.7080 - accuracy: 0.7154 - f1: 0.6977 - precision: 0.7731 - recall: 0.6360 - val_loss: 1.2280 - val_accuracy: 0.5351 - val_f1: 0.5171 - val_precision: 0.5619 - val_recall: 0.4789\n",
            "Epoch 22/30\n",
            "52/52 [==============================] - 29s 562ms/step - loss: 0.7137 - accuracy: 0.7107 - f1: 0.6971 - precision: 0.7713 - recall: 0.6364 - val_loss: 1.2304 - val_accuracy: 0.5460 - val_f1: 0.5271 - val_precision: 0.5728 - val_recall: 0.4882\n",
            "Epoch 23/30\n",
            "52/52 [==============================] - 29s 560ms/step - loss: 0.6722 - accuracy: 0.7300 - f1: 0.7157 - precision: 0.7830 - recall: 0.6593 - val_loss: 1.2842 - val_accuracy: 0.5462 - val_f1: 0.5269 - val_precision: 0.5694 - val_recall: 0.4903\n",
            "Epoch 24/30\n",
            "52/52 [==============================] - 29s 562ms/step - loss: 0.6548 - accuracy: 0.7360 - f1: 0.7220 - precision: 0.7879 - recall: 0.6666 - val_loss: 1.3216 - val_accuracy: 0.5309 - val_f1: 0.5147 - val_precision: 0.5600 - val_recall: 0.4763\n",
            "Epoch 25/30\n",
            "52/52 [==============================] - 29s 560ms/step - loss: 0.6470 - accuracy: 0.7393 - f1: 0.7285 - precision: 0.7951 - recall: 0.6725 - val_loss: 1.2752 - val_accuracy: 0.5280 - val_f1: 0.5088 - val_precision: 0.5674 - val_recall: 0.4612\n",
            "Epoch 26/30\n",
            "52/52 [==============================] - 29s 560ms/step - loss: 0.6456 - accuracy: 0.7395 - f1: 0.7260 - precision: 0.7967 - recall: 0.6673 - val_loss: 1.2577 - val_accuracy: 0.5400 - val_f1: 0.5211 - val_precision: 0.5736 - val_recall: 0.4775\n",
            "Epoch 27/30\n",
            "52/52 [==============================] - 29s 558ms/step - loss: 0.6501 - accuracy: 0.7413 - f1: 0.7237 - precision: 0.7958 - recall: 0.6642 - val_loss: 1.3182 - val_accuracy: 0.5323 - val_f1: 0.5157 - val_precision: 0.5590 - val_recall: 0.4786\n",
            "Epoch 28/30\n",
            "52/52 [==============================] - 29s 558ms/step - loss: 0.6478 - accuracy: 0.7400 - f1: 0.7286 - precision: 0.7911 - recall: 0.6760 - val_loss: 1.2678 - val_accuracy: 0.5323 - val_f1: 0.5148 - val_precision: 0.5659 - val_recall: 0.4723\n",
            "Epoch 29/30\n",
            "52/52 [==============================] - 29s 561ms/step - loss: 0.6188 - accuracy: 0.7566 - f1: 0.7475 - precision: 0.8100 - recall: 0.6944 - val_loss: 1.2869 - val_accuracy: 0.5334 - val_f1: 0.5140 - val_precision: 0.5625 - val_recall: 0.4732\n",
            "Epoch 30/30\n",
            "52/52 [==============================] - 29s 557ms/step - loss: 0.6040 - accuracy: 0.7659 - f1: 0.7548 - precision: 0.8156 - recall: 0.7028 - val_loss: 1.3454 - val_accuracy: 0.5315 - val_f1: 0.5148 - val_precision: 0.5567 - val_recall: 0.4788\n",
            "16/16 [==============================] - 4s 243ms/step - loss: 1.3140 - accuracy: 0.5371 - f1: 0.5205 - precision: 0.5632 - recall: 0.4839\n",
            "[1.3139997720718384, 0.5371249914169312, 0.5204859375953674, 0.5631881356239319, 0.48387500643730164]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51aAWN2_qjv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfc90ca-cd22-490d-a364-28a8ecf2f39c"
      },
      "source": [
        "print(\"\\tBiLstm\")\r\n",
        "print(\"glove\",result_table[:1])\r\n",
        "print(\"sg\",result_table[1:2])\r\n",
        "print(\"cbow\",result_table[2:3])\r\n",
        "# print(history_sg_bi.history)\r\n",
        "# plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()\r\n",
        "# plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tBiLstm\n",
            "glove [0.5371249914169312]\n",
            "sg [0.5222499966621399]\n",
            "cbow [0.5066249966621399]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}