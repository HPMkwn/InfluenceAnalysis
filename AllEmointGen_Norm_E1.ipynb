{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointGen_Norm_E1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointGen_Norm_E1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9351d3-4cd8-4c02-c25b-11800478fde1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Emoint/E1.csv')\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjQC2VxK0ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f544b5-8409-4a99-8962-38ccb45208be"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'tweets', 'emotion', 'intensity'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {0:[1,0,0,0],1:[0,1,0,0],2:[0,0,1,0],3:[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['tweets']]\n",
        "data_cat = np.array([category_dict[x] for x in df['emotion']])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62088922-1323-4825-d9b6-845d32b97c8a"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['when', 'my', 'life', 'became', 'such', 'a', 'concern', 'to', 'irrelevant', 'ass', 'people', 'ill', 'never', 'know'], ['in', 'other', 'news', 'my', 'legs', 'hurt', 'running', '5kinmins', 'flatfeet'], ['yo', 'yo', 'yomy', 'name', 'is', 'darthvader', 'ni', 'feel', 'like', 'i', 'need', 'to', 'puff', 'on', 'my', 'inhaler', 'i', 'am', 'no', 'rapper', 'but', 'that', 'was', 'some', 'sick', 'bars', 'breathless', 'bars', 'rap'], ['brb', 'going', 'to', 'sulk', 'in', 'bed', 'until', 'friday'], ['the', 'house', 'no', 'longer', 'echoes', 'with', 'our', 'laughter', 'we', 'light', 'up', 'our', 'memories', 'of', 'the', 'home', 'and', 'sit', 'in', 'the', 'cold', 'waiting', 'amwriting', 'nostalgia']]\n",
            "[[1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 200\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnPxfLf139kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a29066e-5a04-4ac7-f332-1d18289f0e1e"
      },
      "source": [
        "print(len(vocab_sg))\r\n",
        "print(len(vocab_cbow))\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16645\n",
            "16645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      temp = np.array([np.array([w2v.wv.get_vector(i) for i in x if i in vocab]) for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]])\r\n",
        "      temp = np.array([np.pad(x.flatten(),(0,Max_input_size*Embedding_size-len(x.flatten()))).reshape(Max_input_size,Embedding_size) for x in temp])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)\r\n",
        "\r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      temp = np.array([np.array([vocab[i] for i in x if i in vocab.keys()]) for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]])\r\n",
        "      temp = np.array([np.pad(x.flatten(),(0,Max_input_size*Embedding_size-len(x.flatten()))).reshape(Max_input_size,Embedding_size) for x in temp])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 60"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94w9X37VzLZZ"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66070a54-71ce-4a3c-e1a9-0cb95a071e94"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(64))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy',f1,precision,recall])\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 38s 120ms/step - loss: 1.3808 - accuracy: 0.3142 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3695 - val_accuracy: 0.3233 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 1.3558 - accuracy: 0.3296 - f1: 0.0373 - precision: 0.3926 - recall: 0.0200 - val_loss: 1.3565 - val_accuracy: 0.3442 - val_f1: 0.1508 - val_precision: 0.4801 - val_recall: 0.0908\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 104ms/step - loss: 1.3256 - accuracy: 0.3508 - f1: 0.1398 - precision: 0.6658 - recall: 0.0818 - val_loss: 1.3420 - val_accuracy: 0.3458 - val_f1: 0.1947 - val_precision: 0.5177 - val_recall: 0.1208\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 1.3045 - accuracy: 0.3734 - f1: 0.1636 - precision: 0.6927 - recall: 0.0961 - val_loss: 1.3355 - val_accuracy: 0.3717 - val_f1: 0.2091 - val_precision: 0.5494 - val_recall: 0.1300\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 1.2882 - accuracy: 0.3936 - f1: 0.1761 - precision: 0.6708 - recall: 0.1045 - val_loss: 1.3201 - val_accuracy: 0.3883 - val_f1: 0.1785 - val_precision: 0.6078 - val_recall: 0.1050\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2835 - accuracy: 0.3988 - f1: 0.1740 - precision: 0.6940 - recall: 0.1017 - val_loss: 1.3080 - val_accuracy: 0.3908 - val_f1: 0.1972 - val_precision: 0.6223 - val_recall: 0.1183\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 1.2633 - accuracy: 0.4114 - f1: 0.1882 - precision: 0.6882 - recall: 0.1118 - val_loss: 1.3017 - val_accuracy: 0.4058 - val_f1: 0.1588 - val_precision: 0.6241 - val_recall: 0.0917\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.2240 - accuracy: 0.4496 - f1: 0.2395 - precision: 0.7108 - recall: 0.1479 - val_loss: 1.2802 - val_accuracy: 0.4108 - val_f1: 0.2661 - val_precision: 0.6248 - val_recall: 0.1700\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.1822 - accuracy: 0.4666 - f1: 0.3248 - precision: 0.6801 - recall: 0.2153 - val_loss: 1.2938 - val_accuracy: 0.4217 - val_f1: 0.2784 - val_precision: 0.5944 - val_recall: 0.1825\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.1711 - accuracy: 0.4712 - f1: 0.3460 - precision: 0.7010 - recall: 0.2338 - val_loss: 1.2715 - val_accuracy: 0.4292 - val_f1: 0.2643 - val_precision: 0.6560 - val_recall: 0.1667\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.1272 - accuracy: 0.4985 - f1: 0.3665 - precision: 0.7153 - recall: 0.2491 - val_loss: 1.2433 - val_accuracy: 0.4458 - val_f1: 0.3010 - val_precision: 0.6354 - val_recall: 0.1983\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.1041 - accuracy: 0.5182 - f1: 0.3867 - precision: 0.7134 - recall: 0.2671 - val_loss: 1.2740 - val_accuracy: 0.4442 - val_f1: 0.3320 - val_precision: 0.6339 - val_recall: 0.2258\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 1.0802 - accuracy: 0.5208 - f1: 0.3994 - precision: 0.7207 - recall: 0.2778 - val_loss: 1.3209 - val_accuracy: 0.4417 - val_f1: 0.3391 - val_precision: 0.6292 - val_recall: 0.2333\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 1.0622 - accuracy: 0.5315 - f1: 0.4180 - precision: 0.7363 - recall: 0.2930 - val_loss: 1.3689 - val_accuracy: 0.4408 - val_f1: 0.3701 - val_precision: 0.5997 - val_recall: 0.2683\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 1.0345 - accuracy: 0.5465 - f1: 0.4372 - precision: 0.7478 - recall: 0.3105 - val_loss: 1.3809 - val_accuracy: 0.4575 - val_f1: 0.3895 - val_precision: 0.5817 - val_recall: 0.2933\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 1.0077 - accuracy: 0.5506 - f1: 0.4575 - precision: 0.7774 - recall: 0.3249 - val_loss: 1.3864 - val_accuracy: 0.4550 - val_f1: 0.3830 - val_precision: 0.6208 - val_recall: 0.2775\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.9643 - accuracy: 0.5683 - f1: 0.4744 - precision: 0.7801 - recall: 0.3421 - val_loss: 1.3433 - val_accuracy: 0.4658 - val_f1: 0.4124 - val_precision: 0.6513 - val_recall: 0.3025\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.9314 - accuracy: 0.5852 - f1: 0.4979 - precision: 0.8023 - recall: 0.3625 - val_loss: 1.3443 - val_accuracy: 0.4650 - val_f1: 0.4080 - val_precision: 0.6455 - val_recall: 0.3000\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.9328 - accuracy: 0.5849 - f1: 0.5062 - precision: 0.8130 - recall: 0.3691 - val_loss: 1.4045 - val_accuracy: 0.4642 - val_f1: 0.3998 - val_precision: 0.6287 - val_recall: 0.2942\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.9197 - accuracy: 0.5877 - f1: 0.5118 - precision: 0.8097 - recall: 0.3767 - val_loss: 1.4057 - val_accuracy: 0.4867 - val_f1: 0.4171 - val_precision: 0.6136 - val_recall: 0.3167\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.9179 - accuracy: 0.5878 - f1: 0.4988 - precision: 0.8152 - recall: 0.3623 - val_loss: 1.5188 - val_accuracy: 0.4858 - val_f1: 0.4530 - val_precision: 0.6273 - val_recall: 0.3550\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.8978 - accuracy: 0.6052 - f1: 0.5253 - precision: 0.8208 - recall: 0.3912 - val_loss: 1.5429 - val_accuracy: 0.4708 - val_f1: 0.4394 - val_precision: 0.5706 - val_recall: 0.3575\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.8807 - accuracy: 0.6000 - f1: 0.5188 - precision: 0.8043 - recall: 0.3898 - val_loss: 1.5585 - val_accuracy: 0.4892 - val_f1: 0.4716 - val_precision: 0.5671 - val_recall: 0.4042\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.8622 - accuracy: 0.6122 - f1: 0.5500 - precision: 0.8032 - recall: 0.4245 - val_loss: 1.4857 - val_accuracy: 0.4967 - val_f1: 0.4700 - val_precision: 0.5907 - val_recall: 0.3908\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.7986 - accuracy: 0.6451 - f1: 0.5801 - precision: 0.8241 - recall: 0.4511 - val_loss: 1.5573 - val_accuracy: 0.5075 - val_f1: 0.4725 - val_precision: 0.5932 - val_recall: 0.3933\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.7523 - accuracy: 0.6561 - f1: 0.5910 - precision: 0.8249 - recall: 0.4632 - val_loss: 1.5811 - val_accuracy: 0.5192 - val_f1: 0.4852 - val_precision: 0.6064 - val_recall: 0.4050\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.7025 - accuracy: 0.6824 - f1: 0.6191 - precision: 0.8509 - recall: 0.4893 - val_loss: 1.5437 - val_accuracy: 0.5125 - val_f1: 0.4718 - val_precision: 0.6136 - val_recall: 0.3842\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.7225 - accuracy: 0.6721 - f1: 0.6295 - precision: 0.8396 - recall: 0.5059 - val_loss: 1.4539 - val_accuracy: 0.5217 - val_f1: 0.4666 - val_precision: 0.6521 - val_recall: 0.3642\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.6681 - accuracy: 0.6830 - f1: 0.6459 - precision: 0.8379 - recall: 0.5271 - val_loss: 1.4804 - val_accuracy: 0.5067 - val_f1: 0.4645 - val_precision: 0.6175 - val_recall: 0.3725\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.6513 - accuracy: 0.7140 - f1: 0.6688 - precision: 0.8335 - recall: 0.5595 - val_loss: 1.4114 - val_accuracy: 0.5275 - val_f1: 0.4761 - val_precision: 0.6537 - val_recall: 0.3750\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.6441 - accuracy: 0.7319 - f1: 0.6869 - precision: 0.8424 - recall: 0.5805 - val_loss: 1.4941 - val_accuracy: 0.5333 - val_f1: 0.5120 - val_precision: 0.6229 - val_recall: 0.4350\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.6003 - accuracy: 0.7509 - f1: 0.7201 - precision: 0.8336 - recall: 0.6347 - val_loss: 1.5185 - val_accuracy: 0.5533 - val_f1: 0.5298 - val_precision: 0.6157 - val_recall: 0.4658\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5806 - accuracy: 0.7479 - f1: 0.7237 - precision: 0.8204 - recall: 0.6480 - val_loss: 1.5350 - val_accuracy: 0.5475 - val_f1: 0.5234 - val_precision: 0.5979 - val_recall: 0.4658\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.6024 - accuracy: 0.7448 - f1: 0.7311 - precision: 0.8187 - recall: 0.6610 - val_loss: 1.3855 - val_accuracy: 0.5467 - val_f1: 0.5213 - val_precision: 0.5967 - val_recall: 0.4633\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.5771 - accuracy: 0.7529 - f1: 0.7373 - precision: 0.8127 - recall: 0.6753 - val_loss: 1.5136 - val_accuracy: 0.5467 - val_f1: 0.5355 - val_precision: 0.6057 - val_recall: 0.4800\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5248 - accuracy: 0.7756 - f1: 0.7554 - precision: 0.8215 - recall: 0.6999 - val_loss: 1.5298 - val_accuracy: 0.5333 - val_f1: 0.5305 - val_precision: 0.5799 - val_recall: 0.4892\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.4986 - accuracy: 0.7910 - f1: 0.7834 - precision: 0.8292 - recall: 0.7428 - val_loss: 1.5527 - val_accuracy: 0.5525 - val_f1: 0.5495 - val_precision: 0.5893 - val_recall: 0.5150\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5002 - accuracy: 0.7950 - f1: 0.7886 - precision: 0.8264 - recall: 0.7545 - val_loss: 1.6272 - val_accuracy: 0.5642 - val_f1: 0.5479 - val_precision: 0.5878 - val_recall: 0.5133\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5024 - accuracy: 0.7820 - f1: 0.7739 - precision: 0.8142 - recall: 0.7379 - val_loss: 1.6085 - val_accuracy: 0.5575 - val_f1: 0.5528 - val_precision: 0.5883 - val_recall: 0.5217\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5112 - accuracy: 0.7936 - f1: 0.7868 - precision: 0.8211 - recall: 0.7558 - val_loss: 1.5620 - val_accuracy: 0.5692 - val_f1: 0.5607 - val_precision: 0.5934 - val_recall: 0.5317\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.4825 - accuracy: 0.8010 - f1: 0.7969 - precision: 0.8231 - recall: 0.7727 - val_loss: 1.4544 - val_accuracy: 0.5467 - val_f1: 0.5401 - val_precision: 0.5766 - val_recall: 0.5083\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.5413 - accuracy: 0.7809 - f1: 0.7719 - precision: 0.8177 - recall: 0.7313 - val_loss: 1.4507 - val_accuracy: 0.5942 - val_f1: 0.5713 - val_precision: 0.6078 - val_recall: 0.5392\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.4835 - accuracy: 0.8125 - f1: 0.8107 - precision: 0.8475 - recall: 0.7774 - val_loss: 1.5471 - val_accuracy: 0.5525 - val_f1: 0.5454 - val_precision: 0.5696 - val_recall: 0.5233\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.4386 - accuracy: 0.8183 - f1: 0.8108 - precision: 0.8413 - recall: 0.7827 - val_loss: 1.5591 - val_accuracy: 0.5742 - val_f1: 0.5720 - val_precision: 0.5959 - val_recall: 0.5500\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.3789 - accuracy: 0.8565 - f1: 0.8534 - precision: 0.8725 - recall: 0.8353 - val_loss: 1.7930 - val_accuracy: 0.5867 - val_f1: 0.5801 - val_precision: 0.6099 - val_recall: 0.5533\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.3564 - accuracy: 0.8569 - f1: 0.8549 - precision: 0.8737 - recall: 0.8371 - val_loss: 1.7783 - val_accuracy: 0.5708 - val_f1: 0.5693 - val_precision: 0.5941 - val_recall: 0.5467\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3441 - accuracy: 0.8651 - f1: 0.8651 - precision: 0.8826 - recall: 0.8484 - val_loss: 1.6369 - val_accuracy: 0.5858 - val_f1: 0.5841 - val_precision: 0.6057 - val_recall: 0.5642\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.3469 - accuracy: 0.8624 - f1: 0.8627 - precision: 0.8794 - recall: 0.8468 - val_loss: 1.6492 - val_accuracy: 0.5758 - val_f1: 0.5700 - val_precision: 0.5936 - val_recall: 0.5483\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.5779 - accuracy: 0.7843 - f1: 0.7826 - precision: 0.8072 - recall: 0.7599 - val_loss: 1.6119 - val_accuracy: 0.5625 - val_f1: 0.5561 - val_precision: 0.6022 - val_recall: 0.5167\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.4506 - accuracy: 0.8189 - f1: 0.8157 - precision: 0.8525 - recall: 0.7825 - val_loss: 1.9869 - val_accuracy: 0.5892 - val_f1: 0.5815 - val_precision: 0.6040 - val_recall: 0.5608\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3782 - accuracy: 0.8565 - f1: 0.8542 - precision: 0.8726 - recall: 0.8367 - val_loss: 1.8316 - val_accuracy: 0.5883 - val_f1: 0.5837 - val_precision: 0.6047 - val_recall: 0.5642\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.3207 - accuracy: 0.8798 - f1: 0.8801 - precision: 0.8958 - recall: 0.8652 - val_loss: 2.0593 - val_accuracy: 0.5758 - val_f1: 0.5744 - val_precision: 0.5897 - val_recall: 0.5600\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.3055 - accuracy: 0.8905 - f1: 0.8889 - precision: 0.9017 - recall: 0.8766 - val_loss: 1.7523 - val_accuracy: 0.5892 - val_f1: 0.5858 - val_precision: 0.6057 - val_recall: 0.5675\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2847 - accuracy: 0.8963 - f1: 0.8970 - precision: 0.9080 - recall: 0.8864 - val_loss: 1.8981 - val_accuracy: 0.5850 - val_f1: 0.5802 - val_precision: 0.5975 - val_recall: 0.5642\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.2769 - accuracy: 0.8900 - f1: 0.8898 - precision: 0.8996 - recall: 0.8804 - val_loss: 2.0251 - val_accuracy: 0.5758 - val_f1: 0.5762 - val_precision: 0.5878 - val_recall: 0.5650\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.2887 - accuracy: 0.8902 - f1: 0.8921 - precision: 0.9018 - recall: 0.8826 - val_loss: 1.8072 - val_accuracy: 0.5792 - val_f1: 0.5796 - val_precision: 0.5934 - val_recall: 0.5667\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2951 - accuracy: 0.8893 - f1: 0.8891 - precision: 0.9020 - recall: 0.8766 - val_loss: 1.8015 - val_accuracy: 0.5708 - val_f1: 0.5731 - val_precision: 0.5879 - val_recall: 0.5592\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.3215 - accuracy: 0.8776 - f1: 0.8776 - precision: 0.8923 - recall: 0.8636 - val_loss: 1.8849 - val_accuracy: 0.5725 - val_f1: 0.5724 - val_precision: 0.5855 - val_recall: 0.5600\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2837 - accuracy: 0.8954 - f1: 0.8933 - precision: 0.9046 - recall: 0.8824 - val_loss: 1.8145 - val_accuracy: 0.5075 - val_f1: 0.5028 - val_precision: 0.5184 - val_recall: 0.4883\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.4302 - accuracy: 0.8475 - f1: 0.8419 - precision: 0.8641 - recall: 0.8211 - val_loss: 1.7025 - val_accuracy: 0.5708 - val_f1: 0.5758 - val_precision: 0.5908 - val_recall: 0.5617\n",
            "14/14 [==============================] - 1s 88ms/step - loss: 1.6528 - accuracy: 0.5950 - f1: 0.5930 - precision: 0.6100 - recall: 0.5771\n",
            "[1.6528249979019165, 0.5950000286102295, 0.5930396318435669, 0.6100398302078247, 0.5771428346633911]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d61cb76-eef5-4fa4-e5cd-ec2a63a3a93f"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 121ms/step - loss: 1.3782 - accuracy: 0.3083 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3606 - val_accuracy: 0.3408 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 1.3587 - accuracy: 0.3208 - f1: 0.0085 - precision: 0.0865 - recall: 0.0049 - val_loss: 1.3309 - val_accuracy: 0.3558 - val_f1: 0.0115 - val_precision: 0.3750 - val_recall: 0.0058\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 1.3007 - accuracy: 0.3929 - f1: 0.0811 - precision: 0.5353 - recall: 0.0470 - val_loss: 1.2911 - val_accuracy: 0.4108 - val_f1: 0.1007 - val_precision: 0.6425 - val_recall: 0.0550\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 1.2317 - accuracy: 0.4653 - f1: 0.1947 - precision: 0.6929 - recall: 0.1168 - val_loss: 1.2686 - val_accuracy: 0.4350 - val_f1: 0.1602 - val_precision: 0.6358 - val_recall: 0.0925\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.1662 - accuracy: 0.4952 - f1: 0.3104 - precision: 0.6969 - recall: 0.2039 - val_loss: 1.2324 - val_accuracy: 0.4500 - val_f1: 0.2427 - val_precision: 0.6299 - val_recall: 0.1508\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.1207 - accuracy: 0.5270 - f1: 0.3785 - precision: 0.6823 - recall: 0.2644 - val_loss: 1.2214 - val_accuracy: 0.4508 - val_f1: 0.2629 - val_precision: 0.6070 - val_recall: 0.1692\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.0802 - accuracy: 0.5483 - f1: 0.4179 - precision: 0.6966 - recall: 0.3031 - val_loss: 1.2382 - val_accuracy: 0.4567 - val_f1: 0.3305 - val_precision: 0.5729 - val_recall: 0.2325\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.0323 - accuracy: 0.5789 - f1: 0.4816 - precision: 0.6929 - recall: 0.3718 - val_loss: 1.2256 - val_accuracy: 0.4617 - val_f1: 0.3432 - val_precision: 0.5660 - val_recall: 0.2467\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.9939 - accuracy: 0.5998 - f1: 0.5203 - precision: 0.7243 - recall: 0.4085 - val_loss: 1.2234 - val_accuracy: 0.4642 - val_f1: 0.3721 - val_precision: 0.5794 - val_recall: 0.2750\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.9400 - accuracy: 0.6170 - f1: 0.5709 - precision: 0.7401 - recall: 0.4661 - val_loss: 1.2327 - val_accuracy: 0.4700 - val_f1: 0.3807 - val_precision: 0.5733 - val_recall: 0.2858\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.9213 - accuracy: 0.6365 - f1: 0.5747 - precision: 0.7416 - recall: 0.4716 - val_loss: 1.2565 - val_accuracy: 0.4658 - val_f1: 0.3937 - val_precision: 0.5580 - val_recall: 0.3050\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8733 - accuracy: 0.6607 - f1: 0.6086 - precision: 0.7666 - recall: 0.5062 - val_loss: 1.2326 - val_accuracy: 0.4900 - val_f1: 0.4207 - val_precision: 0.5693 - val_recall: 0.3342\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.8448 - accuracy: 0.6778 - f1: 0.6253 - precision: 0.7720 - recall: 0.5274 - val_loss: 1.2943 - val_accuracy: 0.4717 - val_f1: 0.4074 - val_precision: 0.5138 - val_recall: 0.3383\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.8000 - accuracy: 0.6920 - f1: 0.6517 - precision: 0.7753 - recall: 0.5642 - val_loss: 1.3480 - val_accuracy: 0.4758 - val_f1: 0.4339 - val_precision: 0.5186 - val_recall: 0.3733\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.7570 - accuracy: 0.7151 - f1: 0.6901 - precision: 0.7934 - recall: 0.6115 - val_loss: 1.3786 - val_accuracy: 0.4983 - val_f1: 0.4527 - val_precision: 0.5280 - val_recall: 0.3967\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.7259 - accuracy: 0.7283 - f1: 0.6998 - precision: 0.8057 - recall: 0.6204 - val_loss: 1.4590 - val_accuracy: 0.5025 - val_f1: 0.4770 - val_precision: 0.5448 - val_recall: 0.4250\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.6957 - accuracy: 0.7434 - f1: 0.7211 - precision: 0.8120 - recall: 0.6506 - val_loss: 1.5754 - val_accuracy: 0.5050 - val_f1: 0.4780 - val_precision: 0.5261 - val_recall: 0.4383\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.6598 - accuracy: 0.7512 - f1: 0.7403 - precision: 0.8141 - recall: 0.6801 - val_loss: 1.5575 - val_accuracy: 0.5208 - val_f1: 0.4989 - val_precision: 0.5539 - val_recall: 0.4542\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.6160 - accuracy: 0.7676 - f1: 0.7538 - precision: 0.8217 - recall: 0.6974 - val_loss: 1.5947 - val_accuracy: 0.5117 - val_f1: 0.5109 - val_precision: 0.5600 - val_recall: 0.4700\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5703 - accuracy: 0.7878 - f1: 0.7824 - precision: 0.8439 - recall: 0.7301 - val_loss: 1.7227 - val_accuracy: 0.5275 - val_f1: 0.5138 - val_precision: 0.5508 - val_recall: 0.4817\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5710 - accuracy: 0.7971 - f1: 0.7883 - precision: 0.8481 - recall: 0.7372 - val_loss: 1.6973 - val_accuracy: 0.5317 - val_f1: 0.5123 - val_precision: 0.5581 - val_recall: 0.4742\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5210 - accuracy: 0.8110 - f1: 0.8065 - precision: 0.8613 - recall: 0.7591 - val_loss: 1.7977 - val_accuracy: 0.5292 - val_f1: 0.5134 - val_precision: 0.5523 - val_recall: 0.4800\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.4892 - accuracy: 0.8200 - f1: 0.8182 - precision: 0.8678 - recall: 0.7748 - val_loss: 1.7662 - val_accuracy: 0.5400 - val_f1: 0.5368 - val_precision: 0.5712 - val_recall: 0.5067\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.4794 - accuracy: 0.8185 - f1: 0.8163 - precision: 0.8602 - recall: 0.7773 - val_loss: 1.8287 - val_accuracy: 0.5450 - val_f1: 0.5394 - val_precision: 0.5664 - val_recall: 0.5150\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.4388 - accuracy: 0.8328 - f1: 0.8361 - precision: 0.8724 - recall: 0.8033 - val_loss: 1.8690 - val_accuracy: 0.5383 - val_f1: 0.5332 - val_precision: 0.5649 - val_recall: 0.5050\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.4153 - accuracy: 0.8488 - f1: 0.8455 - precision: 0.8850 - recall: 0.8101 - val_loss: 1.9801 - val_accuracy: 0.5475 - val_f1: 0.5335 - val_precision: 0.5637 - val_recall: 0.5067\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.3924 - accuracy: 0.8589 - f1: 0.8602 - precision: 0.8964 - recall: 0.8271 - val_loss: 1.7871 - val_accuracy: 0.5625 - val_f1: 0.5616 - val_precision: 0.5925 - val_recall: 0.5342\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3450 - accuracy: 0.8829 - f1: 0.8811 - precision: 0.9082 - recall: 0.8561 - val_loss: 1.7798 - val_accuracy: 0.5433 - val_f1: 0.5346 - val_precision: 0.5632 - val_recall: 0.5092\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.3182 - accuracy: 0.8907 - f1: 0.8904 - precision: 0.9142 - recall: 0.8681 - val_loss: 1.8674 - val_accuracy: 0.5633 - val_f1: 0.5663 - val_precision: 0.5865 - val_recall: 0.5475\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.3122 - accuracy: 0.8931 - f1: 0.8931 - precision: 0.9158 - recall: 0.8716 - val_loss: 2.0525 - val_accuracy: 0.5517 - val_f1: 0.5516 - val_precision: 0.5692 - val_recall: 0.5350\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2784 - accuracy: 0.9035 - f1: 0.9024 - precision: 0.9182 - recall: 0.8874 - val_loss: 2.0793 - val_accuracy: 0.5642 - val_f1: 0.5588 - val_precision: 0.5791 - val_recall: 0.5400\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.2553 - accuracy: 0.9110 - f1: 0.9125 - precision: 0.9283 - recall: 0.8974 - val_loss: 2.2414 - val_accuracy: 0.5608 - val_f1: 0.5549 - val_precision: 0.5688 - val_recall: 0.5417\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2617 - accuracy: 0.9073 - f1: 0.9062 - precision: 0.9222 - recall: 0.8907 - val_loss: 2.3036 - val_accuracy: 0.5575 - val_f1: 0.5570 - val_precision: 0.5706 - val_recall: 0.5442\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2406 - accuracy: 0.9119 - f1: 0.9092 - precision: 0.9221 - recall: 0.8967 - val_loss: 2.4024 - val_accuracy: 0.5650 - val_f1: 0.5647 - val_precision: 0.5785 - val_recall: 0.5517\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.2348 - accuracy: 0.9239 - f1: 0.9235 - precision: 0.9357 - recall: 0.9118 - val_loss: 2.3163 - val_accuracy: 0.5608 - val_f1: 0.5575 - val_precision: 0.5745 - val_recall: 0.5417\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2326 - accuracy: 0.9190 - f1: 0.9202 - precision: 0.9347 - recall: 0.9063 - val_loss: 2.2572 - val_accuracy: 0.5642 - val_f1: 0.5625 - val_precision: 0.5785 - val_recall: 0.5475\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2268 - accuracy: 0.9181 - f1: 0.9212 - precision: 0.9338 - recall: 0.9091 - val_loss: 2.2083 - val_accuracy: 0.5667 - val_f1: 0.5657 - val_precision: 0.5786 - val_recall: 0.5533\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2135 - accuracy: 0.9261 - f1: 0.9277 - precision: 0.9401 - recall: 0.9157 - val_loss: 2.3858 - val_accuracy: 0.5700 - val_f1: 0.5649 - val_precision: 0.5799 - val_recall: 0.5508\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.2102 - accuracy: 0.9275 - f1: 0.9279 - precision: 0.9378 - recall: 0.9183 - val_loss: 2.1548 - val_accuracy: 0.5667 - val_f1: 0.5641 - val_precision: 0.5792 - val_recall: 0.5500\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.2261 - accuracy: 0.9167 - f1: 0.9183 - precision: 0.9311 - recall: 0.9060 - val_loss: 2.1120 - val_accuracy: 0.5750 - val_f1: 0.5734 - val_precision: 0.5922 - val_recall: 0.5558\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.1914 - accuracy: 0.9376 - f1: 0.9359 - precision: 0.9469 - recall: 0.9252 - val_loss: 2.2450 - val_accuracy: 0.5692 - val_f1: 0.5713 - val_precision: 0.5840 - val_recall: 0.5592\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.1840 - accuracy: 0.9352 - f1: 0.9352 - precision: 0.9443 - recall: 0.9263 - val_loss: 2.1411 - val_accuracy: 0.5892 - val_f1: 0.5816 - val_precision: 0.5974 - val_recall: 0.5667\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.1701 - accuracy: 0.9448 - f1: 0.9432 - precision: 0.9526 - recall: 0.9340 - val_loss: 2.2883 - val_accuracy: 0.5783 - val_f1: 0.5729 - val_precision: 0.5848 - val_recall: 0.5617\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.1770 - accuracy: 0.9380 - f1: 0.9386 - precision: 0.9502 - recall: 0.9273 - val_loss: 2.2192 - val_accuracy: 0.5683 - val_f1: 0.5655 - val_precision: 0.5811 - val_recall: 0.5508\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.1428 - accuracy: 0.9514 - f1: 0.9505 - precision: 0.9576 - recall: 0.9436 - val_loss: 2.4243 - val_accuracy: 0.5775 - val_f1: 0.5727 - val_precision: 0.5806 - val_recall: 0.5650\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.1697 - accuracy: 0.9418 - f1: 0.9417 - precision: 0.9490 - recall: 0.9346 - val_loss: 2.4190 - val_accuracy: 0.5800 - val_f1: 0.5801 - val_precision: 0.5907 - val_recall: 0.5700\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.1303 - accuracy: 0.9527 - f1: 0.9545 - precision: 0.9605 - recall: 0.9486 - val_loss: 2.5190 - val_accuracy: 0.5817 - val_f1: 0.5820 - val_precision: 0.5902 - val_recall: 0.5742\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.1180 - accuracy: 0.9590 - f1: 0.9585 - precision: 0.9644 - recall: 0.9527 - val_loss: 2.5291 - val_accuracy: 0.5767 - val_f1: 0.5747 - val_precision: 0.5868 - val_recall: 0.5633\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1196 - accuracy: 0.9583 - f1: 0.9580 - precision: 0.9625 - recall: 0.9536 - val_loss: 2.4874 - val_accuracy: 0.5808 - val_f1: 0.5785 - val_precision: 0.5909 - val_recall: 0.5667\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1034 - accuracy: 0.9595 - f1: 0.9604 - precision: 0.9657 - recall: 0.9551 - val_loss: 2.3675 - val_accuracy: 0.5917 - val_f1: 0.5935 - val_precision: 0.6059 - val_recall: 0.5817\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.1030 - accuracy: 0.9626 - f1: 0.9642 - precision: 0.9685 - recall: 0.9601 - val_loss: 2.5056 - val_accuracy: 0.5867 - val_f1: 0.5880 - val_precision: 0.5972 - val_recall: 0.5792\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1227 - accuracy: 0.9551 - f1: 0.9548 - precision: 0.9583 - recall: 0.9514 - val_loss: 2.5087 - val_accuracy: 0.5867 - val_f1: 0.5867 - val_precision: 0.5945 - val_recall: 0.5792\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1708 - accuracy: 0.9474 - f1: 0.9476 - precision: 0.9531 - recall: 0.9422 - val_loss: 2.5268 - val_accuracy: 0.5742 - val_f1: 0.5735 - val_precision: 0.5815 - val_recall: 0.5658\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1839 - accuracy: 0.9429 - f1: 0.9428 - precision: 0.9505 - recall: 0.9353 - val_loss: 2.4116 - val_accuracy: 0.5883 - val_f1: 0.5850 - val_precision: 0.5964 - val_recall: 0.5742\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1130 - accuracy: 0.9639 - f1: 0.9632 - precision: 0.9669 - recall: 0.9596 - val_loss: 2.4806 - val_accuracy: 0.5700 - val_f1: 0.5683 - val_precision: 0.5805 - val_recall: 0.5567\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.0985 - accuracy: 0.9628 - f1: 0.9637 - precision: 0.9670 - recall: 0.9605 - val_loss: 2.5657 - val_accuracy: 0.5783 - val_f1: 0.5784 - val_precision: 0.5853 - val_recall: 0.5717\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.0926 - accuracy: 0.9698 - f1: 0.9694 - precision: 0.9725 - recall: 0.9665 - val_loss: 2.6574 - val_accuracy: 0.5692 - val_f1: 0.5694 - val_precision: 0.5774 - val_recall: 0.5617\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1091 - accuracy: 0.9638 - f1: 0.9626 - precision: 0.9652 - recall: 0.9601 - val_loss: 2.6243 - val_accuracy: 0.5725 - val_f1: 0.5695 - val_precision: 0.5777 - val_recall: 0.5617\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.1072 - accuracy: 0.9626 - f1: 0.9627 - precision: 0.9659 - recall: 0.9596 - val_loss: 2.7218 - val_accuracy: 0.5792 - val_f1: 0.5817 - val_precision: 0.5913 - val_recall: 0.5725\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.0916 - accuracy: 0.9682 - f1: 0.9670 - precision: 0.9688 - recall: 0.9652 - val_loss: 2.6625 - val_accuracy: 0.5742 - val_f1: 0.5732 - val_precision: 0.5810 - val_recall: 0.5658\n",
            "14/14 [==============================] - 1s 88ms/step - loss: 2.6698 - accuracy: 0.6014 - f1: 0.6035 - precision: 0.6079 - recall: 0.5993\n",
            "[2.6698436737060547, 0.6014285683631897, 0.6035441160202026, 0.607893168926239, 0.5992856621742249]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab4376b-529b-4107-c453-353b86164541"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 7s 126ms/step - loss: 1.3901 - accuracy: 0.2765 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3692 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3635 - accuracy: 0.3224 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3651 - val_accuracy: 0.3233 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3521 - accuracy: 0.3319 - f1: 0.0024 - precision: 0.0897 - recall: 0.0012 - val_loss: 1.3639 - val_accuracy: 0.3208 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3272 - accuracy: 0.3697 - f1: 0.0207 - precision: 0.6026 - recall: 0.0106 - val_loss: 1.3498 - val_accuracy: 0.3317 - val_f1: 0.0275 - val_precision: 0.6042 - val_recall: 0.0142\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.2960 - accuracy: 0.3952 - f1: 0.0596 - precision: 0.5966 - recall: 0.0325 - val_loss: 1.3539 - val_accuracy: 0.3158 - val_f1: 0.0440 - val_precision: 0.4440 - val_recall: 0.0233\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2495 - accuracy: 0.4169 - f1: 0.1294 - precision: 0.6649 - recall: 0.0746 - val_loss: 1.3529 - val_accuracy: 0.3458 - val_f1: 0.1182 - val_precision: 0.4565 - val_recall: 0.0683\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.1988 - accuracy: 0.4574 - f1: 0.2377 - precision: 0.6405 - recall: 0.1480 - val_loss: 1.3729 - val_accuracy: 0.3358 - val_f1: 0.1507 - val_precision: 0.3900 - val_recall: 0.0942\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1645 - accuracy: 0.4865 - f1: 0.2927 - precision: 0.6426 - recall: 0.1908 - val_loss: 1.4155 - val_accuracy: 0.3225 - val_f1: 0.1829 - val_precision: 0.3577 - val_recall: 0.1233\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1215 - accuracy: 0.5053 - f1: 0.3792 - precision: 0.6542 - recall: 0.2684 - val_loss: 1.4621 - val_accuracy: 0.3150 - val_f1: 0.2016 - val_precision: 0.3396 - val_recall: 0.1442\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1255 - accuracy: 0.5012 - f1: 0.3994 - precision: 0.6273 - recall: 0.2940 - val_loss: 1.4442 - val_accuracy: 0.3433 - val_f1: 0.2223 - val_precision: 0.3559 - val_recall: 0.1625\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.0551 - accuracy: 0.5454 - f1: 0.4428 - precision: 0.6808 - recall: 0.3297 - val_loss: 1.4855 - val_accuracy: 0.3483 - val_f1: 0.2485 - val_precision: 0.3574 - val_recall: 0.1908\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.0089 - accuracy: 0.5893 - f1: 0.4977 - precision: 0.6909 - recall: 0.3898 - val_loss: 1.5515 - val_accuracy: 0.3317 - val_f1: 0.2612 - val_precision: 0.3635 - val_recall: 0.2042\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.9985 - accuracy: 0.5902 - f1: 0.5205 - precision: 0.7004 - recall: 0.4154 - val_loss: 1.5603 - val_accuracy: 0.3450 - val_f1: 0.2614 - val_precision: 0.3470 - val_recall: 0.2100\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.9420 - accuracy: 0.6219 - f1: 0.5493 - precision: 0.7119 - recall: 0.4478 - val_loss: 1.6067 - val_accuracy: 0.3225 - val_f1: 0.2707 - val_precision: 0.3429 - val_recall: 0.2242\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.9113 - accuracy: 0.6315 - f1: 0.5860 - precision: 0.7248 - recall: 0.4925 - val_loss: 1.6360 - val_accuracy: 0.3458 - val_f1: 0.2933 - val_precision: 0.3575 - val_recall: 0.2492\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.9179 - accuracy: 0.6132 - f1: 0.5842 - precision: 0.7104 - recall: 0.4967 - val_loss: 1.6658 - val_accuracy: 0.3500 - val_f1: 0.3062 - val_precision: 0.3699 - val_recall: 0.2617\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.8882 - accuracy: 0.6325 - f1: 0.6060 - precision: 0.7197 - recall: 0.5240 - val_loss: 1.7100 - val_accuracy: 0.3500 - val_f1: 0.3123 - val_precision: 0.3697 - val_recall: 0.2708\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.8579 - accuracy: 0.6582 - f1: 0.6119 - precision: 0.7252 - recall: 0.5300 - val_loss: 1.7352 - val_accuracy: 0.3583 - val_f1: 0.3343 - val_precision: 0.3836 - val_recall: 0.2967\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.8646 - accuracy: 0.6602 - f1: 0.6246 - precision: 0.7249 - recall: 0.5496 - val_loss: 1.7219 - val_accuracy: 0.3517 - val_f1: 0.3123 - val_precision: 0.3660 - val_recall: 0.2725\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.8220 - accuracy: 0.6647 - f1: 0.6449 - precision: 0.7444 - recall: 0.5695 - val_loss: 1.7808 - val_accuracy: 0.3283 - val_f1: 0.3014 - val_precision: 0.3548 - val_recall: 0.2625\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.8357 - accuracy: 0.6605 - f1: 0.6315 - precision: 0.7335 - recall: 0.5549 - val_loss: 1.7842 - val_accuracy: 0.3333 - val_f1: 0.3013 - val_precision: 0.3564 - val_recall: 0.2617\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.7719 - accuracy: 0.6974 - f1: 0.6696 - precision: 0.7688 - recall: 0.5934 - val_loss: 1.7346 - val_accuracy: 0.3400 - val_f1: 0.3136 - val_precision: 0.3715 - val_recall: 0.2717\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.7379 - accuracy: 0.7191 - f1: 0.6965 - precision: 0.7829 - recall: 0.6276 - val_loss: 1.8537 - val_accuracy: 0.3475 - val_f1: 0.3014 - val_precision: 0.3487 - val_recall: 0.2658\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.7099 - accuracy: 0.7180 - f1: 0.7033 - precision: 0.7862 - recall: 0.6368 - val_loss: 1.9670 - val_accuracy: 0.3242 - val_f1: 0.2999 - val_precision: 0.3421 - val_recall: 0.2675\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.6980 - accuracy: 0.7187 - f1: 0.7112 - precision: 0.7923 - recall: 0.6457 - val_loss: 1.9608 - val_accuracy: 0.3308 - val_f1: 0.3007 - val_precision: 0.3410 - val_recall: 0.2692\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.7184 - accuracy: 0.7064 - f1: 0.6940 - precision: 0.7670 - recall: 0.6342 - val_loss: 1.9877 - val_accuracy: 0.3267 - val_f1: 0.2960 - val_precision: 0.3317 - val_recall: 0.2675\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.7130 - accuracy: 0.7190 - f1: 0.7012 - precision: 0.7743 - recall: 0.6414 - val_loss: 2.0112 - val_accuracy: 0.3475 - val_f1: 0.3195 - val_precision: 0.3612 - val_recall: 0.2867\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.7368 - accuracy: 0.6962 - f1: 0.6837 - precision: 0.7601 - recall: 0.6221 - val_loss: 1.9842 - val_accuracy: 0.3525 - val_f1: 0.3153 - val_precision: 0.3571 - val_recall: 0.2825\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.7049 - accuracy: 0.7103 - f1: 0.6940 - precision: 0.7616 - recall: 0.6378 - val_loss: 1.9671 - val_accuracy: 0.3450 - val_f1: 0.3176 - val_precision: 0.3541 - val_recall: 0.2883\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.6483 - accuracy: 0.7396 - f1: 0.7296 - precision: 0.8002 - recall: 0.6710 - val_loss: 1.9518 - val_accuracy: 0.3567 - val_f1: 0.3172 - val_precision: 0.3579 - val_recall: 0.2850\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.5722 - accuracy: 0.7812 - f1: 0.7753 - precision: 0.8352 - recall: 0.7242 - val_loss: 2.0367 - val_accuracy: 0.3475 - val_f1: 0.3158 - val_precision: 0.3531 - val_recall: 0.2858\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.5891 - accuracy: 0.7741 - f1: 0.7608 - precision: 0.8138 - recall: 0.7145 - val_loss: 2.1100 - val_accuracy: 0.3408 - val_f1: 0.3139 - val_precision: 0.3485 - val_recall: 0.2858\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.5904 - accuracy: 0.7651 - f1: 0.7574 - precision: 0.8075 - recall: 0.7136 - val_loss: 2.1003 - val_accuracy: 0.3450 - val_f1: 0.3160 - val_precision: 0.3496 - val_recall: 0.2883\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.5801 - accuracy: 0.7756 - f1: 0.7560 - precision: 0.8134 - recall: 0.7071 - val_loss: 2.1506 - val_accuracy: 0.3367 - val_f1: 0.3235 - val_precision: 0.3557 - val_recall: 0.2967\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.5459 - accuracy: 0.7807 - f1: 0.7745 - precision: 0.8182 - recall: 0.7356 - val_loss: 2.2439 - val_accuracy: 0.3258 - val_f1: 0.3176 - val_precision: 0.3465 - val_recall: 0.2933\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.5371 - accuracy: 0.7871 - f1: 0.7886 - precision: 0.8309 - recall: 0.7507 - val_loss: 2.2828 - val_accuracy: 0.3533 - val_f1: 0.3264 - val_precision: 0.3541 - val_recall: 0.3033\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.5078 - accuracy: 0.8023 - f1: 0.7937 - precision: 0.8338 - recall: 0.7576 - val_loss: 2.3424 - val_accuracy: 0.3417 - val_f1: 0.3307 - val_precision: 0.3568 - val_recall: 0.3083\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.5574 - accuracy: 0.7792 - f1: 0.7775 - precision: 0.8154 - recall: 0.7434 - val_loss: 2.2434 - val_accuracy: 0.3650 - val_f1: 0.3524 - val_precision: 0.3818 - val_recall: 0.3275\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.5000 - accuracy: 0.8009 - f1: 0.7919 - precision: 0.8319 - recall: 0.7559 - val_loss: 2.2546 - val_accuracy: 0.3725 - val_f1: 0.3562 - val_precision: 0.3871 - val_recall: 0.3300\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.4344 - accuracy: 0.8306 - f1: 0.8238 - precision: 0.8561 - recall: 0.7942 - val_loss: 2.4333 - val_accuracy: 0.3742 - val_f1: 0.3620 - val_precision: 0.3831 - val_recall: 0.3433\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.4280 - accuracy: 0.8358 - f1: 0.8309 - precision: 0.8584 - recall: 0.8053 - val_loss: 2.5612 - val_accuracy: 0.3608 - val_f1: 0.3516 - val_precision: 0.3710 - val_recall: 0.3342\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.4172 - accuracy: 0.8378 - f1: 0.8354 - precision: 0.8635 - recall: 0.8092 - val_loss: 2.6707 - val_accuracy: 0.3608 - val_f1: 0.3468 - val_precision: 0.3655 - val_recall: 0.3300\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3836 - accuracy: 0.8459 - f1: 0.8455 - precision: 0.8720 - recall: 0.8208 - val_loss: 2.7592 - val_accuracy: 0.3425 - val_f1: 0.3326 - val_precision: 0.3503 - val_recall: 0.3167\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3596 - accuracy: 0.8700 - f1: 0.8676 - precision: 0.8875 - recall: 0.8486 - val_loss: 2.7109 - val_accuracy: 0.3492 - val_f1: 0.3406 - val_precision: 0.3559 - val_recall: 0.3267\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.3951 - accuracy: 0.8460 - f1: 0.8437 - precision: 0.8669 - recall: 0.8218 - val_loss: 2.8840 - val_accuracy: 0.3425 - val_f1: 0.3353 - val_precision: 0.3524 - val_recall: 0.3200\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4009 - accuracy: 0.8528 - f1: 0.8508 - precision: 0.8775 - recall: 0.8258 - val_loss: 2.8518 - val_accuracy: 0.3442 - val_f1: 0.3335 - val_precision: 0.3515 - val_recall: 0.3175\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.4082 - accuracy: 0.8472 - f1: 0.8441 - precision: 0.8660 - recall: 0.8235 - val_loss: 2.7754 - val_accuracy: 0.3675 - val_f1: 0.3625 - val_precision: 0.3791 - val_recall: 0.3475\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4009 - accuracy: 0.8539 - f1: 0.8519 - precision: 0.8695 - recall: 0.8352 - val_loss: 2.6761 - val_accuracy: 0.3833 - val_f1: 0.3810 - val_precision: 0.3997 - val_recall: 0.3642\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4319 - accuracy: 0.8376 - f1: 0.8375 - precision: 0.8571 - recall: 0.8190 - val_loss: 2.7047 - val_accuracy: 0.3758 - val_f1: 0.3671 - val_precision: 0.3854 - val_recall: 0.3508\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.4114 - accuracy: 0.8451 - f1: 0.8437 - precision: 0.8613 - recall: 0.8269 - val_loss: 2.6454 - val_accuracy: 0.3775 - val_f1: 0.3682 - val_precision: 0.3866 - val_recall: 0.3517\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3688 - accuracy: 0.8556 - f1: 0.8552 - precision: 0.8757 - recall: 0.8360 - val_loss: 2.7647 - val_accuracy: 0.3717 - val_f1: 0.3682 - val_precision: 0.3845 - val_recall: 0.3533\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.3495 - accuracy: 0.8667 - f1: 0.8658 - precision: 0.8825 - recall: 0.8499 - val_loss: 2.7024 - val_accuracy: 0.3783 - val_f1: 0.3640 - val_precision: 0.3824 - val_recall: 0.3475\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3901 - accuracy: 0.8540 - f1: 0.8529 - precision: 0.8730 - recall: 0.8339 - val_loss: 2.7925 - val_accuracy: 0.3733 - val_f1: 0.3610 - val_precision: 0.3748 - val_recall: 0.3483\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.4174 - accuracy: 0.8371 - f1: 0.8355 - precision: 0.8586 - recall: 0.8138 - val_loss: 2.7841 - val_accuracy: 0.3717 - val_f1: 0.3719 - val_precision: 0.3877 - val_recall: 0.3575\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.4154 - accuracy: 0.8364 - f1: 0.8352 - precision: 0.8543 - recall: 0.8172 - val_loss: 2.7705 - val_accuracy: 0.3883 - val_f1: 0.3839 - val_precision: 0.4010 - val_recall: 0.3683\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.4613 - accuracy: 0.8238 - f1: 0.8208 - precision: 0.8412 - recall: 0.8015 - val_loss: 2.5930 - val_accuracy: 0.3992 - val_f1: 0.3934 - val_precision: 0.4078 - val_recall: 0.3800\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.3639 - accuracy: 0.8607 - f1: 0.8600 - precision: 0.8773 - recall: 0.8436 - val_loss: 2.7686 - val_accuracy: 0.3942 - val_f1: 0.3867 - val_precision: 0.4021 - val_recall: 0.3725\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 0.4044 - accuracy: 0.8445 - f1: 0.8458 - precision: 0.8665 - recall: 0.8262 - val_loss: 2.7880 - val_accuracy: 0.3950 - val_f1: 0.3778 - val_precision: 0.3935 - val_recall: 0.3633\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3613 - accuracy: 0.8578 - f1: 0.8577 - precision: 0.8780 - recall: 0.8385 - val_loss: 2.7363 - val_accuracy: 0.3917 - val_f1: 0.3803 - val_precision: 0.3961 - val_recall: 0.3658\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3280 - accuracy: 0.8865 - f1: 0.8840 - precision: 0.8979 - recall: 0.8706 - val_loss: 2.7371 - val_accuracy: 0.4000 - val_f1: 0.3922 - val_precision: 0.4044 - val_recall: 0.3808\n",
            "14/14 [==============================] - 1s 84ms/step - loss: 2.8457 - accuracy: 0.4000 - f1: 0.3926 - precision: 0.4080 - recall: 0.3786\n",
            "[2.8456995487213135, 0.4000000059604645, 0.39263924956321716, 0.40799790620803833, 0.378571480512619]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4b0953-9d8b-4d10-d983-9a222f25309f"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(64))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 7s 117ms/step - loss: 1.3813 - accuracy: 0.3200 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3708 - val_accuracy: 0.3233 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.3626 - accuracy: 0.3317 - f1: 0.0196 - precision: 0.1941 - recall: 0.0108 - val_loss: 1.3536 - val_accuracy: 0.3358 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.3245 - accuracy: 0.3585 - f1: 0.0940 - precision: 0.6108 - recall: 0.0529 - val_loss: 1.3335 - val_accuracy: 0.3533 - val_f1: 0.0837 - val_precision: 0.6537 - val_recall: 0.0450\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.3074 - accuracy: 0.3656 - f1: 0.1619 - precision: 0.6478 - recall: 0.0936 - val_loss: 1.3240 - val_accuracy: 0.3858 - val_f1: 0.1884 - val_precision: 0.5738 - val_recall: 0.1133\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.2916 - accuracy: 0.3869 - f1: 0.1810 - precision: 0.6756 - recall: 0.1079 - val_loss: 1.3217 - val_accuracy: 0.3833 - val_f1: 0.2047 - val_precision: 0.5819 - val_recall: 0.1250\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 1.2872 - accuracy: 0.3884 - f1: 0.1847 - precision: 0.6682 - recall: 0.1093 - val_loss: 1.3165 - val_accuracy: 0.3883 - val_f1: 0.1618 - val_precision: 0.6310 - val_recall: 0.0942\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 1.2704 - accuracy: 0.3926 - f1: 0.1996 - precision: 0.7197 - recall: 0.1181 - val_loss: 1.3391 - val_accuracy: 0.3767 - val_f1: 0.1243 - val_precision: 0.6192 - val_recall: 0.0700\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.2584 - accuracy: 0.4057 - f1: 0.2075 - precision: 0.7023 - recall: 0.1239 - val_loss: 1.3521 - val_accuracy: 0.3783 - val_f1: 0.1255 - val_precision: 0.6050 - val_recall: 0.0708\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.2414 - accuracy: 0.4199 - f1: 0.2300 - precision: 0.7078 - recall: 0.1395 - val_loss: 1.3737 - val_accuracy: 0.3900 - val_f1: 0.1396 - val_precision: 0.5925 - val_recall: 0.0800\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.2119 - accuracy: 0.4375 - f1: 0.2666 - precision: 0.7008 - recall: 0.1668 - val_loss: 1.3461 - val_accuracy: 0.4000 - val_f1: 0.2935 - val_precision: 0.5789 - val_recall: 0.1983\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.1730 - accuracy: 0.4673 - f1: 0.3289 - precision: 0.6777 - recall: 0.2199 - val_loss: 1.3676 - val_accuracy: 0.4100 - val_f1: 0.3382 - val_precision: 0.5404 - val_recall: 0.2475\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 1.1445 - accuracy: 0.4696 - f1: 0.3544 - precision: 0.6944 - recall: 0.2427 - val_loss: 1.3532 - val_accuracy: 0.4242 - val_f1: 0.3563 - val_precision: 0.5395 - val_recall: 0.2667\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 1.0976 - accuracy: 0.5098 - f1: 0.3994 - precision: 0.7286 - recall: 0.2810 - val_loss: 1.3562 - val_accuracy: 0.4283 - val_f1: 0.3462 - val_precision: 0.5472 - val_recall: 0.2542\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 1.0898 - accuracy: 0.5147 - f1: 0.4030 - precision: 0.7099 - recall: 0.2875 - val_loss: 1.3697 - val_accuracy: 0.4242 - val_f1: 0.3497 - val_precision: 0.5595 - val_recall: 0.2550\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 1.0578 - accuracy: 0.5410 - f1: 0.4141 - precision: 0.7288 - recall: 0.2928 - val_loss: 1.3968 - val_accuracy: 0.4300 - val_f1: 0.3448 - val_precision: 0.5620 - val_recall: 0.2500\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.0704 - accuracy: 0.5404 - f1: 0.4271 - precision: 0.7458 - recall: 0.3040 - val_loss: 1.4064 - val_accuracy: 0.4533 - val_f1: 0.3997 - val_precision: 0.5342 - val_recall: 0.3200\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.0438 - accuracy: 0.5405 - f1: 0.4493 - precision: 0.7498 - recall: 0.3287 - val_loss: 1.3044 - val_accuracy: 0.4683 - val_f1: 0.4035 - val_precision: 0.5633 - val_recall: 0.3150\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.9802 - accuracy: 0.5786 - f1: 0.4915 - precision: 0.7620 - recall: 0.3662 - val_loss: 1.2922 - val_accuracy: 0.4700 - val_f1: 0.4084 - val_precision: 0.5900 - val_recall: 0.3133\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.9342 - accuracy: 0.6036 - f1: 0.5198 - precision: 0.7811 - recall: 0.3907 - val_loss: 1.3164 - val_accuracy: 0.4808 - val_f1: 0.4201 - val_precision: 0.5984 - val_recall: 0.3242\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.8931 - accuracy: 0.6238 - f1: 0.5438 - precision: 0.8041 - recall: 0.4128 - val_loss: 1.3371 - val_accuracy: 0.4692 - val_f1: 0.4112 - val_precision: 0.6040 - val_recall: 0.3125\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8590 - accuracy: 0.6403 - f1: 0.5504 - precision: 0.8278 - recall: 0.4152 - val_loss: 1.3871 - val_accuracy: 0.4858 - val_f1: 0.4227 - val_precision: 0.6192 - val_recall: 0.3217\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8370 - accuracy: 0.6577 - f1: 0.5681 - precision: 0.8321 - recall: 0.4344 - val_loss: 1.3521 - val_accuracy: 0.4908 - val_f1: 0.4469 - val_precision: 0.6288 - val_recall: 0.3475\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.8240 - accuracy: 0.6590 - f1: 0.5816 - precision: 0.8295 - recall: 0.4518 - val_loss: 1.5234 - val_accuracy: 0.4942 - val_f1: 0.4297 - val_precision: 0.6043 - val_recall: 0.3342\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.8720 - accuracy: 0.6307 - f1: 0.5491 - precision: 0.8086 - recall: 0.4200 - val_loss: 1.4149 - val_accuracy: 0.4900 - val_f1: 0.4368 - val_precision: 0.5850 - val_recall: 0.3500\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8382 - accuracy: 0.6534 - f1: 0.5861 - precision: 0.8149 - recall: 0.4598 - val_loss: 1.3708 - val_accuracy: 0.5067 - val_f1: 0.4660 - val_precision: 0.5959 - val_recall: 0.3833\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.7567 - accuracy: 0.6871 - f1: 0.6476 - precision: 0.8054 - recall: 0.5424 - val_loss: 1.3669 - val_accuracy: 0.5192 - val_f1: 0.4872 - val_precision: 0.5871 - val_recall: 0.4175\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.7429 - accuracy: 0.7014 - f1: 0.6591 - precision: 0.7978 - recall: 0.5623 - val_loss: 1.3308 - val_accuracy: 0.4975 - val_f1: 0.4684 - val_precision: 0.5537 - val_recall: 0.4067\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.7338 - accuracy: 0.6965 - f1: 0.6661 - precision: 0.8003 - recall: 0.5720 - val_loss: 1.3127 - val_accuracy: 0.5208 - val_f1: 0.4955 - val_precision: 0.5609 - val_recall: 0.4442\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.6539 - accuracy: 0.7388 - f1: 0.7199 - precision: 0.8243 - recall: 0.6401 - val_loss: 1.3784 - val_accuracy: 0.5150 - val_f1: 0.4999 - val_precision: 0.5516 - val_recall: 0.4575\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.6392 - accuracy: 0.7368 - f1: 0.7299 - precision: 0.8196 - recall: 0.6590 - val_loss: 1.4943 - val_accuracy: 0.5075 - val_f1: 0.4983 - val_precision: 0.5419 - val_recall: 0.4617\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.6356 - accuracy: 0.7497 - f1: 0.7353 - precision: 0.8190 - recall: 0.6682 - val_loss: 1.5009 - val_accuracy: 0.5092 - val_f1: 0.5000 - val_precision: 0.5465 - val_recall: 0.4608\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.5931 - accuracy: 0.7693 - f1: 0.7563 - precision: 0.8295 - recall: 0.6954 - val_loss: 1.4591 - val_accuracy: 0.5050 - val_f1: 0.4933 - val_precision: 0.5369 - val_recall: 0.4567\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.5958 - accuracy: 0.7668 - f1: 0.7544 - precision: 0.8214 - recall: 0.6981 - val_loss: 1.4940 - val_accuracy: 0.5050 - val_f1: 0.4943 - val_precision: 0.5382 - val_recall: 0.4575\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.6040 - accuracy: 0.7589 - f1: 0.7500 - precision: 0.8164 - recall: 0.6942 - val_loss: 1.5109 - val_accuracy: 0.5233 - val_f1: 0.5052 - val_precision: 0.5444 - val_recall: 0.4717\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.5867 - accuracy: 0.7611 - f1: 0.7560 - precision: 0.8180 - recall: 0.7034 - val_loss: 1.4511 - val_accuracy: 0.5408 - val_f1: 0.5271 - val_precision: 0.5707 - val_recall: 0.4900\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.5647 - accuracy: 0.7710 - f1: 0.7655 - precision: 0.8353 - recall: 0.7071 - val_loss: 1.4802 - val_accuracy: 0.5408 - val_f1: 0.5281 - val_precision: 0.5599 - val_recall: 0.5000\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.5393 - accuracy: 0.7964 - f1: 0.7844 - precision: 0.8411 - recall: 0.7354 - val_loss: 1.5551 - val_accuracy: 0.5492 - val_f1: 0.5413 - val_precision: 0.5739 - val_recall: 0.5125\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.5074 - accuracy: 0.8069 - f1: 0.7990 - precision: 0.8542 - recall: 0.7510 - val_loss: 1.6300 - val_accuracy: 0.5458 - val_f1: 0.5387 - val_precision: 0.5742 - val_recall: 0.5075\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.5209 - accuracy: 0.7960 - f1: 0.7968 - precision: 0.8517 - recall: 0.7490 - val_loss: 1.6895 - val_accuracy: 0.5592 - val_f1: 0.5539 - val_precision: 0.5888 - val_recall: 0.5233\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.4954 - accuracy: 0.8123 - f1: 0.8087 - precision: 0.8594 - recall: 0.7642 - val_loss: 1.7200 - val_accuracy: 0.5525 - val_f1: 0.5529 - val_precision: 0.5789 - val_recall: 0.5292\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.4644 - accuracy: 0.8295 - f1: 0.8233 - precision: 0.8706 - recall: 0.7813 - val_loss: 1.8049 - val_accuracy: 0.5600 - val_f1: 0.5539 - val_precision: 0.5898 - val_recall: 0.5225\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.4743 - accuracy: 0.8250 - f1: 0.8217 - precision: 0.8682 - recall: 0.7803 - val_loss: 1.6134 - val_accuracy: 0.5642 - val_f1: 0.5615 - val_precision: 0.5859 - val_recall: 0.5392\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.5212 - accuracy: 0.8054 - f1: 0.7967 - precision: 0.8464 - recall: 0.7533 - val_loss: 1.3954 - val_accuracy: 0.5767 - val_f1: 0.5644 - val_precision: 0.6039 - val_recall: 0.5300\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.4446 - accuracy: 0.8368 - f1: 0.8306 - precision: 0.8805 - recall: 0.7865 - val_loss: 1.5659 - val_accuracy: 0.5875 - val_f1: 0.5801 - val_precision: 0.6140 - val_recall: 0.5500\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3657 - accuracy: 0.8730 - f1: 0.8682 - precision: 0.9033 - recall: 0.8361 - val_loss: 1.7965 - val_accuracy: 0.5875 - val_f1: 0.5853 - val_precision: 0.6143 - val_recall: 0.5592\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3067 - accuracy: 0.8934 - f1: 0.8900 - precision: 0.9170 - recall: 0.8646 - val_loss: 1.9800 - val_accuracy: 0.5825 - val_f1: 0.5857 - val_precision: 0.6034 - val_recall: 0.5692\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.3164 - accuracy: 0.8897 - f1: 0.8887 - precision: 0.9106 - recall: 0.8681 - val_loss: 2.0939 - val_accuracy: 0.6042 - val_f1: 0.5996 - val_precision: 0.6143 - val_recall: 0.5858\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.2988 - accuracy: 0.8937 - f1: 0.8950 - precision: 0.9140 - recall: 0.8769 - val_loss: 2.1679 - val_accuracy: 0.5467 - val_f1: 0.5496 - val_precision: 0.5690 - val_recall: 0.5317\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3437 - accuracy: 0.8735 - f1: 0.8750 - precision: 0.8986 - recall: 0.8528 - val_loss: 1.9741 - val_accuracy: 0.5892 - val_f1: 0.5778 - val_precision: 0.5969 - val_recall: 0.5600\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.3034 - accuracy: 0.8926 - f1: 0.8921 - precision: 0.9127 - recall: 0.8727 - val_loss: 1.9908 - val_accuracy: 0.5867 - val_f1: 0.5863 - val_precision: 0.6020 - val_recall: 0.5717\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.2908 - accuracy: 0.9024 - f1: 0.8990 - precision: 0.9157 - recall: 0.8830 - val_loss: 1.9446 - val_accuracy: 0.5892 - val_f1: 0.5793 - val_precision: 0.5973 - val_recall: 0.5625\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2495 - accuracy: 0.9154 - f1: 0.9143 - precision: 0.9314 - recall: 0.8980 - val_loss: 1.9567 - val_accuracy: 0.5833 - val_f1: 0.5801 - val_precision: 0.5935 - val_recall: 0.5675\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.2482 - accuracy: 0.9207 - f1: 0.9178 - precision: 0.9344 - recall: 0.9018 - val_loss: 1.9667 - val_accuracy: 0.5767 - val_f1: 0.5715 - val_precision: 0.5873 - val_recall: 0.5567\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.2541 - accuracy: 0.9139 - f1: 0.9119 - precision: 0.9273 - recall: 0.8973 - val_loss: 1.9620 - val_accuracy: 0.5650 - val_f1: 0.5584 - val_precision: 0.5744 - val_recall: 0.5433\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.2580 - accuracy: 0.9127 - f1: 0.9103 - precision: 0.9247 - recall: 0.8965 - val_loss: 1.9413 - val_accuracy: 0.5525 - val_f1: 0.5499 - val_precision: 0.5665 - val_recall: 0.5342\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.2721 - accuracy: 0.9056 - f1: 0.9054 - precision: 0.9221 - recall: 0.8896 - val_loss: 1.6836 - val_accuracy: 0.5567 - val_f1: 0.5577 - val_precision: 0.5730 - val_recall: 0.5433\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.3178 - accuracy: 0.8844 - f1: 0.8840 - precision: 0.9048 - recall: 0.8645 - val_loss: 1.7386 - val_accuracy: 0.5867 - val_f1: 0.5885 - val_precision: 0.6027 - val_recall: 0.5750\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.2528 - accuracy: 0.9046 - f1: 0.9081 - precision: 0.9218 - recall: 0.8949 - val_loss: 1.8675 - val_accuracy: 0.6000 - val_f1: 0.5945 - val_precision: 0.6071 - val_recall: 0.5825\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.1894 - accuracy: 0.9373 - f1: 0.9380 - precision: 0.9465 - recall: 0.9298 - val_loss: 2.0691 - val_accuracy: 0.5900 - val_f1: 0.5901 - val_precision: 0.5979 - val_recall: 0.5825\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.1757 - accuracy: 0.9438 - f1: 0.9434 - precision: 0.9524 - recall: 0.9346 - val_loss: 2.0624 - val_accuracy: 0.5883 - val_f1: 0.5891 - val_precision: 0.6042 - val_recall: 0.5750\n",
            "14/14 [==============================] - 1s 83ms/step - loss: 2.1132 - accuracy: 0.5943 - f1: 0.5939 - precision: 0.6032 - recall: 0.5850\n",
            "[2.1132256984710693, 0.5942857265472412, 0.5939328074455261, 0.6031944155693054, 0.5850000381469727]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d341bb25-ed95-40b5-c085-75762e20cd05"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 124ms/step - loss: 1.3825 - accuracy: 0.2838 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3659 - val_accuracy: 0.3408 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.3661 - accuracy: 0.3167 - f1: 7.6929e-04 - precision: 0.0208 - recall: 3.9688e-04 - val_loss: 1.3358 - val_accuracy: 0.3908 - val_f1: 0.0480 - val_precision: 0.7153 - val_recall: 0.0250\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.3184 - accuracy: 0.3988 - f1: 0.0524 - precision: 0.6365 - recall: 0.0292 - val_loss: 1.2922 - val_accuracy: 0.4217 - val_f1: 0.1180 - val_precision: 0.6095 - val_recall: 0.0658\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 1.2425 - accuracy: 0.4504 - f1: 0.1761 - precision: 0.7452 - recall: 0.1031 - val_loss: 1.2590 - val_accuracy: 0.4383 - val_f1: 0.1776 - val_precision: 0.6005 - val_recall: 0.1050\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 1.1660 - accuracy: 0.4959 - f1: 0.2880 - precision: 0.7058 - recall: 0.1844 - val_loss: 1.2383 - val_accuracy: 0.4492 - val_f1: 0.2383 - val_precision: 0.5867 - val_recall: 0.1500\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 1.1116 - accuracy: 0.5429 - f1: 0.3699 - precision: 0.7138 - recall: 0.2532 - val_loss: 1.2221 - val_accuracy: 0.4633 - val_f1: 0.3248 - val_precision: 0.5717 - val_recall: 0.2275\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.0693 - accuracy: 0.5547 - f1: 0.4441 - precision: 0.6978 - recall: 0.3281 - val_loss: 1.2399 - val_accuracy: 0.4483 - val_f1: 0.3104 - val_precision: 0.5414 - val_recall: 0.2183\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 1.0454 - accuracy: 0.5761 - f1: 0.4798 - precision: 0.6958 - recall: 0.3677 - val_loss: 1.2622 - val_accuracy: 0.4508 - val_f1: 0.3824 - val_precision: 0.5339 - val_recall: 0.2983\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 1.0060 - accuracy: 0.5892 - f1: 0.5130 - precision: 0.7147 - recall: 0.4017 - val_loss: 1.2726 - val_accuracy: 0.4592 - val_f1: 0.3966 - val_precision: 0.5339 - val_recall: 0.3158\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.9883 - accuracy: 0.6014 - f1: 0.5244 - precision: 0.7162 - recall: 0.4158 - val_loss: 1.2448 - val_accuracy: 0.4667 - val_f1: 0.3935 - val_precision: 0.5470 - val_recall: 0.3075\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.9392 - accuracy: 0.6308 - f1: 0.5611 - precision: 0.7393 - recall: 0.4534 - val_loss: 1.3261 - val_accuracy: 0.4525 - val_f1: 0.3959 - val_precision: 0.5155 - val_recall: 0.3217\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8887 - accuracy: 0.6558 - f1: 0.6044 - precision: 0.7576 - recall: 0.5038 - val_loss: 1.3416 - val_accuracy: 0.4558 - val_f1: 0.3877 - val_precision: 0.5010 - val_recall: 0.3167\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.8569 - accuracy: 0.6628 - f1: 0.6196 - precision: 0.7755 - recall: 0.5172 - val_loss: 1.3948 - val_accuracy: 0.4375 - val_f1: 0.3925 - val_precision: 0.4921 - val_recall: 0.3267\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8352 - accuracy: 0.6744 - f1: 0.6384 - precision: 0.7703 - recall: 0.5463 - val_loss: 1.3940 - val_accuracy: 0.4467 - val_f1: 0.3856 - val_precision: 0.4946 - val_recall: 0.3167\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.8391 - accuracy: 0.6761 - f1: 0.6230 - precision: 0.7699 - recall: 0.5249 - val_loss: 1.3899 - val_accuracy: 0.4675 - val_f1: 0.4339 - val_precision: 0.5287 - val_recall: 0.3683\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.8046 - accuracy: 0.6944 - f1: 0.6580 - precision: 0.7867 - recall: 0.5664 - val_loss: 1.4130 - val_accuracy: 0.4767 - val_f1: 0.4426 - val_precision: 0.5293 - val_recall: 0.3808\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.7621 - accuracy: 0.7074 - f1: 0.6882 - precision: 0.8040 - recall: 0.6022 - val_loss: 1.4443 - val_accuracy: 0.4867 - val_f1: 0.4490 - val_precision: 0.5265 - val_recall: 0.3917\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.7142 - accuracy: 0.7216 - f1: 0.7035 - precision: 0.8106 - recall: 0.6222 - val_loss: 1.4670 - val_accuracy: 0.4875 - val_f1: 0.4542 - val_precision: 0.5366 - val_recall: 0.3942\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.6754 - accuracy: 0.7411 - f1: 0.7234 - precision: 0.8237 - recall: 0.6455 - val_loss: 1.5190 - val_accuracy: 0.4592 - val_f1: 0.4341 - val_precision: 0.5104 - val_recall: 0.3783\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.6544 - accuracy: 0.7520 - f1: 0.7293 - precision: 0.8250 - recall: 0.6544 - val_loss: 1.5345 - val_accuracy: 0.4892 - val_f1: 0.4676 - val_precision: 0.5267 - val_recall: 0.4208\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.6130 - accuracy: 0.7704 - f1: 0.7603 - precision: 0.8387 - recall: 0.6960 - val_loss: 1.5618 - val_accuracy: 0.4875 - val_f1: 0.4631 - val_precision: 0.5218 - val_recall: 0.4167\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.5841 - accuracy: 0.7856 - f1: 0.7774 - precision: 0.8528 - recall: 0.7146 - val_loss: 1.6633 - val_accuracy: 0.4650 - val_f1: 0.4524 - val_precision: 0.5011 - val_recall: 0.4125\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.5457 - accuracy: 0.7927 - f1: 0.7925 - precision: 0.8517 - recall: 0.7414 - val_loss: 1.6981 - val_accuracy: 0.4808 - val_f1: 0.4548 - val_precision: 0.5013 - val_recall: 0.4167\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.5316 - accuracy: 0.8044 - f1: 0.7974 - precision: 0.8649 - recall: 0.7405 - val_loss: 1.8280 - val_accuracy: 0.4850 - val_f1: 0.4742 - val_precision: 0.5161 - val_recall: 0.4392\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.4780 - accuracy: 0.8299 - f1: 0.8273 - precision: 0.8794 - recall: 0.7813 - val_loss: 2.0475 - val_accuracy: 0.4892 - val_f1: 0.4787 - val_precision: 0.5105 - val_recall: 0.4508\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.4508 - accuracy: 0.8335 - f1: 0.8317 - precision: 0.8801 - recall: 0.7889 - val_loss: 1.9002 - val_accuracy: 0.4950 - val_f1: 0.4844 - val_precision: 0.5231 - val_recall: 0.4517\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.4369 - accuracy: 0.8331 - f1: 0.8343 - precision: 0.8844 - recall: 0.7902 - val_loss: 1.6631 - val_accuracy: 0.5050 - val_f1: 0.4908 - val_precision: 0.5330 - val_recall: 0.4550\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.4527 - accuracy: 0.8323 - f1: 0.8306 - precision: 0.8791 - recall: 0.7878 - val_loss: 1.7340 - val_accuracy: 0.4892 - val_f1: 0.4800 - val_precision: 0.5153 - val_recall: 0.4492\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.3864 - accuracy: 0.8614 - f1: 0.8571 - precision: 0.9008 - recall: 0.8182 - val_loss: 1.8525 - val_accuracy: 0.5100 - val_f1: 0.5019 - val_precision: 0.5322 - val_recall: 0.4750\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.3603 - accuracy: 0.8668 - f1: 0.8653 - precision: 0.9019 - recall: 0.8320 - val_loss: 1.8322 - val_accuracy: 0.5033 - val_f1: 0.4969 - val_precision: 0.5284 - val_recall: 0.4692\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.3544 - accuracy: 0.8739 - f1: 0.8726 - precision: 0.9095 - recall: 0.8390 - val_loss: 2.0597 - val_accuracy: 0.5133 - val_f1: 0.5120 - val_precision: 0.5363 - val_recall: 0.4900\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3669 - accuracy: 0.8675 - f1: 0.8655 - precision: 0.8991 - recall: 0.8345 - val_loss: 2.0203 - val_accuracy: 0.5083 - val_f1: 0.5035 - val_precision: 0.5245 - val_recall: 0.4842\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3151 - accuracy: 0.8861 - f1: 0.8892 - precision: 0.9118 - recall: 0.8680 - val_loss: 2.3183 - val_accuracy: 0.5150 - val_f1: 0.5115 - val_precision: 0.5294 - val_recall: 0.4950\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.3267 - accuracy: 0.8774 - f1: 0.8775 - precision: 0.9032 - recall: 0.8534 - val_loss: 2.1917 - val_accuracy: 0.5133 - val_f1: 0.5085 - val_precision: 0.5288 - val_recall: 0.4900\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 106ms/step - loss: 0.2824 - accuracy: 0.8950 - f1: 0.8963 - precision: 0.9164 - recall: 0.8773 - val_loss: 2.2876 - val_accuracy: 0.5083 - val_f1: 0.5112 - val_precision: 0.5296 - val_recall: 0.4942\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3040 - accuracy: 0.8883 - f1: 0.8898 - precision: 0.9111 - recall: 0.8696 - val_loss: 2.1729 - val_accuracy: 0.5050 - val_f1: 0.5047 - val_precision: 0.5252 - val_recall: 0.4858\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.2740 - accuracy: 0.9037 - f1: 0.9014 - precision: 0.9242 - recall: 0.8800 - val_loss: 2.1301 - val_accuracy: 0.5108 - val_f1: 0.5036 - val_precision: 0.5241 - val_recall: 0.4850\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.2673 - accuracy: 0.9007 - f1: 0.9017 - precision: 0.9228 - recall: 0.8816 - val_loss: 2.3117 - val_accuracy: 0.5192 - val_f1: 0.5137 - val_precision: 0.5284 - val_recall: 0.5000\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 105ms/step - loss: 0.2171 - accuracy: 0.9221 - f1: 0.9226 - precision: 0.9369 - recall: 0.9088 - val_loss: 2.5524 - val_accuracy: 0.5133 - val_f1: 0.5083 - val_precision: 0.5224 - val_recall: 0.4950\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.2381 - accuracy: 0.9113 - f1: 0.9134 - precision: 0.9285 - recall: 0.8990 - val_loss: 2.5704 - val_accuracy: 0.5317 - val_f1: 0.5248 - val_precision: 0.5379 - val_recall: 0.5125\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.2058 - accuracy: 0.9269 - f1: 0.9273 - precision: 0.9412 - recall: 0.9138 - val_loss: 2.4098 - val_accuracy: 0.5317 - val_f1: 0.5280 - val_precision: 0.5427 - val_recall: 0.5142\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.1946 - accuracy: 0.9279 - f1: 0.9292 - precision: 0.9415 - recall: 0.9173 - val_loss: 2.4669 - val_accuracy: 0.5317 - val_f1: 0.5283 - val_precision: 0.5397 - val_recall: 0.5175\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.1743 - accuracy: 0.9357 - f1: 0.9357 - precision: 0.9477 - recall: 0.9241 - val_loss: 2.3444 - val_accuracy: 0.5342 - val_f1: 0.5340 - val_precision: 0.5460 - val_recall: 0.5225\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.1603 - accuracy: 0.9401 - f1: 0.9423 - precision: 0.9551 - recall: 0.9300 - val_loss: 2.7026 - val_accuracy: 0.5317 - val_f1: 0.5275 - val_precision: 0.5389 - val_recall: 0.5167\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.1510 - accuracy: 0.9469 - f1: 0.9471 - precision: 0.9586 - recall: 0.9360 - val_loss: 2.7301 - val_accuracy: 0.5242 - val_f1: 0.5256 - val_precision: 0.5358 - val_recall: 0.5158\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.1637 - accuracy: 0.9438 - f1: 0.9449 - precision: 0.9528 - recall: 0.9372 - val_loss: 2.7039 - val_accuracy: 0.5375 - val_f1: 0.5373 - val_precision: 0.5468 - val_recall: 0.5283\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.1609 - accuracy: 0.9443 - f1: 0.9448 - precision: 0.9533 - recall: 0.9367 - val_loss: 2.3936 - val_accuracy: 0.5375 - val_f1: 0.5322 - val_precision: 0.5459 - val_recall: 0.5192\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1745 - accuracy: 0.9438 - f1: 0.9433 - precision: 0.9505 - recall: 0.9363 - val_loss: 2.5990 - val_accuracy: 0.5300 - val_f1: 0.5263 - val_precision: 0.5382 - val_recall: 0.5150\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.1409 - accuracy: 0.9475 - f1: 0.9498 - precision: 0.9553 - recall: 0.9443 - val_loss: 2.6217 - val_accuracy: 0.5400 - val_f1: 0.5349 - val_precision: 0.5472 - val_recall: 0.5233\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.1194 - accuracy: 0.9576 - f1: 0.9561 - precision: 0.9620 - recall: 0.9503 - val_loss: 2.7818 - val_accuracy: 0.5417 - val_f1: 0.5407 - val_precision: 0.5475 - val_recall: 0.5342\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.1071 - accuracy: 0.9604 - f1: 0.9587 - precision: 0.9636 - recall: 0.9540 - val_loss: 2.9266 - val_accuracy: 0.5450 - val_f1: 0.5442 - val_precision: 0.5530 - val_recall: 0.5358\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.1127 - accuracy: 0.9596 - f1: 0.9606 - precision: 0.9653 - recall: 0.9560 - val_loss: 2.7998 - val_accuracy: 0.5467 - val_f1: 0.5458 - val_precision: 0.5563 - val_recall: 0.5358\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.0908 - accuracy: 0.9670 - f1: 0.9669 - precision: 0.9710 - recall: 0.9629 - val_loss: 2.8969 - val_accuracy: 0.5492 - val_f1: 0.5449 - val_precision: 0.5500 - val_recall: 0.5400\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.0909 - accuracy: 0.9641 - f1: 0.9643 - precision: 0.9675 - recall: 0.9611 - val_loss: 2.9508 - val_accuracy: 0.5517 - val_f1: 0.5485 - val_precision: 0.5564 - val_recall: 0.5408\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.0908 - accuracy: 0.9686 - f1: 0.9683 - precision: 0.9706 - recall: 0.9660 - val_loss: 3.0092 - val_accuracy: 0.5408 - val_f1: 0.5394 - val_precision: 0.5473 - val_recall: 0.5317\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.0853 - accuracy: 0.9647 - f1: 0.9649 - precision: 0.9674 - recall: 0.9625 - val_loss: 3.1161 - val_accuracy: 0.5533 - val_f1: 0.5536 - val_precision: 0.5591 - val_recall: 0.5483\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 109ms/step - loss: 0.0900 - accuracy: 0.9645 - f1: 0.9639 - precision: 0.9654 - recall: 0.9624 - val_loss: 3.2633 - val_accuracy: 0.5367 - val_f1: 0.5357 - val_precision: 0.5434 - val_recall: 0.5283\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.0984 - accuracy: 0.9628 - f1: 0.9636 - precision: 0.9666 - recall: 0.9606 - val_loss: 3.0290 - val_accuracy: 0.5442 - val_f1: 0.5432 - val_precision: 0.5499 - val_recall: 0.5367\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.1230 - accuracy: 0.9529 - f1: 0.9543 - precision: 0.9587 - recall: 0.9499 - val_loss: 3.1950 - val_accuracy: 0.5300 - val_f1: 0.5299 - val_precision: 0.5357 - val_recall: 0.5242\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 110ms/step - loss: 0.1016 - accuracy: 0.9597 - f1: 0.9601 - precision: 0.9614 - recall: 0.9587 - val_loss: 3.0305 - val_accuracy: 0.5442 - val_f1: 0.5458 - val_precision: 0.5518 - val_recall: 0.5400\n",
            "14/14 [==============================] - 1s 87ms/step - loss: 2.9634 - accuracy: 0.5586 - f1: 0.5609 - precision: 0.5678 - recall: 0.5543\n",
            "[2.9634451866149902, 0.558571457862854, 0.5609195828437805, 0.5677722096443176, 0.5542857050895691]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbeb9229-c8d7-41ab-8d57-a6250d2d5a42"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 7s 129ms/step - loss: 1.3906 - accuracy: 0.2667 - f1: 3.5420e-04 - precision: 0.0179 - recall: 1.7887e-04 - val_loss: 1.3755 - val_accuracy: 0.3008 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.3644 - accuracy: 0.3323 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3789 - val_accuracy: 0.3058 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.3443 - accuracy: 0.3568 - f1: 0.0022 - precision: 0.0913 - recall: 0.0011 - val_loss: 1.3813 - val_accuracy: 0.2967 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.3200 - accuracy: 0.3802 - f1: 0.0280 - precision: 0.3892 - recall: 0.0148 - val_loss: 1.3841 - val_accuracy: 0.3008 - val_f1: 0.0208 - val_precision: 0.3125 - val_recall: 0.0108\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2911 - accuracy: 0.4010 - f1: 0.0838 - precision: 0.5228 - recall: 0.0467 - val_loss: 1.3810 - val_accuracy: 0.3175 - val_f1: 0.0539 - val_precision: 0.4297 - val_recall: 0.0292\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2511 - accuracy: 0.4344 - f1: 0.1605 - precision: 0.5922 - recall: 0.0947 - val_loss: 1.4061 - val_accuracy: 0.3217 - val_f1: 0.1055 - val_precision: 0.4095 - val_recall: 0.0608\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.2172 - accuracy: 0.4575 - f1: 0.2188 - precision: 0.5973 - recall: 0.1364 - val_loss: 1.4191 - val_accuracy: 0.3108 - val_f1: 0.1394 - val_precision: 0.3918 - val_recall: 0.0850\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.1938 - accuracy: 0.4654 - f1: 0.2806 - precision: 0.6328 - recall: 0.1810 - val_loss: 1.4386 - val_accuracy: 0.3033 - val_f1: 0.1647 - val_precision: 0.3549 - val_recall: 0.1075\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1371 - accuracy: 0.5067 - f1: 0.3668 - precision: 0.6302 - recall: 0.2598 - val_loss: 1.4759 - val_accuracy: 0.3108 - val_f1: 0.2043 - val_precision: 0.3692 - val_recall: 0.1417\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1039 - accuracy: 0.5280 - f1: 0.4087 - precision: 0.6697 - recall: 0.2950 - val_loss: 1.4836 - val_accuracy: 0.3275 - val_f1: 0.2048 - val_precision: 0.3406 - val_recall: 0.1467\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0658 - accuracy: 0.5565 - f1: 0.4581 - precision: 0.6676 - recall: 0.3498 - val_loss: 1.5432 - val_accuracy: 0.3083 - val_f1: 0.2172 - val_precision: 0.3357 - val_recall: 0.1608\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.0407 - accuracy: 0.5674 - f1: 0.4836 - precision: 0.6764 - recall: 0.3776 - val_loss: 1.5251 - val_accuracy: 0.3300 - val_f1: 0.2371 - val_precision: 0.3599 - val_recall: 0.1775\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.0007 - accuracy: 0.5795 - f1: 0.5018 - precision: 0.6786 - recall: 0.3995 - val_loss: 1.5874 - val_accuracy: 0.3317 - val_f1: 0.2784 - val_precision: 0.3761 - val_recall: 0.2217\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.9904 - accuracy: 0.5922 - f1: 0.5292 - precision: 0.6966 - recall: 0.4276 - val_loss: 1.6119 - val_accuracy: 0.3317 - val_f1: 0.2715 - val_precision: 0.3554 - val_recall: 0.2200\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.9306 - accuracy: 0.6141 - f1: 0.5733 - precision: 0.7206 - recall: 0.4772 - val_loss: 1.6486 - val_accuracy: 0.3508 - val_f1: 0.2905 - val_precision: 0.3690 - val_recall: 0.2400\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.9304 - accuracy: 0.6328 - f1: 0.5826 - precision: 0.7139 - recall: 0.4929 - val_loss: 1.6431 - val_accuracy: 0.3683 - val_f1: 0.3001 - val_precision: 0.3820 - val_recall: 0.2475\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9145 - accuracy: 0.6325 - f1: 0.5896 - precision: 0.7280 - recall: 0.4972 - val_loss: 1.6865 - val_accuracy: 0.3708 - val_f1: 0.3195 - val_precision: 0.3882 - val_recall: 0.2717\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.8949 - accuracy: 0.6316 - f1: 0.5921 - precision: 0.7119 - recall: 0.5074 - val_loss: 1.7553 - val_accuracy: 0.3433 - val_f1: 0.3120 - val_precision: 0.3787 - val_recall: 0.2658\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.8514 - accuracy: 0.6598 - f1: 0.6374 - precision: 0.7458 - recall: 0.5570 - val_loss: 1.7797 - val_accuracy: 0.3617 - val_f1: 0.3177 - val_precision: 0.3869 - val_recall: 0.2700\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.8110 - accuracy: 0.6764 - f1: 0.6485 - precision: 0.7526 - recall: 0.5706 - val_loss: 1.8335 - val_accuracy: 0.3600 - val_f1: 0.3282 - val_precision: 0.3857 - val_recall: 0.2858\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.8145 - accuracy: 0.6755 - f1: 0.6572 - precision: 0.7562 - recall: 0.5817 - val_loss: 1.8276 - val_accuracy: 0.3500 - val_f1: 0.3099 - val_precision: 0.3643 - val_recall: 0.2700\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.8109 - accuracy: 0.6751 - f1: 0.6432 - precision: 0.7455 - recall: 0.5667 - val_loss: 1.8615 - val_accuracy: 0.3333 - val_f1: 0.3099 - val_precision: 0.3654 - val_recall: 0.2692\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.7580 - accuracy: 0.7083 - f1: 0.6848 - precision: 0.7801 - recall: 0.6108 - val_loss: 1.8688 - val_accuracy: 0.3400 - val_f1: 0.3078 - val_precision: 0.3648 - val_recall: 0.2667\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.7538 - accuracy: 0.6925 - f1: 0.6748 - precision: 0.7616 - recall: 0.6062 - val_loss: 1.9493 - val_accuracy: 0.3483 - val_f1: 0.3095 - val_precision: 0.3554 - val_recall: 0.2742\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.7258 - accuracy: 0.7160 - f1: 0.6932 - precision: 0.7717 - recall: 0.6301 - val_loss: 1.9777 - val_accuracy: 0.3433 - val_f1: 0.3220 - val_precision: 0.3639 - val_recall: 0.2892\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.6772 - accuracy: 0.7408 - f1: 0.7262 - precision: 0.8034 - recall: 0.6629 - val_loss: 2.0954 - val_accuracy: 0.3425 - val_f1: 0.3150 - val_precision: 0.3547 - val_recall: 0.2833\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.6839 - accuracy: 0.7387 - f1: 0.7205 - precision: 0.7895 - recall: 0.6631 - val_loss: 2.1065 - val_accuracy: 0.3333 - val_f1: 0.3184 - val_precision: 0.3496 - val_recall: 0.2925\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.6912 - accuracy: 0.7149 - f1: 0.7080 - precision: 0.7691 - recall: 0.6564 - val_loss: 2.0700 - val_accuracy: 0.3300 - val_f1: 0.3259 - val_precision: 0.3569 - val_recall: 0.3000\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.7510 - accuracy: 0.7030 - f1: 0.6887 - precision: 0.7584 - recall: 0.6311 - val_loss: 2.0657 - val_accuracy: 0.3342 - val_f1: 0.3155 - val_precision: 0.3498 - val_recall: 0.2875\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.6370 - accuracy: 0.7547 - f1: 0.7407 - precision: 0.8059 - recall: 0.6858 - val_loss: 2.0764 - val_accuracy: 0.3617 - val_f1: 0.3402 - val_precision: 0.3735 - val_recall: 0.3125\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.6584 - accuracy: 0.7434 - f1: 0.7320 - precision: 0.7936 - recall: 0.6796 - val_loss: 2.0721 - val_accuracy: 0.3417 - val_f1: 0.3242 - val_precision: 0.3642 - val_recall: 0.2925\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.6466 - accuracy: 0.7405 - f1: 0.7351 - precision: 0.7984 - recall: 0.6815 - val_loss: 2.1133 - val_accuracy: 0.3358 - val_f1: 0.3162 - val_precision: 0.3502 - val_recall: 0.2883\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 0.6188 - accuracy: 0.7553 - f1: 0.7435 - precision: 0.8004 - recall: 0.6946 - val_loss: 2.1765 - val_accuracy: 0.3450 - val_f1: 0.3151 - val_precision: 0.3476 - val_recall: 0.2883\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.5973 - accuracy: 0.7603 - f1: 0.7497 - precision: 0.7983 - recall: 0.7071 - val_loss: 2.1490 - val_accuracy: 0.3700 - val_f1: 0.3532 - val_precision: 0.3847 - val_recall: 0.3267\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.5723 - accuracy: 0.7685 - f1: 0.7623 - precision: 0.8099 - recall: 0.7205 - val_loss: 2.2559 - val_accuracy: 0.3300 - val_f1: 0.3211 - val_precision: 0.3491 - val_recall: 0.2975\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.5881 - accuracy: 0.7669 - f1: 0.7544 - precision: 0.8027 - recall: 0.7118 - val_loss: 2.2302 - val_accuracy: 0.3575 - val_f1: 0.3435 - val_precision: 0.3731 - val_recall: 0.3183\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.5607 - accuracy: 0.7864 - f1: 0.7763 - precision: 0.8221 - recall: 0.7357 - val_loss: 2.2555 - val_accuracy: 0.3600 - val_f1: 0.3478 - val_precision: 0.3721 - val_recall: 0.3267\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.5686 - accuracy: 0.7751 - f1: 0.7707 - precision: 0.8171 - recall: 0.7298 - val_loss: 2.3023 - val_accuracy: 0.3567 - val_f1: 0.3437 - val_precision: 0.3702 - val_recall: 0.3208\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.5426 - accuracy: 0.7930 - f1: 0.7873 - precision: 0.8263 - recall: 0.7522 - val_loss: 2.3463 - val_accuracy: 0.3667 - val_f1: 0.3508 - val_precision: 0.3768 - val_recall: 0.3283\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.5303 - accuracy: 0.7905 - f1: 0.7851 - precision: 0.8248 - recall: 0.7492 - val_loss: 2.3890 - val_accuracy: 0.3692 - val_f1: 0.3530 - val_precision: 0.3774 - val_recall: 0.3317\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.5229 - accuracy: 0.7969 - f1: 0.7950 - precision: 0.8291 - recall: 0.7638 - val_loss: 2.4574 - val_accuracy: 0.3700 - val_f1: 0.3584 - val_precision: 0.3802 - val_recall: 0.3392\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4801 - accuracy: 0.8133 - f1: 0.8133 - precision: 0.8455 - recall: 0.7838 - val_loss: 2.5171 - val_accuracy: 0.3550 - val_f1: 0.3481 - val_precision: 0.3719 - val_recall: 0.3275\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4673 - accuracy: 0.8218 - f1: 0.8184 - precision: 0.8516 - recall: 0.7878 - val_loss: 2.6597 - val_accuracy: 0.3642 - val_f1: 0.3556 - val_precision: 0.3709 - val_recall: 0.3417\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4697 - accuracy: 0.8189 - f1: 0.8195 - precision: 0.8523 - recall: 0.7894 - val_loss: 2.8246 - val_accuracy: 0.3525 - val_f1: 0.3386 - val_precision: 0.3575 - val_recall: 0.3217\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4964 - accuracy: 0.8134 - f1: 0.8131 - precision: 0.8421 - recall: 0.7863 - val_loss: 2.6600 - val_accuracy: 0.3792 - val_f1: 0.3688 - val_precision: 0.3839 - val_recall: 0.3550\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.4210 - accuracy: 0.8475 - f1: 0.8473 - precision: 0.8705 - recall: 0.8254 - val_loss: 2.7925 - val_accuracy: 0.3533 - val_f1: 0.3457 - val_precision: 0.3601 - val_recall: 0.3325\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4171 - accuracy: 0.8366 - f1: 0.8381 - precision: 0.8630 - recall: 0.8149 - val_loss: 2.8060 - val_accuracy: 0.3675 - val_f1: 0.3578 - val_precision: 0.3748 - val_recall: 0.3425\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3577 - accuracy: 0.8654 - f1: 0.8621 - precision: 0.8855 - recall: 0.8403 - val_loss: 2.9048 - val_accuracy: 0.3592 - val_f1: 0.3484 - val_precision: 0.3649 - val_recall: 0.3333\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.3852 - accuracy: 0.8507 - f1: 0.8524 - precision: 0.8722 - recall: 0.8336 - val_loss: 2.8507 - val_accuracy: 0.3767 - val_f1: 0.3702 - val_precision: 0.3858 - val_recall: 0.3558\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3831 - accuracy: 0.8544 - f1: 0.8564 - precision: 0.8760 - recall: 0.8378 - val_loss: 2.8688 - val_accuracy: 0.3658 - val_f1: 0.3595 - val_precision: 0.3765 - val_recall: 0.3442\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3664 - accuracy: 0.8545 - f1: 0.8527 - precision: 0.8751 - recall: 0.8317 - val_loss: 2.9660 - val_accuracy: 0.3692 - val_f1: 0.3612 - val_precision: 0.3773 - val_recall: 0.3467\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3521 - accuracy: 0.8685 - f1: 0.8663 - precision: 0.8865 - recall: 0.8471 - val_loss: 3.0490 - val_accuracy: 0.3567 - val_f1: 0.3487 - val_precision: 0.3637 - val_recall: 0.3350\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3721 - accuracy: 0.8517 - f1: 0.8531 - precision: 0.8771 - recall: 0.8307 - val_loss: 2.9700 - val_accuracy: 0.3792 - val_f1: 0.3678 - val_precision: 0.3806 - val_recall: 0.3558\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3747 - accuracy: 0.8588 - f1: 0.8571 - precision: 0.8749 - recall: 0.8403 - val_loss: 2.9577 - val_accuracy: 0.3542 - val_f1: 0.3515 - val_precision: 0.3650 - val_recall: 0.3392\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.3918 - accuracy: 0.8520 - f1: 0.8510 - precision: 0.8693 - recall: 0.8335 - val_loss: 2.9913 - val_accuracy: 0.3492 - val_f1: 0.3389 - val_precision: 0.3541 - val_recall: 0.3250\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.4486 - accuracy: 0.8309 - f1: 0.8280 - precision: 0.8465 - recall: 0.8106 - val_loss: 2.9239 - val_accuracy: 0.3633 - val_f1: 0.3521 - val_precision: 0.3671 - val_recall: 0.3383\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3752 - accuracy: 0.8587 - f1: 0.8559 - precision: 0.8767 - recall: 0.8363 - val_loss: 3.0207 - val_accuracy: 0.3550 - val_f1: 0.3449 - val_precision: 0.3574 - val_recall: 0.3333\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3754 - accuracy: 0.8574 - f1: 0.8592 - precision: 0.8794 - recall: 0.8401 - val_loss: 2.9453 - val_accuracy: 0.3800 - val_f1: 0.3710 - val_precision: 0.3867 - val_recall: 0.3567\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.3531 - accuracy: 0.8593 - f1: 0.8610 - precision: 0.8782 - recall: 0.8446 - val_loss: 3.0081 - val_accuracy: 0.3733 - val_f1: 0.3647 - val_precision: 0.3778 - val_recall: 0.3525\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3139 - accuracy: 0.8839 - f1: 0.8821 - precision: 0.8954 - recall: 0.8694 - val_loss: 3.0670 - val_accuracy: 0.3717 - val_f1: 0.3600 - val_precision: 0.3727 - val_recall: 0.3483\n",
            "14/14 [==============================] - 1s 85ms/step - loss: 2.9979 - accuracy: 0.3807 - f1: 0.3739 - precision: 0.3857 - recall: 0.3629\n",
            "[2.9978854656219482, 0.3807142972946167, 0.3738837242126465, 0.38570716977119446, 0.3628571629524231]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cacb978-fbab-4d5a-98ab-51054baf992c"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(64))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 3s 25ms/step - loss: 1.3796 - accuracy: 0.3045 - f1: 8.1568e-05 - precision: 0.0023 - recall: 4.1731e-05 - val_loss: 1.3000 - val_accuracy: 0.3642 - val_f1: 0.0115 - val_precision: 0.4583 - val_recall: 0.0058\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.2256 - accuracy: 0.4572 - f1: 0.2090 - precision: 0.6578 - recall: 0.1429 - val_loss: 1.0598 - val_accuracy: 0.5583 - val_f1: 0.4897 - val_precision: 0.7206 - val_recall: 0.3725\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 1.0123 - accuracy: 0.5734 - f1: 0.5088 - precision: 0.7064 - recall: 0.4033 - val_loss: 0.9388 - val_accuracy: 0.5958 - val_f1: 0.5555 - val_precision: 0.7649 - val_recall: 0.4367\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.8739 - accuracy: 0.6374 - f1: 0.5925 - precision: 0.7764 - recall: 0.4817 - val_loss: 0.9215 - val_accuracy: 0.6358 - val_f1: 0.5704 - val_precision: 0.8097 - val_recall: 0.4408\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.7880 - accuracy: 0.6935 - f1: 0.6411 - precision: 0.8007 - recall: 0.5380 - val_loss: 0.8143 - val_accuracy: 0.6858 - val_f1: 0.6717 - val_precision: 0.7648 - val_recall: 0.5992\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.6830 - accuracy: 0.7432 - f1: 0.7263 - precision: 0.8214 - recall: 0.6517 - val_loss: 0.8231 - val_accuracy: 0.6917 - val_f1: 0.6836 - val_precision: 0.7480 - val_recall: 0.6300\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.5986 - accuracy: 0.7792 - f1: 0.7716 - precision: 0.8385 - recall: 0.7150 - val_loss: 0.7872 - val_accuracy: 0.7217 - val_f1: 0.7241 - val_precision: 0.7779 - val_recall: 0.6775\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4989 - accuracy: 0.8273 - f1: 0.8187 - precision: 0.8738 - recall: 0.7706 - val_loss: 0.7460 - val_accuracy: 0.7367 - val_f1: 0.7249 - val_precision: 0.7929 - val_recall: 0.6683\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4719 - accuracy: 0.8348 - f1: 0.8312 - precision: 0.8827 - recall: 0.7859 - val_loss: 0.7199 - val_accuracy: 0.7475 - val_f1: 0.7372 - val_precision: 0.7907 - val_recall: 0.6908\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4490 - accuracy: 0.8559 - f1: 0.8457 - precision: 0.8937 - recall: 0.8030 - val_loss: 0.6766 - val_accuracy: 0.7558 - val_f1: 0.7572 - val_precision: 0.8094 - val_recall: 0.7117\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3851 - accuracy: 0.8731 - f1: 0.8700 - precision: 0.9029 - recall: 0.8399 - val_loss: 0.7142 - val_accuracy: 0.7667 - val_f1: 0.7696 - val_precision: 0.8059 - val_recall: 0.7367\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3862 - accuracy: 0.8741 - f1: 0.8724 - precision: 0.9044 - recall: 0.8430 - val_loss: 0.7160 - val_accuracy: 0.7825 - val_f1: 0.7755 - val_precision: 0.8032 - val_recall: 0.7500\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3176 - accuracy: 0.8954 - f1: 0.8961 - precision: 0.9190 - recall: 0.8747 - val_loss: 0.7841 - val_accuracy: 0.7508 - val_f1: 0.7463 - val_precision: 0.7645 - val_recall: 0.7292\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3348 - accuracy: 0.8801 - f1: 0.8784 - precision: 0.9024 - recall: 0.8558 - val_loss: 0.7312 - val_accuracy: 0.7808 - val_f1: 0.7782 - val_precision: 0.7976 - val_recall: 0.7600\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2467 - accuracy: 0.9226 - f1: 0.9237 - precision: 0.9369 - recall: 0.9111 - val_loss: 0.7685 - val_accuracy: 0.7942 - val_f1: 0.7942 - val_precision: 0.8119 - val_recall: 0.7775\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2268 - accuracy: 0.9316 - f1: 0.9307 - precision: 0.9421 - recall: 0.9196 - val_loss: 0.8539 - val_accuracy: 0.7950 - val_f1: 0.7950 - val_precision: 0.8098 - val_recall: 0.7808\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1960 - accuracy: 0.9397 - f1: 0.9402 - precision: 0.9506 - recall: 0.9302 - val_loss: 0.8274 - val_accuracy: 0.7775 - val_f1: 0.7823 - val_precision: 0.7969 - val_recall: 0.7683\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1919 - accuracy: 0.9428 - f1: 0.9413 - precision: 0.9484 - recall: 0.9345 - val_loss: 0.8076 - val_accuracy: 0.7908 - val_f1: 0.7931 - val_precision: 0.8033 - val_recall: 0.7833\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1806 - accuracy: 0.9435 - f1: 0.9426 - precision: 0.9489 - recall: 0.9364 - val_loss: 0.7992 - val_accuracy: 0.7717 - val_f1: 0.7753 - val_precision: 0.7878 - val_recall: 0.7633\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1931 - accuracy: 0.9398 - f1: 0.9403 - precision: 0.9490 - recall: 0.9318 - val_loss: 0.9195 - val_accuracy: 0.7725 - val_f1: 0.7741 - val_precision: 0.7834 - val_recall: 0.7650\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1944 - accuracy: 0.9401 - f1: 0.9391 - precision: 0.9460 - recall: 0.9323 - val_loss: 0.9359 - val_accuracy: 0.7808 - val_f1: 0.7810 - val_precision: 0.7897 - val_recall: 0.7725\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1978 - accuracy: 0.9403 - f1: 0.9395 - precision: 0.9464 - recall: 0.9328 - val_loss: 0.8041 - val_accuracy: 0.7842 - val_f1: 0.7835 - val_precision: 0.7950 - val_recall: 0.7725\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2025 - accuracy: 0.9353 - f1: 0.9345 - precision: 0.9411 - recall: 0.9281 - val_loss: 0.8135 - val_accuracy: 0.7583 - val_f1: 0.7617 - val_precision: 0.7748 - val_recall: 0.7492\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2427 - accuracy: 0.9236 - f1: 0.9241 - precision: 0.9323 - recall: 0.9161 - val_loss: 0.7615 - val_accuracy: 0.7892 - val_f1: 0.7895 - val_precision: 0.8012 - val_recall: 0.7783\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1913 - accuracy: 0.9424 - f1: 0.9403 - precision: 0.9471 - recall: 0.9338 - val_loss: 0.8072 - val_accuracy: 0.7842 - val_f1: 0.7848 - val_precision: 0.7932 - val_recall: 0.7767\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1623 - accuracy: 0.9478 - f1: 0.9483 - precision: 0.9539 - recall: 0.9429 - val_loss: 0.8594 - val_accuracy: 0.7892 - val_f1: 0.7899 - val_precision: 0.7975 - val_recall: 0.7825\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1647 - accuracy: 0.9493 - f1: 0.9501 - precision: 0.9547 - recall: 0.9456 - val_loss: 0.9925 - val_accuracy: 0.7817 - val_f1: 0.7825 - val_precision: 0.7901 - val_recall: 0.7750\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1508 - accuracy: 0.9502 - f1: 0.9508 - precision: 0.9555 - recall: 0.9462 - val_loss: 0.8691 - val_accuracy: 0.8017 - val_f1: 0.8040 - val_precision: 0.8115 - val_recall: 0.7967\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1275 - accuracy: 0.9560 - f1: 0.9569 - precision: 0.9609 - recall: 0.9530 - val_loss: 0.9392 - val_accuracy: 0.7958 - val_f1: 0.7967 - val_precision: 0.8001 - val_recall: 0.7933\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1474 - accuracy: 0.9500 - f1: 0.9506 - precision: 0.9530 - recall: 0.9482 - val_loss: 0.9100 - val_accuracy: 0.7983 - val_f1: 0.7981 - val_precision: 0.8022 - val_recall: 0.7942\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1190 - accuracy: 0.9607 - f1: 0.9620 - precision: 0.9651 - recall: 0.9589 - val_loss: 0.9832 - val_accuracy: 0.8042 - val_f1: 0.8018 - val_precision: 0.8062 - val_recall: 0.7975\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1048 - accuracy: 0.9661 - f1: 0.9664 - precision: 0.9681 - recall: 0.9648 - val_loss: 1.0056 - val_accuracy: 0.8075 - val_f1: 0.8069 - val_precision: 0.8114 - val_recall: 0.8025\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1162 - accuracy: 0.9576 - f1: 0.9585 - precision: 0.9607 - recall: 0.9565 - val_loss: 1.0653 - val_accuracy: 0.7958 - val_f1: 0.7980 - val_precision: 0.8010 - val_recall: 0.7950\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1081 - accuracy: 0.9623 - f1: 0.9626 - precision: 0.9637 - recall: 0.9615 - val_loss: 0.9656 - val_accuracy: 0.7950 - val_f1: 0.7957 - val_precision: 0.7981 - val_recall: 0.7933\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0910 - accuracy: 0.9673 - f1: 0.9670 - precision: 0.9689 - recall: 0.9651 - val_loss: 1.0519 - val_accuracy: 0.7992 - val_f1: 0.7988 - val_precision: 0.8018 - val_recall: 0.7958\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0848 - accuracy: 0.9708 - f1: 0.9715 - precision: 0.9733 - recall: 0.9697 - val_loss: 1.0515 - val_accuracy: 0.8050 - val_f1: 0.8041 - val_precision: 0.8083 - val_recall: 0.8000\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0853 - accuracy: 0.9681 - f1: 0.9680 - precision: 0.9688 - recall: 0.9672 - val_loss: 1.0426 - val_accuracy: 0.8092 - val_f1: 0.8099 - val_precision: 0.8140 - val_recall: 0.8058\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0758 - accuracy: 0.9718 - f1: 0.9708 - precision: 0.9727 - recall: 0.9689 - val_loss: 1.1785 - val_accuracy: 0.7967 - val_f1: 0.7942 - val_precision: 0.7986 - val_recall: 0.7900\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0657 - accuracy: 0.9748 - f1: 0.9741 - precision: 0.9752 - recall: 0.9730 - val_loss: 1.1010 - val_accuracy: 0.8100 - val_f1: 0.8102 - val_precision: 0.8130 - val_recall: 0.8075\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0734 - accuracy: 0.9688 - f1: 0.9692 - precision: 0.9710 - recall: 0.9674 - val_loss: 1.1455 - val_accuracy: 0.8008 - val_f1: 0.8020 - val_precision: 0.8041 - val_recall: 0.8000\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0680 - accuracy: 0.9705 - f1: 0.9715 - precision: 0.9737 - recall: 0.9692 - val_loss: 1.2265 - val_accuracy: 0.7892 - val_f1: 0.7905 - val_precision: 0.7936 - val_recall: 0.7875\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0676 - accuracy: 0.9709 - f1: 0.9709 - precision: 0.9733 - recall: 0.9685 - val_loss: 1.2786 - val_accuracy: 0.7942 - val_f1: 0.7921 - val_precision: 0.7967 - val_recall: 0.7875\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0767 - accuracy: 0.9710 - f1: 0.9709 - precision: 0.9742 - recall: 0.9677 - val_loss: 1.1842 - val_accuracy: 0.7942 - val_f1: 0.7946 - val_precision: 0.7977 - val_recall: 0.7917\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0696 - accuracy: 0.9686 - f1: 0.9694 - precision: 0.9719 - recall: 0.9669 - val_loss: 1.1720 - val_accuracy: 0.7900 - val_f1: 0.7905 - val_precision: 0.7979 - val_recall: 0.7833\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1004 - accuracy: 0.9635 - f1: 0.9645 - precision: 0.9679 - recall: 0.9612 - val_loss: 1.0733 - val_accuracy: 0.7858 - val_f1: 0.7874 - val_precision: 0.7925 - val_recall: 0.7825\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0874 - accuracy: 0.9665 - f1: 0.9681 - precision: 0.9716 - recall: 0.9646 - val_loss: 1.0527 - val_accuracy: 0.7875 - val_f1: 0.7870 - val_precision: 0.7907 - val_recall: 0.7833\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0946 - accuracy: 0.9615 - f1: 0.9633 - precision: 0.9667 - recall: 0.9600 - val_loss: 1.0232 - val_accuracy: 0.7983 - val_f1: 0.7954 - val_precision: 0.8018 - val_recall: 0.7892\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0689 - accuracy: 0.9737 - f1: 0.9738 - precision: 0.9769 - recall: 0.9708 - val_loss: 1.0799 - val_accuracy: 0.7983 - val_f1: 0.7963 - val_precision: 0.8011 - val_recall: 0.7917\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0710 - accuracy: 0.9701 - f1: 0.9708 - precision: 0.9739 - recall: 0.9677 - val_loss: 1.0919 - val_accuracy: 0.7942 - val_f1: 0.7955 - val_precision: 0.8003 - val_recall: 0.7908\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0644 - accuracy: 0.9694 - f1: 0.9685 - precision: 0.9709 - recall: 0.9661 - val_loss: 1.0227 - val_accuracy: 0.8125 - val_f1: 0.8115 - val_precision: 0.8164 - val_recall: 0.8067\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0902 - accuracy: 0.9658 - f1: 0.9648 - precision: 0.9688 - recall: 0.9610 - val_loss: 0.9436 - val_accuracy: 0.8075 - val_f1: 0.8084 - val_precision: 0.8118 - val_recall: 0.8050\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0789 - accuracy: 0.9649 - f1: 0.9661 - precision: 0.9692 - recall: 0.9631 - val_loss: 1.0734 - val_accuracy: 0.7842 - val_f1: 0.7868 - val_precision: 0.7911 - val_recall: 0.7825\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0602 - accuracy: 0.9746 - f1: 0.9749 - precision: 0.9780 - recall: 0.9718 - val_loss: 1.0100 - val_accuracy: 0.8083 - val_f1: 0.8084 - val_precision: 0.8135 - val_recall: 0.8033\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0697 - accuracy: 0.9699 - f1: 0.9699 - precision: 0.9729 - recall: 0.9670 - val_loss: 1.1263 - val_accuracy: 0.8017 - val_f1: 0.7991 - val_precision: 0.8032 - val_recall: 0.7950\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0627 - accuracy: 0.9689 - f1: 0.9695 - precision: 0.9721 - recall: 0.9669 - val_loss: 1.1594 - val_accuracy: 0.7958 - val_f1: 0.7980 - val_precision: 0.8010 - val_recall: 0.7950\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0545 - accuracy: 0.9734 - f1: 0.9740 - precision: 0.9764 - recall: 0.9716 - val_loss: 1.0886 - val_accuracy: 0.8042 - val_f1: 0.8048 - val_precision: 0.8089 - val_recall: 0.8008\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0585 - accuracy: 0.9754 - f1: 0.9742 - precision: 0.9782 - recall: 0.9702 - val_loss: 1.2116 - val_accuracy: 0.7942 - val_f1: 0.7970 - val_precision: 0.8008 - val_recall: 0.7933\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0605 - accuracy: 0.9746 - f1: 0.9743 - precision: 0.9762 - recall: 0.9724 - val_loss: 1.2575 - val_accuracy: 0.7958 - val_f1: 0.7956 - val_precision: 0.7996 - val_recall: 0.7917\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0526 - accuracy: 0.9745 - f1: 0.9746 - precision: 0.9764 - recall: 0.9730 - val_loss: 1.1563 - val_accuracy: 0.7900 - val_f1: 0.7898 - val_precision: 0.7921 - val_recall: 0.7875\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0561 - accuracy: 0.9761 - f1: 0.9756 - precision: 0.9783 - recall: 0.9729 - val_loss: 1.2827 - val_accuracy: 0.7858 - val_f1: 0.7855 - val_precision: 0.7868 - val_recall: 0.7842\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2028 - accuracy: 0.8021 - f1: 0.8013 - precision: 0.8033 - recall: 0.7993\n",
            "[1.2027506828308105, 0.802142858505249, 0.801302433013916, 0.8033462166786194, 0.7992857098579407]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8c28e8-f260-4699-8bd9-9b8ef6b4b226"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 4s 30ms/step - loss: 1.3649 - accuracy: 0.3217 - f1: 0.0038 - precision: 0.0792 - recall: 0.0020 - val_loss: 1.2283 - val_accuracy: 0.4633 - val_f1: 0.0709 - val_precision: 0.7313 - val_recall: 0.0375\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 1.1486 - accuracy: 0.5133 - f1: 0.2931 - precision: 0.6634 - recall: 0.2013 - val_loss: 0.9496 - val_accuracy: 0.6150 - val_f1: 0.5743 - val_precision: 0.7106 - val_recall: 0.4825\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.8815 - accuracy: 0.6491 - f1: 0.6161 - precision: 0.7236 - recall: 0.5372 - val_loss: 0.8275 - val_accuracy: 0.6583 - val_f1: 0.6469 - val_precision: 0.7292 - val_recall: 0.5825\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.7157 - accuracy: 0.7258 - f1: 0.7138 - precision: 0.7966 - recall: 0.6483 - val_loss: 0.8057 - val_accuracy: 0.6658 - val_f1: 0.6596 - val_precision: 0.7235 - val_recall: 0.6067\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.6177 - accuracy: 0.7726 - f1: 0.7579 - precision: 0.8133 - recall: 0.7102 - val_loss: 0.7918 - val_accuracy: 0.6825 - val_f1: 0.6717 - val_precision: 0.7199 - val_recall: 0.6300\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.5684 - accuracy: 0.7929 - f1: 0.7802 - precision: 0.8313 - recall: 0.7359 - val_loss: 0.7553 - val_accuracy: 0.7058 - val_f1: 0.7030 - val_precision: 0.7459 - val_recall: 0.6650\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.4963 - accuracy: 0.8191 - f1: 0.8184 - precision: 0.8583 - recall: 0.7824 - val_loss: 0.7405 - val_accuracy: 0.7258 - val_f1: 0.7228 - val_precision: 0.7608 - val_recall: 0.6892\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4289 - accuracy: 0.8439 - f1: 0.8395 - precision: 0.8718 - recall: 0.8098 - val_loss: 0.7917 - val_accuracy: 0.7208 - val_f1: 0.7209 - val_precision: 0.7435 - val_recall: 0.7000\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4148 - accuracy: 0.8533 - f1: 0.8531 - precision: 0.8801 - recall: 0.8280 - val_loss: 0.7787 - val_accuracy: 0.7425 - val_f1: 0.7394 - val_precision: 0.7564 - val_recall: 0.7233\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.3471 - accuracy: 0.8771 - f1: 0.8773 - precision: 0.8960 - recall: 0.8595 - val_loss: 0.7260 - val_accuracy: 0.7617 - val_f1: 0.7589 - val_precision: 0.7717 - val_recall: 0.7467\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2722 - accuracy: 0.9014 - f1: 0.8991 - precision: 0.9125 - recall: 0.8863 - val_loss: 0.8084 - val_accuracy: 0.7592 - val_f1: 0.7585 - val_precision: 0.7691 - val_recall: 0.7483\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2153 - accuracy: 0.9256 - f1: 0.9236 - precision: 0.9324 - recall: 0.9150 - val_loss: 0.8370 - val_accuracy: 0.7558 - val_f1: 0.7594 - val_precision: 0.7655 - val_recall: 0.7533\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1813 - accuracy: 0.9359 - f1: 0.9372 - precision: 0.9458 - recall: 0.9289 - val_loss: 0.8414 - val_accuracy: 0.7333 - val_f1: 0.7368 - val_precision: 0.7472 - val_recall: 0.7267\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1844 - accuracy: 0.9284 - f1: 0.9284 - precision: 0.9348 - recall: 0.9221 - val_loss: 0.8558 - val_accuracy: 0.7583 - val_f1: 0.7603 - val_precision: 0.7701 - val_recall: 0.7508\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1815 - accuracy: 0.9335 - f1: 0.9336 - precision: 0.9391 - recall: 0.9283 - val_loss: 0.9613 - val_accuracy: 0.7533 - val_f1: 0.7547 - val_precision: 0.7666 - val_recall: 0.7433\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1574 - accuracy: 0.9390 - f1: 0.9393 - precision: 0.9444 - recall: 0.9343 - val_loss: 0.9317 - val_accuracy: 0.7592 - val_f1: 0.7583 - val_precision: 0.7694 - val_recall: 0.7475\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1482 - accuracy: 0.9493 - f1: 0.9489 - precision: 0.9531 - recall: 0.9449 - val_loss: 1.0073 - val_accuracy: 0.7408 - val_f1: 0.7431 - val_precision: 0.7533 - val_recall: 0.7333\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1186 - accuracy: 0.9546 - f1: 0.9554 - precision: 0.9593 - recall: 0.9517 - val_loss: 1.0816 - val_accuracy: 0.7458 - val_f1: 0.7468 - val_precision: 0.7565 - val_recall: 0.7375\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1382 - accuracy: 0.9464 - f1: 0.9473 - precision: 0.9505 - recall: 0.9441 - val_loss: 1.0305 - val_accuracy: 0.7533 - val_f1: 0.7510 - val_precision: 0.7581 - val_recall: 0.7442\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1211 - accuracy: 0.9512 - f1: 0.9507 - precision: 0.9540 - recall: 0.9474 - val_loss: 0.9589 - val_accuracy: 0.7633 - val_f1: 0.7662 - val_precision: 0.7708 - val_recall: 0.7617\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1183 - accuracy: 0.9517 - f1: 0.9515 - precision: 0.9542 - recall: 0.9489 - val_loss: 0.9396 - val_accuracy: 0.7700 - val_f1: 0.7710 - val_precision: 0.7788 - val_recall: 0.7633\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1046 - accuracy: 0.9606 - f1: 0.9611 - precision: 0.9628 - recall: 0.9594 - val_loss: 0.9657 - val_accuracy: 0.7808 - val_f1: 0.7810 - val_precision: 0.7862 - val_recall: 0.7758\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0993 - accuracy: 0.9620 - f1: 0.9634 - precision: 0.9664 - recall: 0.9604 - val_loss: 0.9410 - val_accuracy: 0.7925 - val_f1: 0.7949 - val_precision: 0.8000 - val_recall: 0.7900\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0858 - accuracy: 0.9645 - f1: 0.9639 - precision: 0.9647 - recall: 0.9631 - val_loss: 0.9844 - val_accuracy: 0.7767 - val_f1: 0.7804 - val_precision: 0.7867 - val_recall: 0.7742\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0771 - accuracy: 0.9703 - f1: 0.9705 - precision: 0.9713 - recall: 0.9698 - val_loss: 1.0062 - val_accuracy: 0.7775 - val_f1: 0.7792 - val_precision: 0.7836 - val_recall: 0.7750\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0698 - accuracy: 0.9723 - f1: 0.9723 - precision: 0.9729 - recall: 0.9717 - val_loss: 1.0003 - val_accuracy: 0.7808 - val_f1: 0.7806 - val_precision: 0.7873 - val_recall: 0.7742\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0693 - accuracy: 0.9686 - f1: 0.9683 - precision: 0.9691 - recall: 0.9675 - val_loss: 1.0073 - val_accuracy: 0.7742 - val_f1: 0.7763 - val_precision: 0.7802 - val_recall: 0.7725\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0639 - accuracy: 0.9728 - f1: 0.9724 - precision: 0.9728 - recall: 0.9719 - val_loss: 1.0403 - val_accuracy: 0.7858 - val_f1: 0.7839 - val_precision: 0.7862 - val_recall: 0.7817\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0566 - accuracy: 0.9734 - f1: 0.9733 - precision: 0.9736 - recall: 0.9730 - val_loss: 1.0231 - val_accuracy: 0.7867 - val_f1: 0.7897 - val_precision: 0.7972 - val_recall: 0.7825\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0527 - accuracy: 0.9763 - f1: 0.9759 - precision: 0.9766 - recall: 0.9752 - val_loss: 1.0657 - val_accuracy: 0.7917 - val_f1: 0.7938 - val_precision: 0.7995 - val_recall: 0.7883\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0479 - accuracy: 0.9792 - f1: 0.9795 - precision: 0.9803 - recall: 0.9788 - val_loss: 1.0758 - val_accuracy: 0.7900 - val_f1: 0.7898 - val_precision: 0.7938 - val_recall: 0.7858\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0485 - accuracy: 0.9768 - f1: 0.9763 - precision: 0.9770 - recall: 0.9757 - val_loss: 1.0702 - val_accuracy: 0.8000 - val_f1: 0.8015 - val_precision: 0.8055 - val_recall: 0.7975\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0512 - accuracy: 0.9740 - f1: 0.9740 - precision: 0.9740 - recall: 0.9740 - val_loss: 1.0775 - val_accuracy: 0.7933 - val_f1: 0.7961 - val_precision: 0.7998 - val_recall: 0.7925\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0479 - accuracy: 0.9729 - f1: 0.9730 - precision: 0.9738 - recall: 0.9722 - val_loss: 1.0454 - val_accuracy: 0.8067 - val_f1: 0.8062 - val_precision: 0.8101 - val_recall: 0.8025\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0429 - accuracy: 0.9753 - f1: 0.9745 - precision: 0.9753 - recall: 0.9738 - val_loss: 1.0964 - val_accuracy: 0.7683 - val_f1: 0.7660 - val_precision: 0.7722 - val_recall: 0.7600\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0685 - accuracy: 0.9684 - f1: 0.9687 - precision: 0.9694 - recall: 0.9680 - val_loss: 1.1688 - val_accuracy: 0.7850 - val_f1: 0.7837 - val_precision: 0.7883 - val_recall: 0.7792\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0502 - accuracy: 0.9756 - f1: 0.9751 - precision: 0.9761 - recall: 0.9741 - val_loss: 1.0877 - val_accuracy: 0.7867 - val_f1: 0.7872 - val_precision: 0.7928 - val_recall: 0.7817\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0423 - accuracy: 0.9751 - f1: 0.9752 - precision: 0.9759 - recall: 0.9745 - val_loss: 1.1380 - val_accuracy: 0.7900 - val_f1: 0.7911 - val_precision: 0.7947 - val_recall: 0.7875\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0400 - accuracy: 0.9781 - f1: 0.9780 - precision: 0.9784 - recall: 0.9777 - val_loss: 1.1853 - val_accuracy: 0.7883 - val_f1: 0.7904 - val_precision: 0.7951 - val_recall: 0.7858\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0380 - accuracy: 0.9775 - f1: 0.9773 - precision: 0.9776 - recall: 0.9770 - val_loss: 1.1565 - val_accuracy: 0.7842 - val_f1: 0.7849 - val_precision: 0.7899 - val_recall: 0.7800\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0373 - accuracy: 0.9772 - f1: 0.9770 - precision: 0.9772 - recall: 0.9768 - val_loss: 1.2154 - val_accuracy: 0.7825 - val_f1: 0.7814 - val_precision: 0.7854 - val_recall: 0.7775\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0335 - accuracy: 0.9822 - f1: 0.9817 - precision: 0.9823 - recall: 0.9812 - val_loss: 1.2363 - val_accuracy: 0.7858 - val_f1: 0.7873 - val_precision: 0.7913 - val_recall: 0.7833\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0339 - accuracy: 0.9760 - f1: 0.9760 - precision: 0.9764 - recall: 0.9756 - val_loss: 1.2270 - val_accuracy: 0.7900 - val_f1: 0.7886 - val_precision: 0.7922 - val_recall: 0.7850\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0330 - accuracy: 0.9789 - f1: 0.9795 - precision: 0.9800 - recall: 0.9789 - val_loss: 1.2789 - val_accuracy: 0.7858 - val_f1: 0.7869 - val_precision: 0.7896 - val_recall: 0.7842\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0332 - accuracy: 0.9806 - f1: 0.9805 - precision: 0.9807 - recall: 0.9803 - val_loss: 1.1943 - val_accuracy: 0.8017 - val_f1: 0.8025 - val_precision: 0.8059 - val_recall: 0.7992\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0407 - accuracy: 0.9772 - f1: 0.9774 - precision: 0.9776 - recall: 0.9772 - val_loss: 1.3470 - val_accuracy: 0.7850 - val_f1: 0.7858 - val_precision: 0.7891 - val_recall: 0.7825\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0479 - accuracy: 0.9743 - f1: 0.9739 - precision: 0.9745 - recall: 0.9734 - val_loss: 1.1977 - val_accuracy: 0.7800 - val_f1: 0.7827 - val_precision: 0.7897 - val_recall: 0.7758\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0449 - accuracy: 0.9787 - f1: 0.9786 - precision: 0.9792 - recall: 0.9780 - val_loss: 1.2205 - val_accuracy: 0.7775 - val_f1: 0.7776 - val_precision: 0.7803 - val_recall: 0.7750\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0602 - accuracy: 0.9692 - f1: 0.9694 - precision: 0.9699 - recall: 0.9688 - val_loss: 1.1489 - val_accuracy: 0.7658 - val_f1: 0.7653 - val_precision: 0.7725 - val_recall: 0.7583\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0803 - accuracy: 0.9649 - f1: 0.9651 - precision: 0.9657 - recall: 0.9644 - val_loss: 1.1163 - val_accuracy: 0.7783 - val_f1: 0.7819 - val_precision: 0.7873 - val_recall: 0.7767\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0547 - accuracy: 0.9743 - f1: 0.9742 - precision: 0.9744 - recall: 0.9740 - val_loss: 1.3231 - val_accuracy: 0.7633 - val_f1: 0.7651 - val_precision: 0.7686 - val_recall: 0.7617\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0640 - accuracy: 0.9708 - f1: 0.9704 - precision: 0.9711 - recall: 0.9697 - val_loss: 1.1279 - val_accuracy: 0.7817 - val_f1: 0.7829 - val_precision: 0.7858 - val_recall: 0.7800\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0532 - accuracy: 0.9738 - f1: 0.9737 - precision: 0.9741 - recall: 0.9733 - val_loss: 1.1614 - val_accuracy: 0.7867 - val_f1: 0.7878 - val_precision: 0.7942 - val_recall: 0.7817\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0503 - accuracy: 0.9725 - f1: 0.9723 - precision: 0.9727 - recall: 0.9719 - val_loss: 1.1020 - val_accuracy: 0.7933 - val_f1: 0.7926 - val_precision: 0.7969 - val_recall: 0.7883\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0462 - accuracy: 0.9739 - f1: 0.9736 - precision: 0.9744 - recall: 0.9728 - val_loss: 1.1602 - val_accuracy: 0.7958 - val_f1: 0.7965 - val_precision: 0.7988 - val_recall: 0.7942\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0443 - accuracy: 0.9733 - f1: 0.9735 - precision: 0.9741 - recall: 0.9729 - val_loss: 1.1326 - val_accuracy: 0.8083 - val_f1: 0.8086 - val_precision: 0.8090 - val_recall: 0.8083\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0400 - accuracy: 0.9754 - f1: 0.9757 - precision: 0.9761 - recall: 0.9754 - val_loss: 1.2305 - val_accuracy: 0.8042 - val_f1: 0.8049 - val_precision: 0.8056 - val_recall: 0.8042\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0374 - accuracy: 0.9766 - f1: 0.9764 - precision: 0.9769 - recall: 0.9758 - val_loss: 1.2849 - val_accuracy: 0.8050 - val_f1: 0.8028 - val_precision: 0.8048 - val_recall: 0.8008\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0403 - accuracy: 0.9770 - f1: 0.9769 - precision: 0.9774 - recall: 0.9764 - val_loss: 1.2529 - val_accuracy: 0.7967 - val_f1: 0.7983 - val_precision: 0.8000 - val_recall: 0.7967\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0341 - accuracy: 0.9766 - f1: 0.9766 - precision: 0.9773 - recall: 0.9759 - val_loss: 1.2771 - val_accuracy: 0.7958 - val_f1: 0.7956 - val_precision: 0.7997 - val_recall: 0.7917\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2009 - accuracy: 0.7993 - f1: 0.8007 - precision: 0.8036 - recall: 0.7979\n",
            "[1.2009376287460327, 0.7992857098579407, 0.8007117509841919, 0.8036202192306519, 0.7978571653366089]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74ee628-f7da-4601-94e9-73b7de2d840e"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 3s 37ms/step - loss: 1.3968 - accuracy: 0.2615 - f1: 7.2491e-04 - precision: 0.0366 - recall: 3.6608e-04 - val_loss: 1.3634 - val_accuracy: 0.3342 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 33ms/step - loss: 1.3598 - accuracy: 0.3421 - f1: 6.3921e-04 - precision: 0.0093 - recall: 3.4333e-04 - val_loss: 1.3008 - val_accuracy: 0.4025 - val_f1: 0.0431 - val_precision: 0.5645 - val_recall: 0.0225\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 1.2989 - accuracy: 0.3880 - f1: 0.1157 - precision: 0.5188 - recall: 0.0691 - val_loss: 1.2454 - val_accuracy: 0.4383 - val_f1: 0.3765 - val_precision: 0.5424 - val_recall: 0.2892\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 1.2005 - accuracy: 0.4774 - f1: 0.2930 - precision: 0.5979 - recall: 0.1979 - val_loss: 1.2264 - val_accuracy: 0.4542 - val_f1: 0.3918 - val_precision: 0.5556 - val_recall: 0.3033\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 1.1277 - accuracy: 0.5188 - f1: 0.3640 - precision: 0.6822 - recall: 0.2535 - val_loss: 1.1987 - val_accuracy: 0.4833 - val_f1: 0.3782 - val_precision: 0.6401 - val_recall: 0.2692\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 1.0807 - accuracy: 0.5524 - f1: 0.4312 - precision: 0.7110 - recall: 0.3131 - val_loss: 1.2445 - val_accuracy: 0.4525 - val_f1: 0.3856 - val_precision: 0.5687 - val_recall: 0.2933\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.0634 - accuracy: 0.5495 - f1: 0.4620 - precision: 0.6906 - recall: 0.3502 - val_loss: 1.2495 - val_accuracy: 0.4600 - val_f1: 0.4194 - val_precision: 0.5499 - val_recall: 0.3400\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.0297 - accuracy: 0.5812 - f1: 0.4968 - precision: 0.6856 - recall: 0.3918 - val_loss: 1.2401 - val_accuracy: 0.4442 - val_f1: 0.3711 - val_precision: 0.5760 - val_recall: 0.2742\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.9845 - accuracy: 0.6001 - f1: 0.5192 - precision: 0.7229 - recall: 0.4072 - val_loss: 1.2365 - val_accuracy: 0.4783 - val_f1: 0.4182 - val_precision: 0.5671 - val_recall: 0.3317\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.0042 - accuracy: 0.5773 - f1: 0.4963 - precision: 0.6937 - recall: 0.3895 - val_loss: 1.2884 - val_accuracy: 0.4825 - val_f1: 0.4589 - val_precision: 0.5402 - val_recall: 0.3992\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.9587 - accuracy: 0.6022 - f1: 0.5551 - precision: 0.7150 - recall: 0.4553 - val_loss: 1.3275 - val_accuracy: 0.4750 - val_f1: 0.4510 - val_precision: 0.5289 - val_recall: 0.3933\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.9237 - accuracy: 0.6339 - f1: 0.5856 - precision: 0.7247 - recall: 0.4931 - val_loss: 1.3831 - val_accuracy: 0.4683 - val_f1: 0.4487 - val_precision: 0.5287 - val_recall: 0.3900\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.8698 - accuracy: 0.6514 - f1: 0.6116 - precision: 0.7357 - recall: 0.5247 - val_loss: 1.3715 - val_accuracy: 0.4600 - val_f1: 0.4420 - val_precision: 0.5196 - val_recall: 0.3850\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.7934 - accuracy: 0.6943 - f1: 0.6577 - precision: 0.7658 - recall: 0.5770 - val_loss: 1.4053 - val_accuracy: 0.4475 - val_f1: 0.4191 - val_precision: 0.4909 - val_recall: 0.3658\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.7599 - accuracy: 0.7008 - f1: 0.6757 - precision: 0.7778 - recall: 0.5984 - val_loss: 1.5123 - val_accuracy: 0.4492 - val_f1: 0.4424 - val_precision: 0.4997 - val_recall: 0.3975\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.7401 - accuracy: 0.7141 - f1: 0.6950 - precision: 0.7771 - recall: 0.6295 - val_loss: 1.5547 - val_accuracy: 0.4425 - val_f1: 0.4380 - val_precision: 0.4886 - val_recall: 0.3975\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.6948 - accuracy: 0.7313 - f1: 0.7158 - precision: 0.7925 - recall: 0.6534 - val_loss: 1.7205 - val_accuracy: 0.3942 - val_f1: 0.3817 - val_precision: 0.4203 - val_recall: 0.3500\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.7182 - accuracy: 0.7271 - f1: 0.7020 - precision: 0.7677 - recall: 0.6475 - val_loss: 1.6655 - val_accuracy: 0.4442 - val_f1: 0.4333 - val_precision: 0.4688 - val_recall: 0.4033\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.7986 - accuracy: 0.6909 - f1: 0.6707 - precision: 0.7481 - recall: 0.6090 - val_loss: 1.7055 - val_accuracy: 0.4542 - val_f1: 0.4496 - val_precision: 0.4819 - val_recall: 0.4217\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.7241 - accuracy: 0.7248 - f1: 0.7007 - precision: 0.7685 - recall: 0.6450 - val_loss: 1.6915 - val_accuracy: 0.4325 - val_f1: 0.4207 - val_precision: 0.4579 - val_recall: 0.3892\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.6877 - accuracy: 0.7267 - f1: 0.7159 - precision: 0.7814 - recall: 0.6613 - val_loss: 1.6190 - val_accuracy: 0.4400 - val_f1: 0.4112 - val_precision: 0.4616 - val_recall: 0.3708\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.6091 - accuracy: 0.7729 - f1: 0.7618 - precision: 0.8234 - recall: 0.7094 - val_loss: 1.6799 - val_accuracy: 0.4325 - val_f1: 0.4196 - val_precision: 0.4674 - val_recall: 0.3808\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4880 - accuracy: 0.8244 - f1: 0.8156 - precision: 0.8649 - recall: 0.7719 - val_loss: 1.8246 - val_accuracy: 0.4283 - val_f1: 0.4121 - val_precision: 0.4514 - val_recall: 0.3792\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4392 - accuracy: 0.8395 - f1: 0.8361 - precision: 0.8708 - recall: 0.8043 - val_loss: 1.9713 - val_accuracy: 0.3733 - val_f1: 0.3686 - val_precision: 0.3983 - val_recall: 0.3433\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4299 - accuracy: 0.8377 - f1: 0.8382 - precision: 0.8680 - recall: 0.8106 - val_loss: 2.0325 - val_accuracy: 0.3742 - val_f1: 0.3724 - val_precision: 0.4001 - val_recall: 0.3483\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4188 - accuracy: 0.8456 - f1: 0.8416 - precision: 0.8703 - recall: 0.8150 - val_loss: 2.0832 - val_accuracy: 0.3917 - val_f1: 0.3887 - val_precision: 0.4191 - val_recall: 0.3625\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3969 - accuracy: 0.8482 - f1: 0.8477 - precision: 0.8760 - recall: 0.8214 - val_loss: 2.1010 - val_accuracy: 0.4050 - val_f1: 0.3954 - val_precision: 0.4183 - val_recall: 0.3750\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3744 - accuracy: 0.8614 - f1: 0.8596 - precision: 0.8823 - recall: 0.8383 - val_loss: 2.1965 - val_accuracy: 0.3792 - val_f1: 0.3668 - val_precision: 0.3884 - val_recall: 0.3475\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.3968 - accuracy: 0.8535 - f1: 0.8520 - precision: 0.8763 - recall: 0.8292 - val_loss: 2.2326 - val_accuracy: 0.4142 - val_f1: 0.4009 - val_precision: 0.4254 - val_recall: 0.3792\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4624 - accuracy: 0.8278 - f1: 0.8269 - precision: 0.8503 - recall: 0.8049 - val_loss: 2.1357 - val_accuracy: 0.4025 - val_f1: 0.3836 - val_precision: 0.4085 - val_recall: 0.3617\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4549 - accuracy: 0.8371 - f1: 0.8323 - precision: 0.8591 - recall: 0.8075 - val_loss: 2.0476 - val_accuracy: 0.4417 - val_f1: 0.4314 - val_precision: 0.4513 - val_recall: 0.4133\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3953 - accuracy: 0.8614 - f1: 0.8587 - precision: 0.8824 - recall: 0.8365 - val_loss: 2.1866 - val_accuracy: 0.4592 - val_f1: 0.4574 - val_precision: 0.4756 - val_recall: 0.4408\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4097 - accuracy: 0.8488 - f1: 0.8461 - precision: 0.8731 - recall: 0.8210 - val_loss: 2.1560 - val_accuracy: 0.4292 - val_f1: 0.4268 - val_precision: 0.4472 - val_recall: 0.4083\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4067 - accuracy: 0.8499 - f1: 0.8479 - precision: 0.8743 - recall: 0.8232 - val_loss: 2.2636 - val_accuracy: 0.4292 - val_f1: 0.4195 - val_precision: 0.4363 - val_recall: 0.4042\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4402 - accuracy: 0.8393 - f1: 0.8456 - precision: 0.8755 - recall: 0.8179 - val_loss: 2.2054 - val_accuracy: 0.3925 - val_f1: 0.3770 - val_precision: 0.4051 - val_recall: 0.3525\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.6121 - accuracy: 0.7805 - f1: 0.7797 - precision: 0.8110 - recall: 0.7512 - val_loss: 2.0248 - val_accuracy: 0.3675 - val_f1: 0.3557 - val_precision: 0.3894 - val_recall: 0.3275\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.5286 - accuracy: 0.8104 - f1: 0.8055 - precision: 0.8414 - recall: 0.7730 - val_loss: 2.0945 - val_accuracy: 0.4042 - val_f1: 0.3848 - val_precision: 0.4113 - val_recall: 0.3617\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4306 - accuracy: 0.8489 - f1: 0.8432 - precision: 0.8708 - recall: 0.8175 - val_loss: 2.1670 - val_accuracy: 0.4042 - val_f1: 0.3909 - val_precision: 0.4134 - val_recall: 0.3708\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3737 - accuracy: 0.8583 - f1: 0.8622 - precision: 0.8860 - recall: 0.8398 - val_loss: 2.2572 - val_accuracy: 0.3917 - val_f1: 0.3848 - val_precision: 0.4063 - val_recall: 0.3658\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3214 - accuracy: 0.8843 - f1: 0.8835 - precision: 0.9005 - recall: 0.8674 - val_loss: 2.3249 - val_accuracy: 0.4242 - val_f1: 0.4142 - val_precision: 0.4315 - val_recall: 0.3983\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2774 - accuracy: 0.8918 - f1: 0.8913 - precision: 0.9052 - recall: 0.8779 - val_loss: 2.4268 - val_accuracy: 0.4442 - val_f1: 0.4416 - val_precision: 0.4578 - val_recall: 0.4267\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2477 - accuracy: 0.9142 - f1: 0.9117 - precision: 0.9225 - recall: 0.9012 - val_loss: 2.4441 - val_accuracy: 0.4342 - val_f1: 0.4265 - val_precision: 0.4436 - val_recall: 0.4108\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2562 - accuracy: 0.9038 - f1: 0.9051 - precision: 0.9166 - recall: 0.8940 - val_loss: 2.5840 - val_accuracy: 0.4233 - val_f1: 0.4184 - val_precision: 0.4328 - val_recall: 0.4050\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2131 - accuracy: 0.9226 - f1: 0.9245 - precision: 0.9358 - recall: 0.9137 - val_loss: 2.6972 - val_accuracy: 0.4233 - val_f1: 0.4193 - val_precision: 0.4300 - val_recall: 0.4092\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.2341 - accuracy: 0.9141 - f1: 0.9155 - precision: 0.9238 - recall: 0.9073 - val_loss: 2.6821 - val_accuracy: 0.4117 - val_f1: 0.4064 - val_precision: 0.4216 - val_recall: 0.3925\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2031 - accuracy: 0.9288 - f1: 0.9274 - precision: 0.9358 - recall: 0.9193 - val_loss: 2.9046 - val_accuracy: 0.4008 - val_f1: 0.3982 - val_precision: 0.4059 - val_recall: 0.3908\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2534 - accuracy: 0.9092 - f1: 0.9114 - precision: 0.9190 - recall: 0.9039 - val_loss: 2.7805 - val_accuracy: 0.4292 - val_f1: 0.4259 - val_precision: 0.4347 - val_recall: 0.4175\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2179 - accuracy: 0.9174 - f1: 0.9171 - precision: 0.9252 - recall: 0.9092 - val_loss: 3.0124 - val_accuracy: 0.4233 - val_f1: 0.4170 - val_precision: 0.4289 - val_recall: 0.4058\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2289 - accuracy: 0.9173 - f1: 0.9170 - precision: 0.9249 - recall: 0.9093 - val_loss: 2.9940 - val_accuracy: 0.4092 - val_f1: 0.4075 - val_precision: 0.4172 - val_recall: 0.3983\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.2426 - accuracy: 0.9141 - f1: 0.9125 - precision: 0.9225 - recall: 0.9029 - val_loss: 2.8676 - val_accuracy: 0.4208 - val_f1: 0.4204 - val_precision: 0.4298 - val_recall: 0.4117\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2004 - accuracy: 0.9322 - f1: 0.9306 - precision: 0.9379 - recall: 0.9235 - val_loss: 2.8519 - val_accuracy: 0.4225 - val_f1: 0.4211 - val_precision: 0.4329 - val_recall: 0.4100\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 2s 33ms/step - loss: 0.2282 - accuracy: 0.9184 - f1: 0.9208 - precision: 0.9281 - recall: 0.9136 - val_loss: 2.8452 - val_accuracy: 0.4342 - val_f1: 0.4265 - val_precision: 0.4351 - val_recall: 0.4183\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2088 - accuracy: 0.9226 - f1: 0.9219 - precision: 0.9289 - recall: 0.9151 - val_loss: 2.9071 - val_accuracy: 0.4117 - val_f1: 0.4082 - val_precision: 0.4159 - val_recall: 0.4008\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.1948 - accuracy: 0.9220 - f1: 0.9223 - precision: 0.9301 - recall: 0.9148 - val_loss: 3.0849 - val_accuracy: 0.4042 - val_f1: 0.4016 - val_precision: 0.4085 - val_recall: 0.3950\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2241 - accuracy: 0.9148 - f1: 0.9156 - precision: 0.9231 - recall: 0.9082 - val_loss: 3.1999 - val_accuracy: 0.4075 - val_f1: 0.4040 - val_precision: 0.4135 - val_recall: 0.3950\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 33ms/step - loss: 0.2742 - accuracy: 0.8984 - f1: 0.8982 - precision: 0.9069 - recall: 0.8898 - val_loss: 3.0722 - val_accuracy: 0.4000 - val_f1: 0.3923 - val_precision: 0.4000 - val_recall: 0.3850\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 2s 34ms/step - loss: 0.3175 - accuracy: 0.8859 - f1: 0.8882 - precision: 0.8985 - recall: 0.8784 - val_loss: 2.9368 - val_accuracy: 0.4017 - val_f1: 0.3946 - val_precision: 0.4057 - val_recall: 0.3842\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 33ms/step - loss: 0.3332 - accuracy: 0.8792 - f1: 0.8804 - precision: 0.8931 - recall: 0.8682 - val_loss: 2.7597 - val_accuracy: 0.4308 - val_f1: 0.4253 - val_precision: 0.4379 - val_recall: 0.4133\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 33ms/step - loss: 0.2826 - accuracy: 0.9015 - f1: 0.9005 - precision: 0.9124 - recall: 0.8891 - val_loss: 2.7466 - val_accuracy: 0.4458 - val_f1: 0.4449 - val_precision: 0.4544 - val_recall: 0.4358\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1944 - accuracy: 0.9258 - f1: 0.9253 - precision: 0.9334 - recall: 0.9175 - val_loss: 2.8782 - val_accuracy: 0.4642 - val_f1: 0.4627 - val_precision: 0.4744 - val_recall: 0.4517\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 2.9449 - accuracy: 0.4586 - f1: 0.4560 - precision: 0.4660 - recall: 0.4464\n",
            "[2.944915294647217, 0.4585714340209961, 0.45597121119499207, 0.4660448133945465, 0.4464285671710968]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "7bac54c0-6b54-4037-dfe3-07b8c441a96a"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "print(history_sg_bi.history)\r\n",
        "plt.title(\"Skip-Gram + Bi-LSTM accuracy, val accuracy vs epochs Graph\")\r\n",
        "plt.plot(history_sg_bi.history['val_accuracy'],c='b',label='val_accuracy')\r\n",
        "plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()\r\n",
        "plt.title(\"GloVe + Bi-LSTM accuracy, val accuracy vs epochs Graph\")\r\n",
        "plt.plot(history_glove_bi.history['val_accuracy'],c='b',label='val_accuracy')\r\n",
        "plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.5950000286102295, 0.6014285683631897, 0.4000000059604645]\n",
            "cbow [0.5942857265472412, 0.558571457862854, 0.3807142972946167]\n",
            "glove [0.802142858505249, 0.7992857098579407, 0.4585714340209961]\n",
            "{'loss': [1.3748588562011719, 1.352054238319397, 1.286169171333313, 1.2206363677978516, 1.1696046590805054, 1.1260943412780762, 1.0746515989303589, 1.0269455909729004, 0.9888772964477539, 0.9483209252357483, 0.9308350682258606, 0.8956030011177063, 0.8650184869766235, 0.8175415396690369, 0.7761331796646118, 0.7406750321388245, 0.7163100242614746, 0.6676921844482422, 0.6233659386634827, 0.5911402702331543, 0.5877382755279541, 0.561322033405304, 0.5132942795753479, 0.4890585243701935, 0.48337239027023315, 0.4479158818721771, 0.40611642599105835, 0.3676344156265259, 0.33793628215789795, 0.31481704115867615, 0.2907326817512512, 0.2612582743167877, 0.26413872838020325, 0.2417917251586914, 0.23231758177280426, 0.2526080310344696, 0.22815141081809998, 0.2314334213733673, 0.22182896733283997, 0.224411278963089, 0.19228248298168182, 0.1915031224489212, 0.1697121113538742, 0.19435831904411316, 0.14954398572444916, 0.1549946367740631, 0.12488547712564468, 0.11344622075557709, 0.11013984680175781, 0.10509560257196426, 0.10308495163917542, 0.11680471897125244, 0.16098153591156006, 0.14531713724136353, 0.11331440508365631, 0.09421458840370178, 0.088541679084301, 0.09897450357675552, 0.09800960123538971, 0.09117098897695541], 'accuracy': [0.31326088309288025, 0.3380434811115265, 0.4113043546676636, 0.4689130485057831, 0.49434784054756165, 0.522826075553894, 0.551086962223053, 0.5817391276359558, 0.6030434966087341, 0.616304337978363, 0.6284782886505127, 0.6432608962059021, 0.6608695387840271, 0.6823912858963013, 0.7043478488922119, 0.7130434513092041, 0.7321739196777344, 0.7454347610473633, 0.7656521797180176, 0.7778260707855225, 0.7858695387840271, 0.7941304445266724, 0.8147826194763184, 0.8213043212890625, 0.8173912763595581, 0.8365217447280884, 0.8536956310272217, 0.8708695769309998, 0.8823913335800171, 0.893478274345398, 0.8986956477165222, 0.9073913097381592, 0.9063043594360352, 0.9150000214576721, 0.9226086735725403, 0.9110869765281677, 0.9171739220619202, 0.9171739220619202, 0.9243478178977966, 0.9202173948287964, 0.9334782361984253, 0.9302173852920532, 0.9410869479179382, 0.9323912858963013, 0.945652186870575, 0.9452173709869385, 0.9552174210548401, 0.9578260779380798, 0.9617391228675842, 0.9604347944259644, 0.9619565010070801, 0.9571738839149475, 0.948913037776947, 0.9493478536605835, 0.9628260731697083, 0.9671739339828491, 0.9713043570518494, 0.9665217399597168, 0.9654347896575928, 0.9686956405639648], 'f1': [0.0, 0.01895718462765217, 0.1328585147857666, 0.22597800195217133, 0.31011202931404114, 0.3782541751861572, 0.43663841485977173, 0.4950711727142334, 0.5294642448425293, 0.564083993434906, 0.5756091475486755, 0.5934783816337585, 0.6141065359115601, 0.6425521373748779, 0.67281174659729, 0.6868370175361633, 0.7042339444160461, 0.7320724725723267, 0.7518585920333862, 0.7657939791679382, 0.775978147983551, 0.7880867719650269, 0.8082382082939148, 0.8168002367019653, 0.8166680335998535, 0.833078145980835, 0.8546444177627563, 0.8697017431259155, 0.8810958862304688, 0.8924249410629272, 0.8979551792144775, 0.9084945321083069, 0.9048023223876953, 0.9133726954460144, 0.9234917759895325, 0.9106737971305847, 0.9192630648612976, 0.9186004996299744, 0.9247280359268188, 0.920108437538147, 0.9323346614837646, 0.9302563071250916, 0.9408204555511475, 0.9325395822525024, 0.9449262022972107, 0.9457541108131409, 0.9559542536735535, 0.9574054479598999, 0.9610042572021484, 0.9604909420013428, 0.9630113840103149, 0.9562356472015381, 0.9478484392166138, 0.9493318796157837, 0.9619153141975403, 0.9673663973808289, 0.9712187051773071, 0.965456485748291, 0.9651927947998047, 0.9681165218353271], 'precision': [0.0, 0.15639913082122803, 0.6410740613937378, 0.6904721856117249, 0.6849976181983948, 0.6703055500984192, 0.6869434714317322, 0.6987709403038025, 0.7235972285270691, 0.7310653328895569, 0.736942708492279, 0.7471389174461365, 0.7609941959381104, 0.7745007276535034, 0.7886260747909546, 0.8009026646614075, 0.8123887181282043, 0.8252004384994507, 0.834919810295105, 0.8395057320594788, 0.8476047515869141, 0.8557187914848328, 0.8661163449287415, 0.8675869107246399, 0.8636497855186462, 0.8782202005386353, 0.8927491903305054, 0.9022991061210632, 0.9084590673446655, 0.916827380657196, 0.9170771837234497, 0.9284762740135193, 0.9223052263259888, 0.929649293422699, 0.9380130171775818, 0.9263973236083984, 0.9337202906608582, 0.9313432574272156, 0.9349793195724487, 0.9324105381965637, 0.94329833984375, 0.9396906495094299, 0.9483561515808105, 0.9434809684753418, 0.9515217542648315, 0.9523328542709351, 0.960931122303009, 0.9618542790412903, 0.9653514623641968, 0.9640851020812988, 0.9665142297744751, 0.9599336385726929, 0.9527462124824524, 0.95600426197052, 0.9656394720077515, 0.9706658124923706, 0.9737848043441772, 0.9676766395568848, 0.9680356383323669, 0.9695080518722534], 'recall': [0.0, 0.011086956597864628, 0.07760868221521378, 0.1386956423521042, 0.20652173459529877, 0.2671738564968109, 0.3243478238582611, 0.38608691096305847, 0.41978251934051514, 0.4606521427631378, 0.47456517815589905, 0.4941304624080658, 0.5169565081596375, 0.5521738529205322, 0.5882607698440552, 0.6043478846549988, 0.6252173781394958, 0.6597825884819031, 0.6854347586631775, 0.7052174210548401, 0.7165217399597168, 0.7315217852592468, 0.758695662021637, 0.7723913788795471, 0.7754347324371338, 0.7930435538291931, 0.8199999332427979, 0.8399999737739563, 0.8556522130966187, 0.8695651888847351, 0.8797827959060669, 0.889565110206604, 0.8880434632301331, 0.8978260159492493, 0.9095649719238281, 0.8956521153450012, 0.9054349064826965, 0.9063042402267456, 0.9147825837135315, 0.9082609415054321, 0.9217389822006226, 0.9210869073867798, 0.9334779977798462, 0.9219564199447632, 0.9384782314300537, 0.9393478631973267, 0.9510869383811951, 0.9530434012413025, 0.9567391872406006, 0.9569563865661621, 0.9595650434494019, 0.952608585357666, 0.9430434107780457, 0.942825973033905, 0.9582608342170715, 0.9641304016113281, 0.9686955213546753, 0.9632608890533447, 0.9623913168907166, 0.9667390584945679], 'val_loss': [1.360551357269287, 1.330876350402832, 1.2911248207092285, 1.2685531377792358, 1.232435703277588, 1.221388339996338, 1.238181471824646, 1.2256265878677368, 1.22343111038208, 1.2327131032943726, 1.2565308809280396, 1.2326462268829346, 1.2942956686019897, 1.3480234146118164, 1.3786051273345947, 1.4590034484863281, 1.5754495859146118, 1.5575488805770874, 1.594719648361206, 1.722662091255188, 1.6972829103469849, 1.7977272272109985, 1.766156792640686, 1.828651785850525, 1.869009017944336, 1.9801206588745117, 1.7871304750442505, 1.77983558177948, 1.8673956394195557, 2.0524673461914062, 2.079280376434326, 2.241387367248535, 2.3036205768585205, 2.402398109436035, 2.3162543773651123, 2.2572200298309326, 2.2082865238189697, 2.385761022567749, 2.1548259258270264, 2.1119959354400635, 2.2449588775634766, 2.141127347946167, 2.28826642036438, 2.219238519668579, 2.424306869506836, 2.419048547744751, 2.5190446376800537, 2.529106616973877, 2.487386465072632, 2.367459297180176, 2.5055603981018066, 2.5087249279022217, 2.526792049407959, 2.4115829467773438, 2.4806008338928223, 2.56573486328125, 2.65739107131958, 2.624307870864868, 2.721825122833252, 2.662538528442383], 'val_accuracy': [0.3408333361148834, 0.3558333218097687, 0.41083332896232605, 0.4350000023841858, 0.44999998807907104, 0.4508333206176758, 0.4566666781902313, 0.46166667342185974, 0.46416667103767395, 0.4699999988079071, 0.4658333361148834, 0.49000000953674316, 0.4716666638851166, 0.47583332657814026, 0.4983333349227905, 0.5024999976158142, 0.5049999952316284, 0.5208333134651184, 0.5116666555404663, 0.5274999737739563, 0.5316666960716248, 0.5291666388511658, 0.5400000214576721, 0.5450000166893005, 0.5383333563804626, 0.5475000143051147, 0.5625, 0.5433333516120911, 0.5633333325386047, 0.5516666769981384, 0.5641666650772095, 0.5608333349227905, 0.5575000047683716, 0.5649999976158142, 0.5608333349227905, 0.5641666650772095, 0.5666666626930237, 0.5699999928474426, 0.5666666626930237, 0.574999988079071, 0.5691666603088379, 0.5891666412353516, 0.57833331823349, 0.5683333277702332, 0.5774999856948853, 0.5799999833106995, 0.5816666483879089, 0.5766666531562805, 0.5808333158493042, 0.5916666388511658, 0.5866666436195374, 0.5866666436195374, 0.5741666555404663, 0.5883333086967468, 0.5699999928474426, 0.57833331823349, 0.5691666603088379, 0.5724999904632568, 0.5791666507720947, 0.5741666555404663], 'val_f1': [0.0, 0.01147026289254427, 0.10066426545381546, 0.16016709804534912, 0.2427125722169876, 0.2629476487636566, 0.3305085599422455, 0.34321364760398865, 0.37214425206184387, 0.3807067573070526, 0.39374133944511414, 0.4206942617893219, 0.4073590934276581, 0.4339054524898529, 0.4526941776275635, 0.4769550561904907, 0.47798454761505127, 0.4988637864589691, 0.5108654499053955, 0.513832151889801, 0.5123422741889954, 0.5134148001670837, 0.5368326306343079, 0.5393747687339783, 0.533159077167511, 0.5334653258323669, 0.5615832805633545, 0.5346360802650452, 0.5662667155265808, 0.5515627264976501, 0.5587735176086426, 0.5548840165138245, 0.5570005774497986, 0.5647211670875549, 0.5575423240661621, 0.5625097751617432, 0.5656523704528809, 0.5649296641349792, 0.564139723777771, 0.5733935236930847, 0.5712531208992004, 0.5816038250923157, 0.5729482769966125, 0.5655010342597961, 0.5726704001426697, 0.5801022052764893, 0.5820252299308777, 0.5747488141059875, 0.5784871578216553, 0.5934592485427856, 0.5880282521247864, 0.5866914391517639, 0.5735127925872803, 0.5850366950035095, 0.5682698488235474, 0.5783587098121643, 0.569413423538208, 0.5695254802703857, 0.5817478895187378, 0.5732496380805969], 'val_precision': [0.0, 0.375, 0.6424843668937683, 0.6358381509780884, 0.6298978328704834, 0.6070136427879333, 0.5728574991226196, 0.5660111308097839, 0.5794495940208435, 0.5733305811882019, 0.5579628944396973, 0.5693164467811584, 0.5138490200042725, 0.5186116695404053, 0.5280167460441589, 0.5447567105293274, 0.5261022448539734, 0.5538799166679382, 0.5599578022956848, 0.5508074760437012, 0.5580602288246155, 0.5522834658622742, 0.5711612105369568, 0.566443145275116, 0.5648782849311829, 0.5637245774269104, 0.5924858450889587, 0.563220739364624, 0.5865055322647095, 0.5692419409751892, 0.5790765881538391, 0.5688454508781433, 0.570631742477417, 0.5785014033317566, 0.5745404958724976, 0.5784675478935242, 0.5786193609237671, 0.5798822045326233, 0.5791831016540527, 0.5921630263328552, 0.5840334892272949, 0.5974363088607788, 0.5848367214202881, 0.581110417842865, 0.5806453227996826, 0.5906933546066284, 0.5901619791984558, 0.5867835879325867, 0.5909332036972046, 0.6058909296989441, 0.5972184538841248, 0.5945214033126831, 0.581497848033905, 0.5964338183403015, 0.5805199146270752, 0.5852953791618347, 0.5774038434028625, 0.5777466893196106, 0.5913198590278625, 0.5809810161590576], 'val_recall': [0.0, 0.005833333358168602, 0.05499999597668648, 0.0925000011920929, 0.15083332359790802, 0.1691666692495346, 0.23250000178813934, 0.24666666984558105, 0.2749999761581421, 0.28583329916000366, 0.304999977350235, 0.33416664600372314, 0.3383333384990692, 0.3733333349227905, 0.39666667580604553, 0.4249999523162842, 0.4383333623409271, 0.4541666805744171, 0.4700000286102295, 0.4816666543483734, 0.4741666615009308, 0.48000001907348633, 0.5066666603088379, 0.5149999856948853, 0.5050000548362732, 0.5066667199134827, 0.534166693687439, 0.5091666579246521, 0.5475000143051147, 0.5350000262260437, 0.5399999618530273, 0.5416666269302368, 0.5441666841506958, 0.5516666173934937, 0.541666567325592, 0.54749995470047, 0.5533333420753479, 0.5508332848548889, 0.550000011920929, 0.5558332800865173, 0.559166669845581, 0.5666666626930237, 0.5616666674613953, 0.5508332848548889, 0.5649999976158142, 0.5699999928474426, 0.5741667151451111, 0.5633333325386047, 0.5666666626930237, 0.5816666483879089, 0.57916659116745, 0.57916659116745, 0.5658333897590637, 0.5741667151451111, 0.5566667318344116, 0.5716666579246521, 0.5616666674613953, 0.5616666674613953, 0.5725000500679016, 0.565833330154419]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}