{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2VecCBOW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWd5+fqak0NgFuJ3w15PPl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/Word2VecCBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uoumlUjgDzI",
        "outputId": "113e0536-aed3-4451-84c8-c32b471fd229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df_train = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/Data/Train/clean_all_train.csv')\n",
        "\n",
        "def generate_dictionary_data(text):\n",
        "  word_to_index= dict()\n",
        "  index_to_word = dict()\n",
        "  corpus = []\n",
        "  count = 0\n",
        "  vocab_size = 0    \n",
        "  for row in text:\n",
        "    for word in row.split():\n",
        "      corpus.append(word)\n",
        "      if word_to_index.get(word) == None:\n",
        "        word_to_index.update ( {word : count})\n",
        "        index_to_word.update ( {count : word })\n",
        "        count  += 1\n",
        "      vocab_size = len(word_to_index)\n",
        "      length_of_corpus = len(corpus)\n",
        "  return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus\n",
        "\n",
        "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index):\n",
        "  trgt_word_vector = np.zeros(vocab_size)  \n",
        "  index_of_word_dictionary = word_to_index.get(target_word) \n",
        "  trgt_word_vector[index_of_word_dictionary] = 1  \n",
        "  ctxt_word_vector = np.zeros(vocab_size)\n",
        "  for word in context_words:\n",
        "    index_of_word_dictionary = word_to_index.get(word) \n",
        "    ctxt_word_vector[index_of_word_dictionary] = 1    \n",
        "  return trgt_word_vector,ctxt_word_vector\n",
        "\n",
        "def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,batch_size):\n",
        "  while True:\n",
        "    for k in range(int(length_of_corpus/batch_size)):\n",
        "      training_data_target =  []\n",
        "      training_data_context =  []\n",
        "      for i,word in enumerate(corpus[batch_size*k:min(len(corpus),(batch_size*(k+1)))]):\n",
        "        cor = corpus[batch_size*k:min(len(corpus),(batch_size*(k+1)))]\n",
        "        length_of_cor = len(cor)\n",
        "        index_target_word = i\n",
        "        target_word = word\n",
        "        context_words = []\n",
        "        #when target word is the first word\n",
        "        if i == 0:  \n",
        "          # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
        "          context_words = [cor[x] for x in range(i + 1 , window_size + 1)] \n",
        "        #when target word is the last word\n",
        "        elif i == length_of_cor-1:\n",
        "          # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
        "          context_words = [cor[x] for x in range(length_of_cor - 2 ,length_of_cor -2 - window_size  , -1 )]\n",
        "        #When target word is the middle word\n",
        "        else:\n",
        "          #Before the middle target word\n",
        "          before_target_word_index = index_target_word - 1\n",
        "          for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
        "            if x >=0:\n",
        "              context_words.extend([cor[x]])\n",
        "          #After the middle target word\n",
        "          after_target_word_index = index_target_word + 1\n",
        "          for x in range(after_target_word_index, after_target_word_index + window_size):\n",
        "            if x < length_of_cor:\n",
        "              context_words.extend([cor[x]])\n",
        "        trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index)\n",
        "        training_data_target.append(trgt_word_vector)\n",
        "        training_data_context.append(ctxt_word_vector)\n",
        "      yield (np.array(training_data_context),np.array(training_data_target))\n",
        "      del training_data_target\n",
        "      del training_data_context\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Patdvl2KgOTk",
        "outputId": "c09b4c39-363f-41f0-b286-38f1767a29e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#\n",
        "epochs_num = 20\n",
        "window_size = 3\n",
        "embedding_dimensions = 96\n",
        "batch_size = 128\n",
        "#\n",
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictionary_data(df_train['Tweets'])\n",
        "print(vocab_size)\n",
        "print(len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10220\n",
            "58800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uvo2j4ugQRS",
        "outputId": "bb253716-cb54-47fa-8ff7-d2f52ce9d3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model = keras.Sequential([keras.layers.Dense(embedding_dimensions),\n",
        "                          keras.layers.Dense(vocab_size,activation = 'softmax')])\n",
        "model.compile(optimizer = \"adam\", loss = 'categorical_crossentropy',metrics=[\"accuracy\"])\n",
        "model.fit(x = generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,batch_size),steps_per_epoch=int(length_of_corpus/batch_size),epochs = epochs_num)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 7.8482 - accuracy: 0.0287\n",
            "Epoch 2/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 7.1942 - accuracy: 0.0450\n",
            "Epoch 3/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 6.9095 - accuracy: 0.0755\n",
            "Epoch 4/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 6.5705 - accuracy: 0.1008\n",
            "Epoch 5/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 6.1551 - accuracy: 0.1264\n",
            "Epoch 6/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 5.6663 - accuracy: 0.1602\n",
            "Epoch 7/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 5.1318 - accuracy: 0.1984\n",
            "Epoch 8/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 4.5897 - accuracy: 0.2465\n",
            "Epoch 9/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 4.0686 - accuracy: 0.3007\n",
            "Epoch 10/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 3.5843 - accuracy: 0.3615\n",
            "Epoch 11/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 3.1439 - accuracy: 0.4246\n",
            "Epoch 12/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 2.7502 - accuracy: 0.4905\n",
            "Epoch 13/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 2.4031 - accuracy: 0.5512\n",
            "Epoch 14/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 2.0999 - accuracy: 0.6067\n",
            "Epoch 15/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 1.8364 - accuracy: 0.6579\n",
            "Epoch 16/20\n",
            "459/459 [==============================] - 8s 18ms/step - loss: 1.6082 - accuracy: 0.7017\n",
            "Epoch 17/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 1.4102 - accuracy: 0.7412\n",
            "Epoch 18/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 1.2381 - accuracy: 0.7745\n",
            "Epoch 19/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 1.0884 - accuracy: 0.8058\n",
            "Epoch 20/20\n",
            "459/459 [==============================] - 8s 17ms/step - loss: 0.9580 - accuracy: 0.8312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f56f00cf6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJoDkendgYqj",
        "outputId": "4558f4ec-10ac-4aad-d403-53756f4cafec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def make_dict(index_to_word,weight_matrix):\n",
        "  dict = {}\n",
        "  for index,i in enumerate(weight_matrix):\n",
        "    dict[index_to_word[index]] = i\n",
        "  return dict\n",
        "dict = make_dict(index_to_word,model.get_weights()[2].T)\n",
        "len(dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10220"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdPJAlAxgU86"
      },
      "source": [
        "df = pd.DataFrame.from_dict(dict, orient=\"index\")\n",
        "df.to_csv(\"/content/drive/My Drive/InfluenceAnalysis/Word2VecCBOW/Dictionary.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMaRQx4Tga1b"
      },
      "source": [
        "def find_similar_20(word,dict):\n",
        "  l = []\n",
        "  word_val = {}\n",
        "  if word in dict:\n",
        "    for i in dict:\n",
        "      word_val[i] = sum([x*y for x,y in zip(dict[i],dict[word])])\n",
        "    word_val = {k: v for k, v in sorted(word_val.items(), key=lambda item: item[1],reverse=True)}\n",
        "  c = 0\n",
        "  for i in word_val:\n",
        "    c += 1\n",
        "    if c==21:\n",
        "      break\n",
        "    l.append(i)\n",
        "  return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-xOpvXBgcd7",
        "outputId": "da53b894-2ac2-4cf9-e179-171f7233d2b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "find_similar_20('happy',dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['happy',\n",
              " 'it',\n",
              " 'fun',\n",
              " 'good',\n",
              " 'bday',\n",
              " 'sad',\n",
              " 'their',\n",
              " 'me',\n",
              " 'of',\n",
              " 'You',\n",
              " 'like',\n",
              " 'do',\n",
              " 'love',\n",
              " 'through',\n",
              " 'city',\n",
              " 'important',\n",
              " 's',\n",
              " 'excited',\n",
              " 'for',\n",
              " 'With']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}