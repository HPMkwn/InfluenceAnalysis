{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEkphrasisnegpos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEkphrasisnegpos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1173ba-d487-494b-b663-3f371c6ce7f3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/train_clean.csv',names=['Tweet','Emotion'])\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/test_clean.csv',names=['Tweet','Emotion'])\n",
        "df_train = df_train.sample(frac=1)\n",
        "df_test = df_train.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zrtpGceAiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d6d442d3-186f-498c-fb10-2b79d892c3fb"
      },
      "source": [
        "df_train.head(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8259</th>\n",
              "      <td>tri to spend time away from the realitybrok my...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>how do you twitter</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11148</th>\n",
              "      <td>my poor laptop got a quotrogu malwarequot visi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5814</th>\n",
              "      <td>ouch my back man I am sick</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6527</th>\n",
              "      <td>cannot believ thi shit ive spend minut for for...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10846</th>\n",
              "      <td>oop  correct it nowheremonastri cannot seem to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9749</th>\n",
              "      <td>i miss my boo on anoth note I am soreadi for t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10900</th>\n",
              "      <td>icebergmeadow no but thi is our poor week</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10805</th>\n",
              "      <td>supergirln sorri all the code i have are post ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9486</th>\n",
              "      <td>brandihoov we are comingwont be there til arou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Tweet Emotion\n",
              "8259   tri to spend time away from the realitybrok my...       2\n",
              "942                                   how do you twitter       0\n",
              "11148  my poor laptop got a quotrogu malwarequot visi...       2\n",
              "5814                          ouch my back man I am sick       2\n",
              "6527   cannot believ thi shit ive spend minut for for...       2\n",
              "...                                                  ...     ...\n",
              "10846  oop  correct it nowheremonastri cannot seem to...       0\n",
              "9749   i miss my boo on anoth note I am soreadi for t...       2\n",
              "10900          icebergmeadow no but thi is our poor week       2\n",
              "10805  supergirln sorri all the code i have are post ...       2\n",
              "9486   brandihoov we are comingwont be there til arou...       1\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K68sxL_LfLy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccaa3913-d03f-451a-9b41-b756f6fa3719"
      },
      "source": [
        "df_test.Emotion.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1', '2', '0', '3', 'emotion'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9KFGTW2iP8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331c4c8b-444b-46d4-f448-c3ff7c2e446c"
      },
      "source": [
        "df_train.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tweet      12370\n",
              "Emotion        5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {'0':[1,0,0,0],'1':[0,1,0,0],'2':[0,0,1,0],'3':[0,0,0,1],'emotion':[1,0,0,0]}\n",
        "train_data_tweet = [x.lower().split() for x in df_train['Tweet']]\n",
        "train_data_cat = np.array([category_dict[x] for x in df_train['Emotion']])\n",
        "test_data_tweet = [x.lower().split() for x in df_test['Tweet']]\n",
        "test_data_cat = np.array([category_dict[x] for x in df_test['Emotion']])\n",
        "\n",
        "data_tweet = train_data_tweet + test_data_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b2f39e-f906-400e-ac97-d12147ca832d"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1303926, 1724630)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 500\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor=1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z65iXVhlpeDX"
      },
      "source": [
        "positive = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/positive-words.csv')\r\n",
        "positive = list(positive['0'][1:])\r\n",
        "negative = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/negative-words.csv')\r\n",
        "negative = list(negative['0'][2:])\r\n",
        "pn_dict = {}\r\n",
        "for i in range(len(positive)):\r\n",
        "  pn_dict[positive[i]] = 1\r\n",
        "\r\n",
        "for i in range(len(negative)):\r\n",
        "  pn_dict[negative[i]] = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqvzy05D2iim"
      },
      "source": [
        "X_train = train_data_tweet\r\n",
        "X_test = test_data_tweet\r\n",
        "y_train = train_data_cat\r\n",
        "y_test = test_data_cat \r\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\r\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\r\n",
        "X_train = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_train]\r\n",
        "X_test = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_test]\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIGpYOBRx-kf",
        "outputId": "54c59105-d6bd-4f4e-cee6-7e351a48800b"
      },
      "source": [
        "print(X_train[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('tri', 'NN', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.get_vector(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            tweet[i]=((tweet[i][0]*tweet[i][2]*mul_factor),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            tweet[i]=((tweet[i][0]*tweet[i][2]*mul_factor),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4558dc-0b3d-4a50-c464-f483ec2aee91"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-bddb9a303a31>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            " 2/24 [=>............................] - ETA: 0s - loss: 1.3845 - accuracy: 0.3560WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0280s vs `on_train_batch_end` time: 0.0444s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 1.2397 - accuracy: 0.5685\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 1.0086 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9834 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9863 - accuracy: 0.5845\n",
            "[0.9863371849060059, 0.5845000147819519]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7dbf1df-0346-46d0-d222-b256a3411573"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 1.2277 - accuracy: 0.5772\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 1.0106 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9841 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9838 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9837 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9836 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9835 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9834 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9833 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9831 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9829 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9827 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9824 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9820 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9813 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9800 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9774 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9774 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9730 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9726 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 0.9734 - accuracy: 0.5845\n",
            "[0.9733834862709045, 0.5845000147819519]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2990c85e-ba07-4b8b-ff0b-a2d5a2d89e2c"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 1.0888 - accuracy: 0.5372\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9860 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9848 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9876 - accuracy: 0.5845\n",
            "[0.9875824451446533, 0.5845000147819519]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031a9220-f1e7-497f-b9fc-de909ca6e329"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 215ms/step - loss: 1.2405 - accuracy: 0.5732\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 1.0091 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9841 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9838 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9838 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9837 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9837 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9836 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9835 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9834 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9832 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9829 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9825 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9845 - accuracy: 0.5845\n",
            "[0.9845123887062073, 0.5845000147819519]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a528a46-be9f-4f07-8b25-fb163e4772d5"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 1.2326 - accuracy: 0.5713\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 1.0109 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9837 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9836 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9834 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9833 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9831 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9829 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9828 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9826 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9823 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9820 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9817 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9813 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9807 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9800 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9786 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9769 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9789 - accuracy: 0.5845\n",
            "[0.9788734316825867, 0.5845000147819519]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe21308-6119-4ac4-d118-7a0ff10bc05f"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 1.0689 - accuracy: 0.5699\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9844 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9844 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9844 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9844 - accuracy: 0.5863\n",
            " 2/24 [=>............................] - ETA: 3s - loss: 1.0283 - accuracy: 0.5500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0638s vs `on_test_batch_end` time: 0.1204s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9862 - accuracy: 0.5845\n",
            "[0.9862341284751892, 0.5845000147819519]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEglKIfExKKI"
      },
      "source": [
        "epochs=40\r\n",
        "Embedding_size=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba021fd0-4c76-4991-8524-b51783d5ba49"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 1s 35ms/step - loss: 1.2074 - accuracy: 0.5595\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 1.0011 - accuracy: 0.5863\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.9772 - accuracy: 0.5864\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.9699 - accuracy: 0.5864\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.9667 - accuracy: 0.5864\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9621 - accuracy: 0.5873\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9569 - accuracy: 0.5915\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9502 - accuracy: 0.5943\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9440 - accuracy: 0.5957\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9382 - accuracy: 0.5984\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9318 - accuracy: 0.6010\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.9243 - accuracy: 0.6058\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9165 - accuracy: 0.6107\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.9063 - accuracy: 0.6156\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8894 - accuracy: 0.6238\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.8749 - accuracy: 0.6313\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8599 - accuracy: 0.6400\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8469 - accuracy: 0.6489\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8351 - accuracy: 0.6544\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8236 - accuracy: 0.6631\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8126 - accuracy: 0.6671\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.8046 - accuracy: 0.6743\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 1s 35ms/step - loss: 0.8053 - accuracy: 0.6761\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7954 - accuracy: 0.6814\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7745 - accuracy: 0.6938\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7731 - accuracy: 0.6942\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7815 - accuracy: 0.6848\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7716 - accuracy: 0.6974\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7492 - accuracy: 0.7085\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7414 - accuracy: 0.7142\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7362 - accuracy: 0.7177\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7641 - accuracy: 0.7011\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7699 - accuracy: 0.6957\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7736 - accuracy: 0.6923\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7820 - accuracy: 0.6875\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7391 - accuracy: 0.7109\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7239 - accuracy: 0.7231\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.7079 - accuracy: 0.7336\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.6931 - accuracy: 0.7428\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.6828 - accuracy: 0.7452\n",
            "24/24 [==============================] - 1s 36ms/step - loss: 0.7255 - accuracy: 0.7239\n",
            "[0.7254893183708191, 0.7239166498184204]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026879a6-8997-488b-8103-e7dddbf415b3"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 1.0763 - accuracy: 0.5594\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.9691 - accuracy: 0.5863\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.9499 - accuracy: 0.5878\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.9318 - accuracy: 0.5946\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.9165 - accuracy: 0.6003\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.9021 - accuracy: 0.6063\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.8879 - accuracy: 0.6131\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.8770 - accuracy: 0.6174\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.8659 - accuracy: 0.6250\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.8549 - accuracy: 0.6323\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.8438 - accuracy: 0.6382\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.8322 - accuracy: 0.6431\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.8202 - accuracy: 0.6507\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.8077 - accuracy: 0.6587\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.8055 - accuracy: 0.6603\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.8120 - accuracy: 0.6561\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.7921 - accuracy: 0.6674\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7725 - accuracy: 0.6782\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7643 - accuracy: 0.6809\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.7621 - accuracy: 0.6808\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.7423 - accuracy: 0.6899\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7330 - accuracy: 0.6939\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7341 - accuracy: 0.6921\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7583 - accuracy: 0.6865\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7325 - accuracy: 0.6983\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.7241 - accuracy: 0.7026\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.7184 - accuracy: 0.7057\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.7069 - accuracy: 0.7107\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.6975 - accuracy: 0.7168\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.6827 - accuracy: 0.7204\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.6734 - accuracy: 0.7269\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.6543 - accuracy: 0.7342\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.6457 - accuracy: 0.7358\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.6380 - accuracy: 0.7383\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.6309 - accuracy: 0.7424\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.6414 - accuracy: 0.7373\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.6259 - accuracy: 0.7512\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 1s 39ms/step - loss: 0.6027 - accuracy: 0.7609\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.5851 - accuracy: 0.7687\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 1s 38ms/step - loss: 0.5613 - accuracy: 0.7818\n",
            "24/24 [==============================] - 1s 35ms/step - loss: 0.5924 - accuracy: 0.7701\n",
            "[0.5923612713813782, 0.7700833082199097]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508fea74-54c1-4054-82ef-54bf0896b51e"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 1s 52ms/step - loss: 1.1004 - accuracy: 0.5547\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9853 - accuracy: 0.5864\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9839 - accuracy: 0.5864\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 1s 52ms/step - loss: 0.9818 - accuracy: 0.5866\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9813 - accuracy: 0.5864\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9782 - accuracy: 0.5863\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9725 - accuracy: 0.5871\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9682 - accuracy: 0.5895\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9660 - accuracy: 0.5909\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.9614 - accuracy: 0.5911\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9609 - accuracy: 0.5918\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9553 - accuracy: 0.5954\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9542 - accuracy: 0.5969\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9527 - accuracy: 0.5953\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.9486 - accuracy: 0.5982\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9429 - accuracy: 0.6017\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9492 - accuracy: 0.5974\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9496 - accuracy: 0.5974\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9481 - accuracy: 0.5970\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9395 - accuracy: 0.6038\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9316 - accuracy: 0.6095\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9231 - accuracy: 0.6153\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9181 - accuracy: 0.6192\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.9148 - accuracy: 0.6219\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9170 - accuracy: 0.6212\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9071 - accuracy: 0.6284\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9022 - accuracy: 0.6324\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9014 - accuracy: 0.6338\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9037 - accuracy: 0.6321\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9472 - accuracy: 0.5972\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9059 - accuracy: 0.6295\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.8937 - accuracy: 0.6392\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.9330 - accuracy: 0.6116\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.8983 - accuracy: 0.6327\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.8895 - accuracy: 0.6421\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.9115 - accuracy: 0.6256\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9189 - accuracy: 0.6195\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.8915 - accuracy: 0.6393\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.8928 - accuracy: 0.6404\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.9314 - accuracy: 0.6168\n",
            "24/24 [==============================] - 1s 37ms/step - loss: 0.9398 - accuracy: 0.6159\n",
            "[0.9398112893104553, 0.6159166693687439]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdPYKennH7PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6352e279-cbb5-4aa6-c97b-714e7f36b8a1"
      },
      "source": [
        "model_sg1 = tf.keras.Sequential()\r\n",
        "model_sg1.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_sg1.add(tf.keras.layers.LSTM(64))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg1.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg1.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg1.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table+=[history[1]]\r\n",
        "print(\"SG_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 1.2185 - accuracy: 0.5727\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 1.0274 - accuracy: 0.5862\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9973 - accuracy: 0.5859\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9955 - accuracy: 0.5862\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9957 - accuracy: 0.5864\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9936 - accuracy: 0.5863\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9892 - accuracy: 0.5863\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9915 - accuracy: 0.5863\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9890 - accuracy: 0.5863\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9873 - accuracy: 0.5863\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9879 - accuracy: 0.5863\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9897 - accuracy: 0.5863\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9867 - accuracy: 0.5863\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9837 - accuracy: 0.5863\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.9818 - accuracy: 0.5863\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9809 - accuracy: 0.5863\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9800 - accuracy: 0.5863\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9775 - accuracy: 0.5863\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9781 - accuracy: 0.5863\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9787 - accuracy: 0.5863\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9785 - accuracy: 0.5863\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9764 - accuracy: 0.5863\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9754 - accuracy: 0.5863\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9749 - accuracy: 0.5863\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9758 - accuracy: 0.5863\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9763 - accuracy: 0.5863\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9741 - accuracy: 0.5863\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9746 - accuracy: 0.5863\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9751 - accuracy: 0.5863\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9740 - accuracy: 0.5863\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9741 - accuracy: 0.5863\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9743 - accuracy: 0.5863\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9748 - accuracy: 0.5863\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9746 - accuracy: 0.5863\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9729 - accuracy: 0.5863\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9725 - accuracy: 0.5863\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9743 - accuracy: 0.5863\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9724 - accuracy: 0.5863\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9728 - accuracy: 0.5863\n",
            " 2/24 [=>............................] - ETA: 2s - loss: 1.0172 - accuracy: 0.5500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.1264s vs `on_test_batch_end` time: 0.2123s). Check your callbacks.\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9712 - accuracy: 0.5845\n",
            "[0.9712111949920654, 0.5845000147819519]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9eadb63-3f98-4eb9-f989-0eff7e30a08c"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.5845000147819519, 0.5845000147819519, 0.5845000147819519]\n",
            "cbow [0.5845000147819519, 0.5845000147819519, 0.5845000147819519]\n",
            "glove [0.7239166498184204, 0.7700833082199097, 0.6159166693687439]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLVUTMaIFdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1a3d6a-8746-4f61-a58a-c65a51fff043"
      },
      "source": [
        "print(result_table[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5845000147819519\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}