{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointGen_Norm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointGen_Norm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8739b0ea-8af8-4040-f5f3-ce00b150e375"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3a2697-0732-4dca-fdfc-d9143a133b70"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['with', 'their', 'faggy', 'colors', 'are', 'nice', 'is', 'ok', 'too', 'even', 'tho', 'some', 'might', 'take', 'offense', 'because', 'words', 'lol'], ['unbelievable', 'takes', '10', 'minutes', 'to', 'get', 'through', 'to', 'then', 'there', 'is', 'a', 'fault', 'and', 'the', 'call', 'hangs', 'up', 'treatcustomersfairly'], ['well', 'i', 'did', 'hear', 'once', 'before', 'that', 'girls', 'are', 'attracted', 'to', 'men', 'that', 'lok', 'like', 'their', 'dad', 'ok', 'hand'], ['agreed', 'so', 'tired', 'of', 'this', 'nonsense', 'soros', 'must', 'be', 'elated'], ['by', 'the', 'way', 'i', 'am', 'wearing', 'the', 'smile', 'you', 'gave', 'me', 'today', 'n', 'you', 'me']]\n",
            "[[1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 200\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnPxfLf139kD",
        "outputId": "00179d69-4637-4b18-cb49-d41836472cf4"
      },
      "source": [
        "print(len(vocab_sg))\r\n",
        "print(len(vocab_cbow))\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13658\n",
            "13658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      temp = np.array([np.array([w2v.wv.get_vector(i) for i in x if i in vocab]) for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]])\r\n",
        "      temp = np.array([np.pad(x.flatten(),(0,Max_input_size*Embedding_size-len(x.flatten()))).reshape(Max_input_size,Embedding_size) for x in temp])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)\r\n",
        "\r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      temp = np.array([np.array([vocab[i] for i in x if i in vocab.keys()]) for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]])\r\n",
        "      temp = np.array([np.pad(x.flatten(),(0,Max_input_size*Embedding_size-len(x.flatten()))).reshape(Max_input_size,Embedding_size) for x in temp])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 60"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94w9X37VzLZZ"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f7c326-4f56-4536-8a89-93263ce36308"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(64))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy',f1,precision,recall])\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 9s 133ms/step - loss: 1.3819 - accuracy: 0.2974 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3728 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3783 - accuracy: 0.3054 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3695 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3759 - accuracy: 0.3105 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3709 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3758 - accuracy: 0.3163 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3707 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.3769 - accuracy: 0.3128 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3690 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3768 - accuracy: 0.3118 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3718 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3750 - accuracy: 0.3144 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3715 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.3759 - accuracy: 0.3113 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3692 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3751 - accuracy: 0.3150 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3715 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3756 - accuracy: 0.3116 - f1: 1.8683e-04 - precision: 0.0094 - recall: 9.4349e-05 - val_loss: 1.3957 - val_accuracy: 0.3000 - val_f1: 0.0353 - val_precision: 0.4722 - val_recall: 0.0183\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3561 - accuracy: 0.3254 - f1: 0.0652 - precision: 0.3864 - recall: 0.0373 - val_loss: 1.3415 - val_accuracy: 0.3425 - val_f1: 0.1789 - val_precision: 0.5006 - val_recall: 0.1092\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3271 - accuracy: 0.3532 - f1: 0.1393 - precision: 0.5687 - recall: 0.0840 - val_loss: 1.3410 - val_accuracy: 0.3458 - val_f1: 0.0954 - val_precision: 0.5635 - val_recall: 0.0525\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3135 - accuracy: 0.3674 - f1: 0.1555 - precision: 0.6071 - recall: 0.0929 - val_loss: 1.3536 - val_accuracy: 0.3392 - val_f1: 0.2089 - val_precision: 0.5117 - val_recall: 0.1317\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3193 - accuracy: 0.3793 - f1: 0.1530 - precision: 0.4184 - recall: 0.1017 - val_loss: 1.3537 - val_accuracy: 0.3308 - val_f1: 0.1735 - val_precision: 0.5264 - val_recall: 0.1042\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2957 - accuracy: 0.3879 - f1: 0.2040 - precision: 0.6661 - recall: 0.1276 - val_loss: 1.3348 - val_accuracy: 0.3575 - val_f1: 0.1952 - val_precision: 0.5330 - val_recall: 0.1200\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2913 - accuracy: 0.3907 - f1: 0.2070 - precision: 0.6166 - recall: 0.1282 - val_loss: 1.3700 - val_accuracy: 0.3475 - val_f1: 0.2440 - val_precision: 0.4944 - val_recall: 0.1625\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.2934 - accuracy: 0.4004 - f1: 0.2152 - precision: 0.6518 - recall: 0.1364 - val_loss: 1.3891 - val_accuracy: 0.3592 - val_f1: 0.2536 - val_precision: 0.4846 - val_recall: 0.1725\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2927 - accuracy: 0.4008 - f1: 0.2135 - precision: 0.6651 - recall: 0.1352 - val_loss: 1.3885 - val_accuracy: 0.3483 - val_f1: 0.2361 - val_precision: 0.4927 - val_recall: 0.1558\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2763 - accuracy: 0.4124 - f1: 0.2199 - precision: 0.6369 - recall: 0.1394 - val_loss: 1.4021 - val_accuracy: 0.3492 - val_f1: 0.2522 - val_precision: 0.4950 - val_recall: 0.1700\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2797 - accuracy: 0.4075 - f1: 0.2162 - precision: 0.6405 - recall: 0.1370 - val_loss: 1.4058 - val_accuracy: 0.3458 - val_f1: 0.2442 - val_precision: 0.5067 - val_recall: 0.1617\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2815 - accuracy: 0.4022 - f1: 0.2045 - precision: 0.6804 - recall: 0.1284 - val_loss: 1.3809 - val_accuracy: 0.3417 - val_f1: 0.2489 - val_precision: 0.5289 - val_recall: 0.1633\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2713 - accuracy: 0.4095 - f1: 0.2225 - precision: 0.6800 - recall: 0.1405 - val_loss: 1.3389 - val_accuracy: 0.3550 - val_f1: 0.2460 - val_precision: 0.5563 - val_recall: 0.1583\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.2492 - accuracy: 0.4194 - f1: 0.2539 - precision: 0.6604 - recall: 0.1627 - val_loss: 1.3154 - val_accuracy: 0.3758 - val_f1: 0.2402 - val_precision: 0.5402 - val_recall: 0.1550\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2197 - accuracy: 0.4417 - f1: 0.2817 - precision: 0.6688 - recall: 0.1808 - val_loss: 1.3160 - val_accuracy: 0.3883 - val_f1: 0.2461 - val_precision: 0.5800 - val_recall: 0.1567\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2001 - accuracy: 0.4589 - f1: 0.2982 - precision: 0.7110 - recall: 0.1906 - val_loss: 1.3014 - val_accuracy: 0.4033 - val_f1: 0.2732 - val_precision: 0.5910 - val_recall: 0.1783\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.1835 - accuracy: 0.4650 - f1: 0.3288 - precision: 0.7056 - recall: 0.2180 - val_loss: 1.3072 - val_accuracy: 0.4083 - val_f1: 0.2931 - val_precision: 0.5591 - val_recall: 0.1992\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.1483 - accuracy: 0.4837 - f1: 0.3508 - precision: 0.7390 - recall: 0.2340 - val_loss: 1.3305 - val_accuracy: 0.3875 - val_f1: 0.2818 - val_precision: 0.5886 - val_recall: 0.1858\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.1058 - accuracy: 0.5061 - f1: 0.3880 - precision: 0.7663 - recall: 0.2632 - val_loss: 1.3100 - val_accuracy: 0.4083 - val_f1: 0.2962 - val_precision: 0.6435 - val_recall: 0.1933\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.0898 - accuracy: 0.5103 - f1: 0.4020 - precision: 0.7932 - recall: 0.2721 - val_loss: 1.2601 - val_accuracy: 0.4317 - val_f1: 0.3202 - val_precision: 0.6203 - val_recall: 0.2167\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.0582 - accuracy: 0.5157 - f1: 0.4233 - precision: 0.8075 - recall: 0.2910 - val_loss: 1.2742 - val_accuracy: 0.4508 - val_f1: 0.3823 - val_precision: 0.6209 - val_recall: 0.2767\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.0059 - accuracy: 0.5560 - f1: 0.4619 - precision: 0.8067 - recall: 0.3256 - val_loss: 1.3104 - val_accuracy: 0.4408 - val_f1: 0.3687 - val_precision: 0.5916 - val_recall: 0.2683\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.9762 - accuracy: 0.5622 - f1: 0.4673 - precision: 0.8279 - recall: 0.3285 - val_loss: 1.3105 - val_accuracy: 0.4150 - val_f1: 0.3479 - val_precision: 0.6054 - val_recall: 0.2450\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.0155 - accuracy: 0.5412 - f1: 0.4485 - precision: 0.8130 - recall: 0.3157 - val_loss: 1.2309 - val_accuracy: 0.4433 - val_f1: 0.3309 - val_precision: 0.6841 - val_recall: 0.2183\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.9471 - accuracy: 0.5812 - f1: 0.4929 - precision: 0.8443 - recall: 0.3518 - val_loss: 1.2435 - val_accuracy: 0.4542 - val_f1: 0.3328 - val_precision: 0.6713 - val_recall: 0.2217\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.9479 - accuracy: 0.5829 - f1: 0.4886 - precision: 0.8283 - recall: 0.3499 - val_loss: 1.2357 - val_accuracy: 0.4458 - val_f1: 0.3217 - val_precision: 0.7211 - val_recall: 0.2075\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.9415 - accuracy: 0.5850 - f1: 0.5027 - precision: 0.8340 - recall: 0.3645 - val_loss: 1.2617 - val_accuracy: 0.4517 - val_f1: 0.3358 - val_precision: 0.7054 - val_recall: 0.2208\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.8692 - accuracy: 0.6047 - f1: 0.5365 - precision: 0.8790 - recall: 0.3901 - val_loss: 1.2690 - val_accuracy: 0.4567 - val_f1: 0.3518 - val_precision: 0.6963 - val_recall: 0.2358\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.8296 - accuracy: 0.6312 - f1: 0.5555 - precision: 0.8906 - recall: 0.4066 - val_loss: 1.3123 - val_accuracy: 0.4767 - val_f1: 0.3803 - val_precision: 0.6821 - val_recall: 0.2642\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.8118 - accuracy: 0.6437 - f1: 0.5636 - precision: 0.8880 - recall: 0.4144 - val_loss: 1.3128 - val_accuracy: 0.4642 - val_f1: 0.3495 - val_precision: 0.7012 - val_recall: 0.2333\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.7931 - accuracy: 0.6545 - f1: 0.5667 - precision: 0.8865 - recall: 0.4187 - val_loss: 1.3170 - val_accuracy: 0.4583 - val_f1: 0.3734 - val_precision: 0.7125 - val_recall: 0.2533\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.7624 - accuracy: 0.6611 - f1: 0.5841 - precision: 0.8860 - recall: 0.4376 - val_loss: 1.3755 - val_accuracy: 0.4767 - val_f1: 0.3848 - val_precision: 0.6373 - val_recall: 0.2758\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.7292 - accuracy: 0.6815 - f1: 0.5958 - precision: 0.9021 - recall: 0.4464 - val_loss: 1.3440 - val_accuracy: 0.4633 - val_f1: 0.3694 - val_precision: 0.6803 - val_recall: 0.2542\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.7211 - accuracy: 0.6742 - f1: 0.5960 - precision: 0.8796 - recall: 0.4521 - val_loss: 1.3399 - val_accuracy: 0.4625 - val_f1: 0.3799 - val_precision: 0.6894 - val_recall: 0.2625\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.7579 - accuracy: 0.6657 - f1: 0.5916 - precision: 0.8905 - recall: 0.4460 - val_loss: 1.3254 - val_accuracy: 0.4425 - val_f1: 0.3459 - val_precision: 0.6669 - val_recall: 0.2342\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.7866 - accuracy: 0.6537 - f1: 0.5769 - precision: 0.8746 - recall: 0.4338 - val_loss: 1.2803 - val_accuracy: 0.4542 - val_f1: 0.3626 - val_precision: 0.7230 - val_recall: 0.2425\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.6947 - accuracy: 0.6907 - f1: 0.6115 - precision: 0.9085 - recall: 0.4641 - val_loss: 1.3900 - val_accuracy: 0.4683 - val_f1: 0.3793 - val_precision: 0.7054 - val_recall: 0.2600\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.6477 - accuracy: 0.6998 - f1: 0.6409 - precision: 0.8608 - recall: 0.5127 - val_loss: 1.4163 - val_accuracy: 0.4800 - val_f1: 0.3928 - val_precision: 0.6526 - val_recall: 0.2817\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.6295 - accuracy: 0.7218 - f1: 0.6737 - precision: 0.8463 - recall: 0.5620 - val_loss: 1.3806 - val_accuracy: 0.4892 - val_f1: 0.4074 - val_precision: 0.7001 - val_recall: 0.2883\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.6172 - accuracy: 0.7310 - f1: 0.6715 - precision: 0.8454 - recall: 0.5584 - val_loss: 1.4164 - val_accuracy: 0.4683 - val_f1: 0.4047 - val_precision: 0.6171 - val_recall: 0.3017\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.6332 - accuracy: 0.7152 - f1: 0.6630 - precision: 0.8334 - recall: 0.5525 - val_loss: 1.4526 - val_accuracy: 0.4692 - val_f1: 0.4260 - val_precision: 0.5679 - val_recall: 0.3417\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.6562 - accuracy: 0.7123 - f1: 0.6629 - precision: 0.8222 - recall: 0.5566 - val_loss: 1.5210 - val_accuracy: 0.4808 - val_f1: 0.4527 - val_precision: 0.5389 - val_recall: 0.3908\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.6674 - accuracy: 0.7041 - f1: 0.6636 - precision: 0.8122 - recall: 0.5624 - val_loss: 1.3847 - val_accuracy: 0.4633 - val_f1: 0.3909 - val_precision: 0.6690 - val_recall: 0.2767\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.6688 - accuracy: 0.7026 - f1: 0.6450 - precision: 0.8450 - recall: 0.5233 - val_loss: 1.4146 - val_accuracy: 0.4775 - val_f1: 0.3929 - val_precision: 0.6668 - val_recall: 0.2792\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.6019 - accuracy: 0.7227 - f1: 0.6763 - precision: 0.8732 - recall: 0.5531 - val_loss: 1.5679 - val_accuracy: 0.4917 - val_f1: 0.4758 - val_precision: 0.5740 - val_recall: 0.4067\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.5761 - accuracy: 0.7382 - f1: 0.7004 - precision: 0.8354 - recall: 0.6042 - val_loss: 1.6045 - val_accuracy: 0.4808 - val_f1: 0.4588 - val_precision: 0.5494 - val_recall: 0.3942\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.5754 - accuracy: 0.7397 - f1: 0.6959 - precision: 0.8334 - recall: 0.5992 - val_loss: 1.6005 - val_accuracy: 0.4933 - val_f1: 0.4526 - val_precision: 0.5720 - val_recall: 0.3750\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.5865 - accuracy: 0.7246 - f1: 0.6902 - precision: 0.8211 - recall: 0.5964 - val_loss: 1.5788 - val_accuracy: 0.4817 - val_f1: 0.4650 - val_precision: 0.5589 - val_recall: 0.3983\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.6106 - accuracy: 0.7246 - f1: 0.6848 - precision: 0.8160 - recall: 0.5911 - val_loss: 1.6029 - val_accuracy: 0.4783 - val_f1: 0.4688 - val_precision: 0.5317 - val_recall: 0.4200\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.6080 - accuracy: 0.7212 - f1: 0.6964 - precision: 0.8174 - recall: 0.6080 - val_loss: 1.6125 - val_accuracy: 0.4683 - val_f1: 0.4535 - val_precision: 0.5029 - val_recall: 0.4133\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.5800 - accuracy: 0.7248 - f1: 0.6980 - precision: 0.8194 - recall: 0.6102 - val_loss: 1.6540 - val_accuracy: 0.4300 - val_f1: 0.4061 - val_precision: 0.4675 - val_recall: 0.3592\n",
            "14/14 [==============================] - 1s 87ms/step - loss: 1.4927 - accuracy: 0.4871 - f1: 0.4693 - precision: 0.5416 - recall: 0.4143\n",
            "[1.4927319288253784, 0.4871428608894348, 0.469251424074173, 0.5415894389152527, 0.41428571939468384]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHtKjzc1SA8",
        "outputId": "e086b964-93d5-470a-b1be-921aa2e5b367"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 10s 141ms/step - loss: 1.3818 - accuracy: 0.2805 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3663 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.3649 - accuracy: 0.3184 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3492 - val_accuracy: 0.3675 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.3245 - accuracy: 0.3743 - f1: 0.0029 - precision: 0.0640 - recall: 0.0015 - val_loss: 1.3173 - val_accuracy: 0.3892 - val_f1: 0.0840 - val_precision: 0.5328 - val_recall: 0.0458\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2471 - accuracy: 0.4520 - f1: 0.1397 - precision: 0.6936 - recall: 0.0802 - val_loss: 1.3430 - val_accuracy: 0.3975 - val_f1: 0.2673 - val_precision: 0.4759 - val_recall: 0.1867\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.1771 - accuracy: 0.4944 - f1: 0.2851 - precision: 0.6403 - recall: 0.1858 - val_loss: 1.2969 - val_accuracy: 0.4133 - val_f1: 0.2942 - val_precision: 0.5072 - val_recall: 0.2083\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1226 - accuracy: 0.5315 - f1: 0.3906 - precision: 0.6623 - recall: 0.2784 - val_loss: 1.2884 - val_accuracy: 0.4308 - val_f1: 0.3167 - val_precision: 0.5089 - val_recall: 0.2308\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0993 - accuracy: 0.5422 - f1: 0.4146 - precision: 0.6725 - recall: 0.3016 - val_loss: 1.2534 - val_accuracy: 0.4575 - val_f1: 0.3439 - val_precision: 0.5343 - val_recall: 0.2542\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0646 - accuracy: 0.5676 - f1: 0.4710 - precision: 0.6901 - recall: 0.3590 - val_loss: 1.2344 - val_accuracy: 0.4700 - val_f1: 0.3481 - val_precision: 0.5709 - val_recall: 0.2517\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.0340 - accuracy: 0.5881 - f1: 0.4862 - precision: 0.7144 - recall: 0.3720 - val_loss: 1.2596 - val_accuracy: 0.4675 - val_f1: 0.3715 - val_precision: 0.5314 - val_recall: 0.2867\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0139 - accuracy: 0.5859 - f1: 0.5170 - precision: 0.7214 - recall: 0.4046 - val_loss: 1.2237 - val_accuracy: 0.4900 - val_f1: 0.3649 - val_precision: 0.5710 - val_recall: 0.2692\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.9784 - accuracy: 0.6019 - f1: 0.5414 - precision: 0.7349 - recall: 0.4328 - val_loss: 1.2324 - val_accuracy: 0.4858 - val_f1: 0.3615 - val_precision: 0.5699 - val_recall: 0.2658\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.9451 - accuracy: 0.6242 - f1: 0.5568 - precision: 0.7593 - recall: 0.4446 - val_loss: 1.2406 - val_accuracy: 0.4308 - val_f1: 0.3011 - val_precision: 0.5889 - val_recall: 0.2033\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.9592 - accuracy: 0.6030 - f1: 0.5379 - precision: 0.7518 - recall: 0.4260 - val_loss: 1.2189 - val_accuracy: 0.4600 - val_f1: 0.3391 - val_precision: 0.6063 - val_recall: 0.2367\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.9060 - accuracy: 0.6402 - f1: 0.5808 - precision: 0.7799 - recall: 0.4731 - val_loss: 1.2363 - val_accuracy: 0.4775 - val_f1: 0.3912 - val_precision: 0.5706 - val_recall: 0.2983\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.8733 - accuracy: 0.6545 - f1: 0.5980 - precision: 0.7851 - recall: 0.4906 - val_loss: 1.2487 - val_accuracy: 0.4842 - val_f1: 0.4340 - val_precision: 0.5792 - val_recall: 0.3475\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.8129 - accuracy: 0.6844 - f1: 0.6465 - precision: 0.8068 - recall: 0.5422 - val_loss: 1.2532 - val_accuracy: 0.4992 - val_f1: 0.4206 - val_precision: 0.5664 - val_recall: 0.3350\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.7829 - accuracy: 0.6984 - f1: 0.6517 - precision: 0.7994 - recall: 0.5521 - val_loss: 1.2734 - val_accuracy: 0.5050 - val_f1: 0.4348 - val_precision: 0.5794 - val_recall: 0.3483\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.7518 - accuracy: 0.7024 - f1: 0.6774 - precision: 0.8282 - recall: 0.5760 - val_loss: 1.2839 - val_accuracy: 0.5150 - val_f1: 0.4495 - val_precision: 0.5775 - val_recall: 0.3683\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.7224 - accuracy: 0.7130 - f1: 0.6920 - precision: 0.8267 - recall: 0.5977 - val_loss: 1.2704 - val_accuracy: 0.5083 - val_f1: 0.4503 - val_precision: 0.5807 - val_recall: 0.3683\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.6880 - accuracy: 0.7279 - f1: 0.7109 - precision: 0.8251 - recall: 0.6263 - val_loss: 1.2947 - val_accuracy: 0.4917 - val_f1: 0.4621 - val_precision: 0.5772 - val_recall: 0.3858\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.6621 - accuracy: 0.7463 - f1: 0.7336 - precision: 0.8468 - recall: 0.6493 - val_loss: 1.3031 - val_accuracy: 0.5067 - val_f1: 0.4647 - val_precision: 0.5890 - val_recall: 0.3842\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.6463 - accuracy: 0.7538 - f1: 0.7359 - precision: 0.8459 - recall: 0.6537 - val_loss: 1.3754 - val_accuracy: 0.5125 - val_f1: 0.4702 - val_precision: 0.5748 - val_recall: 0.3983\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.6352 - accuracy: 0.7665 - f1: 0.7405 - precision: 0.8478 - recall: 0.6607 - val_loss: 1.2767 - val_accuracy: 0.5292 - val_f1: 0.4884 - val_precision: 0.5889 - val_recall: 0.4175\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.5889 - accuracy: 0.7756 - f1: 0.7680 - precision: 0.8577 - recall: 0.6966 - val_loss: 1.3132 - val_accuracy: 0.5417 - val_f1: 0.5085 - val_precision: 0.5865 - val_recall: 0.4492\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.5452 - accuracy: 0.7985 - f1: 0.7856 - precision: 0.8635 - recall: 0.7220 - val_loss: 1.3595 - val_accuracy: 0.5192 - val_f1: 0.4906 - val_precision: 0.5766 - val_recall: 0.4275\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.5570 - accuracy: 0.7876 - f1: 0.7828 - precision: 0.8524 - recall: 0.7245 - val_loss: 1.4168 - val_accuracy: 0.5350 - val_f1: 0.5253 - val_precision: 0.5936 - val_recall: 0.4717\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.5025 - accuracy: 0.8061 - f1: 0.8002 - precision: 0.8689 - recall: 0.7429 - val_loss: 1.3879 - val_accuracy: 0.5225 - val_f1: 0.4980 - val_precision: 0.5896 - val_recall: 0.4317\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.4868 - accuracy: 0.8132 - f1: 0.8155 - precision: 0.8774 - recall: 0.7644 - val_loss: 1.3369 - val_accuracy: 0.5408 - val_f1: 0.5259 - val_precision: 0.5959 - val_recall: 0.4708\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.4544 - accuracy: 0.8326 - f1: 0.8291 - precision: 0.8808 - recall: 0.7848 - val_loss: 1.3797 - val_accuracy: 0.5533 - val_f1: 0.5309 - val_precision: 0.5918 - val_recall: 0.4817\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.4110 - accuracy: 0.8449 - f1: 0.8422 - precision: 0.8839 - recall: 0.8048 - val_loss: 1.3967 - val_accuracy: 0.5425 - val_f1: 0.5183 - val_precision: 0.5795 - val_recall: 0.4692\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4268 - accuracy: 0.8418 - f1: 0.8316 - precision: 0.8704 - recall: 0.7971 - val_loss: 1.4259 - val_accuracy: 0.5417 - val_f1: 0.5313 - val_precision: 0.5903 - val_recall: 0.4833\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.3821 - accuracy: 0.8680 - f1: 0.8636 - precision: 0.9057 - recall: 0.8258 - val_loss: 1.4149 - val_accuracy: 0.5750 - val_f1: 0.5651 - val_precision: 0.6156 - val_recall: 0.5225\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.3408 - accuracy: 0.8756 - f1: 0.8730 - precision: 0.9087 - recall: 0.8403 - val_loss: 1.5049 - val_accuracy: 0.5608 - val_f1: 0.5538 - val_precision: 0.5914 - val_recall: 0.5208\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3563 - accuracy: 0.8690 - f1: 0.8711 - precision: 0.9002 - recall: 0.8441 - val_loss: 1.5029 - val_accuracy: 0.5692 - val_f1: 0.5514 - val_precision: 0.5997 - val_recall: 0.5108\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3413 - accuracy: 0.8774 - f1: 0.8731 - precision: 0.9061 - recall: 0.8429 - val_loss: 1.6094 - val_accuracy: 0.5750 - val_f1: 0.5632 - val_precision: 0.5979 - val_recall: 0.5325\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3245 - accuracy: 0.8822 - f1: 0.8834 - precision: 0.9070 - recall: 0.8611 - val_loss: 1.7211 - val_accuracy: 0.5600 - val_f1: 0.5535 - val_precision: 0.5865 - val_recall: 0.5242\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2908 - accuracy: 0.8951 - f1: 0.8959 - precision: 0.9208 - recall: 0.8726 - val_loss: 1.6558 - val_accuracy: 0.5775 - val_f1: 0.5738 - val_precision: 0.6019 - val_recall: 0.5483\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2682 - accuracy: 0.9065 - f1: 0.9075 - precision: 0.9259 - recall: 0.8900 - val_loss: 1.7232 - val_accuracy: 0.5817 - val_f1: 0.5749 - val_precision: 0.6072 - val_recall: 0.5458\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2599 - accuracy: 0.9154 - f1: 0.9090 - precision: 0.9322 - recall: 0.8872 - val_loss: 1.6681 - val_accuracy: 0.6042 - val_f1: 0.5978 - val_precision: 0.6266 - val_recall: 0.5717\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2623 - accuracy: 0.9123 - f1: 0.9110 - precision: 0.9324 - recall: 0.8908 - val_loss: 1.8130 - val_accuracy: 0.5992 - val_f1: 0.5957 - val_precision: 0.6192 - val_recall: 0.5742\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2397 - accuracy: 0.9117 - f1: 0.9126 - precision: 0.9313 - recall: 0.8948 - val_loss: 1.8901 - val_accuracy: 0.5992 - val_f1: 0.5940 - val_precision: 0.6115 - val_recall: 0.5775\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.2306 - accuracy: 0.9204 - f1: 0.9179 - precision: 0.9360 - recall: 0.9007 - val_loss: 2.0438 - val_accuracy: 0.5825 - val_f1: 0.5752 - val_precision: 0.5952 - val_recall: 0.5567\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.2095 - accuracy: 0.9273 - f1: 0.9262 - precision: 0.9392 - recall: 0.9138 - val_loss: 1.8409 - val_accuracy: 0.5883 - val_f1: 0.5891 - val_precision: 0.6078 - val_recall: 0.5717\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.2188 - accuracy: 0.9220 - f1: 0.9225 - precision: 0.9350 - recall: 0.9105 - val_loss: 2.1661 - val_accuracy: 0.5817 - val_f1: 0.5735 - val_precision: 0.5924 - val_recall: 0.5558\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2014 - accuracy: 0.9255 - f1: 0.9282 - precision: 0.9406 - recall: 0.9163 - val_loss: 2.1726 - val_accuracy: 0.5942 - val_f1: 0.5907 - val_precision: 0.6103 - val_recall: 0.5725\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.1997 - accuracy: 0.9287 - f1: 0.9307 - precision: 0.9421 - recall: 0.9197 - val_loss: 2.1535 - val_accuracy: 0.5775 - val_f1: 0.5765 - val_precision: 0.5915 - val_recall: 0.5625\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.2316 - accuracy: 0.9188 - f1: 0.9152 - precision: 0.9270 - recall: 0.9037 - val_loss: 2.1982 - val_accuracy: 0.6067 - val_f1: 0.5979 - val_precision: 0.6162 - val_recall: 0.5808\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1890 - accuracy: 0.9361 - f1: 0.9362 - precision: 0.9451 - recall: 0.9276 - val_loss: 2.0959 - val_accuracy: 0.5983 - val_f1: 0.5962 - val_precision: 0.6125 - val_recall: 0.5808\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1993 - accuracy: 0.9311 - f1: 0.9302 - precision: 0.9417 - recall: 0.9191 - val_loss: 2.0358 - val_accuracy: 0.6000 - val_f1: 0.5934 - val_precision: 0.6103 - val_recall: 0.5775\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1925 - accuracy: 0.9276 - f1: 0.9266 - precision: 0.9365 - recall: 0.9171 - val_loss: 2.0019 - val_accuracy: 0.6000 - val_f1: 0.5954 - val_precision: 0.6129 - val_recall: 0.5792\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1703 - accuracy: 0.9390 - f1: 0.9402 - precision: 0.9479 - recall: 0.9328 - val_loss: 2.1470 - val_accuracy: 0.6050 - val_f1: 0.6047 - val_precision: 0.6185 - val_recall: 0.5917\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1449 - accuracy: 0.9491 - f1: 0.9504 - precision: 0.9589 - recall: 0.9420 - val_loss: 2.3819 - val_accuracy: 0.5950 - val_f1: 0.5925 - val_precision: 0.6085 - val_recall: 0.5775\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1553 - accuracy: 0.9466 - f1: 0.9465 - precision: 0.9534 - recall: 0.9397 - val_loss: 2.3352 - val_accuracy: 0.5975 - val_f1: 0.5937 - val_precision: 0.6064 - val_recall: 0.5817\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1587 - accuracy: 0.9444 - f1: 0.9445 - precision: 0.9529 - recall: 0.9363 - val_loss: 2.2308 - val_accuracy: 0.6217 - val_f1: 0.6209 - val_precision: 0.6304 - val_recall: 0.6117\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1524 - accuracy: 0.9492 - f1: 0.9491 - precision: 0.9570 - recall: 0.9414 - val_loss: 2.3514 - val_accuracy: 0.6050 - val_f1: 0.6079 - val_precision: 0.6196 - val_recall: 0.5967\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1405 - accuracy: 0.9495 - f1: 0.9474 - precision: 0.9554 - recall: 0.9395 - val_loss: 2.1835 - val_accuracy: 0.6125 - val_f1: 0.6093 - val_precision: 0.6181 - val_recall: 0.6008\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1258 - accuracy: 0.9559 - f1: 0.9551 - precision: 0.9628 - recall: 0.9475 - val_loss: 2.2456 - val_accuracy: 0.6083 - val_f1: 0.6127 - val_precision: 0.6259 - val_recall: 0.6000\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1195 - accuracy: 0.9583 - f1: 0.9597 - precision: 0.9675 - recall: 0.9522 - val_loss: 2.3752 - val_accuracy: 0.6225 - val_f1: 0.6203 - val_precision: 0.6275 - val_recall: 0.6133\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1448 - accuracy: 0.9531 - f1: 0.9538 - precision: 0.9593 - recall: 0.9484 - val_loss: 2.2575 - val_accuracy: 0.6158 - val_f1: 0.6138 - val_precision: 0.6247 - val_recall: 0.6033\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1401 - accuracy: 0.9508 - f1: 0.9513 - precision: 0.9575 - recall: 0.9452 - val_loss: 2.2601 - val_accuracy: 0.6283 - val_f1: 0.6263 - val_precision: 0.6337 - val_recall: 0.6192\n",
            "14/14 [==============================] - 1s 86ms/step - loss: 2.2466 - accuracy: 0.5964 - f1: 0.5946 - precision: 0.6031 - recall: 0.5864\n",
            "[2.246607780456543, 0.5964285731315613, 0.5946375131607056, 0.6031407713890076, 0.5864285826683044]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e239cc85-848b-4976-bdd2-3f40a949411f"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 154ms/step - loss: 1.3870 - accuracy: 0.2650 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3712 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3768 - accuracy: 0.3036 - f1: 1.8318e-05 - precision: 9.2507e-04 - recall: 9.2507e-06 - val_loss: 1.3826 - val_accuracy: 0.3067 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3800 - accuracy: 0.2923 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3687 - val_accuracy: 0.3325 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3819 - accuracy: 0.3017 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3618 - val_accuracy: 0.3408 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.3731 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3795 - val_accuracy: 0.3117 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3807 - accuracy: 0.3050 - f1: 1.8683e-04 - precision: 0.0094 - recall: 9.4349e-05 - val_loss: 1.3637 - val_accuracy: 0.3417 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3709 - accuracy: 0.3171 - f1: 0.0016 - precision: 0.0381 - recall: 8.5326e-04 - val_loss: 1.3767 - val_accuracy: 0.2958 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3774 - accuracy: 0.3093 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3651 - val_accuracy: 0.3400 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3682 - accuracy: 0.3153 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3648 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 7s 143ms/step - loss: 1.3661 - accuracy: 0.3251 - f1: 4.3132e-04 - precision: 0.0218 - recall: 2.1782e-04 - val_loss: 1.3915 - val_accuracy: 0.2983 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3789 - accuracy: 0.3116 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3875 - val_accuracy: 0.2867 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 7s 143ms/step - loss: 1.3698 - accuracy: 0.3321 - f1: 0.0032 - precision: 0.1283 - recall: 0.0016 - val_loss: 1.3878 - val_accuracy: 0.2875 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3622 - accuracy: 0.3308 - f1: 0.0251 - precision: 0.3094 - recall: 0.0137 - val_loss: 1.3880 - val_accuracy: 0.2633 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3664 - accuracy: 0.3314 - f1: 0.0064 - precision: 0.0660 - recall: 0.0034 - val_loss: 1.3743 - val_accuracy: 0.3150 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3740 - accuracy: 0.2989 - f1: 1.9761e-04 - precision: 0.0050 - recall: 1.0078e-04 - val_loss: 1.3796 - val_accuracy: 0.3008 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 7s 143ms/step - loss: 1.3658 - accuracy: 0.3160 - f1: 9.3741e-04 - precision: 0.0243 - recall: 4.8120e-04 - val_loss: 1.3655 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3591 - accuracy: 0.3273 - f1: 0.0085 - precision: 0.1180 - recall: 0.0045 - val_loss: 1.3612 - val_accuracy: 0.3267 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3419 - accuracy: 0.3348 - f1: 0.0150 - precision: 0.3048 - recall: 0.0078 - val_loss: 1.3984 - val_accuracy: 0.3067 - val_f1: 0.0284 - val_precision: 0.2929 - val_recall: 0.0150\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3303 - accuracy: 0.3768 - f1: 0.0539 - precision: 0.4882 - recall: 0.0290 - val_loss: 1.4006 - val_accuracy: 0.3075 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3258 - accuracy: 0.3738 - f1: 0.0774 - precision: 0.4874 - recall: 0.0434 - val_loss: 1.3749 - val_accuracy: 0.3008 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3514 - accuracy: 0.3426 - f1: 0.0295 - precision: 0.3168 - recall: 0.0159 - val_loss: 1.3874 - val_accuracy: 0.3158 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3872 - accuracy: 0.2985 - f1: 0.0077 - precision: 0.0429 - recall: 0.0043 - val_loss: 1.3821 - val_accuracy: 0.3225 - val_f1: 0.0472 - val_precision: 0.5016 - val_recall: 0.0250\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3622 - accuracy: 0.3456 - f1: 0.0177 - precision: 0.1335 - recall: 0.0097 - val_loss: 1.3881 - val_accuracy: 0.3108 - val_f1: 0.0574 - val_precision: 0.4600 - val_recall: 0.0308\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3725 - accuracy: 0.3281 - f1: 0.0357 - precision: 0.2072 - recall: 0.0199 - val_loss: 1.3767 - val_accuracy: 0.3200 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3549 - accuracy: 0.3352 - f1: 0.0028 - precision: 0.0996 - recall: 0.0014 - val_loss: 1.3798 - val_accuracy: 0.3258 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3641 - accuracy: 0.3300 - f1: 3.7256e-05 - precision: 0.0019 - recall: 1.8814e-05 - val_loss: 1.3805 - val_accuracy: 0.3108 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3541 - accuracy: 0.3345 - f1: 0.0078 - precision: 0.2564 - recall: 0.0040 - val_loss: 1.3693 - val_accuracy: 0.3217 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3420 - accuracy: 0.3510 - f1: 0.0131 - precision: 0.3026 - recall: 0.0067 - val_loss: 1.3648 - val_accuracy: 0.3192 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3501 - accuracy: 0.3449 - f1: 5.3936e-04 - precision: 0.0189 - recall: 2.7465e-04 - val_loss: 1.3604 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3314 - accuracy: 0.3607 - f1: 0.0173 - precision: 0.5058 - recall: 0.0089 - val_loss: 1.3715 - val_accuracy: 0.3125 - val_f1: 0.0456 - val_precision: 0.4775 - val_recall: 0.0242\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3424 - accuracy: 0.3392 - f1: 0.0650 - precision: 0.4901 - recall: 0.0352 - val_loss: 1.3700 - val_accuracy: 0.3158 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3387 - accuracy: 0.3551 - f1: 0.0176 - precision: 0.2821 - recall: 0.0092 - val_loss: 1.3626 - val_accuracy: 0.3117 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3147 - accuracy: 0.3805 - f1: 0.0412 - precision: 0.5495 - recall: 0.0220 - val_loss: 1.3674 - val_accuracy: 0.3200 - val_f1: 0.0402 - val_precision: 0.6306 - val_recall: 0.0208\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 7s 143ms/step - loss: 1.2977 - accuracy: 0.3914 - f1: 0.1013 - precision: 0.5933 - recall: 0.0565 - val_loss: 1.3798 - val_accuracy: 0.3050 - val_f1: 0.0304 - val_precision: 0.4889 - val_recall: 0.0158\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2871 - accuracy: 0.3992 - f1: 0.1126 - precision: 0.5486 - recall: 0.0638 - val_loss: 1.3842 - val_accuracy: 0.3067 - val_f1: 0.0130 - val_precision: 0.4111 - val_recall: 0.0067\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3396 - accuracy: 0.3551 - f1: 0.0531 - precision: 0.4519 - recall: 0.0286 - val_loss: 1.4018 - val_accuracy: 0.3042 - val_f1: 0.0129 - val_precision: 0.2639 - val_recall: 0.0067\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3571 - accuracy: 0.3242 - f1: 0.0285 - precision: 0.3385 - recall: 0.0153 - val_loss: 1.3890 - val_accuracy: 0.2950 - val_f1: 0.0049 - val_precision: 0.1250 - val_recall: 0.0025\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3335 - accuracy: 0.3623 - f1: 0.0400 - precision: 0.4365 - recall: 0.0212 - val_loss: 1.3654 - val_accuracy: 0.3142 - val_f1: 0.0534 - val_precision: 0.5278 - val_recall: 0.0283\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3384 - accuracy: 0.3515 - f1: 0.0754 - precision: 0.4774 - recall: 0.0413 - val_loss: 1.3549 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3333 - accuracy: 0.3665 - f1: 0.0369 - precision: 0.5919 - recall: 0.0193 - val_loss: 1.3565 - val_accuracy: 0.3417 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3288 - accuracy: 0.3841 - f1: 0.0203 - precision: 0.3519 - recall: 0.0107 - val_loss: 1.3708 - val_accuracy: 0.3233 - val_f1: 0.0653 - val_precision: 0.4415 - val_recall: 0.0358\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3143 - accuracy: 0.3843 - f1: 0.0645 - precision: 0.4784 - recall: 0.0349 - val_loss: 1.3715 - val_accuracy: 0.3192 - val_f1: 0.0584 - val_precision: 0.3847 - val_recall: 0.0317\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3198 - accuracy: 0.3745 - f1: 0.0775 - precision: 0.5385 - recall: 0.0426 - val_loss: 1.3607 - val_accuracy: 0.3358 - val_f1: 0.0546 - val_precision: 0.4454 - val_recall: 0.0292\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3146 - accuracy: 0.3746 - f1: 0.0798 - precision: 0.5680 - recall: 0.0436 - val_loss: 1.3920 - val_accuracy: 0.3042 - val_f1: 0.0762 - val_precision: 0.4127 - val_recall: 0.0425\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3080 - accuracy: 0.3873 - f1: 0.1107 - precision: 0.5342 - recall: 0.0632 - val_loss: 1.3846 - val_accuracy: 0.3142 - val_f1: 0.0396 - val_precision: 0.4460 - val_recall: 0.0208\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.3123 - accuracy: 0.3810 - f1: 0.0783 - precision: 0.5109 - recall: 0.0425 - val_loss: 1.3742 - val_accuracy: 0.3400 - val_f1: 0.0192 - val_precision: 0.2649 - val_recall: 0.0100\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3093 - accuracy: 0.3836 - f1: 0.0514 - precision: 0.5066 - recall: 0.0274 - val_loss: 1.3715 - val_accuracy: 0.3517 - val_f1: 0.0389 - val_precision: 0.3339 - val_recall: 0.0208\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3016 - accuracy: 0.4006 - f1: 0.0725 - precision: 0.5235 - recall: 0.0394 - val_loss: 1.3704 - val_accuracy: 0.3317 - val_f1: 0.0568 - val_precision: 0.6025 - val_recall: 0.0300\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3049 - accuracy: 0.3969 - f1: 0.0725 - precision: 0.6203 - recall: 0.0391 - val_loss: 1.3785 - val_accuracy: 0.3142 - val_f1: 0.0130 - val_precision: 0.2500 - val_recall: 0.0067\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3174 - accuracy: 0.3856 - f1: 0.0600 - precision: 0.5561 - recall: 0.0322 - val_loss: 1.3588 - val_accuracy: 0.3408 - val_f1: 0.0549 - val_precision: 0.4865 - val_recall: 0.0292\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.2936 - accuracy: 0.4034 - f1: 0.0982 - precision: 0.6506 - recall: 0.0539 - val_loss: 1.3795 - val_accuracy: 0.3242 - val_f1: 0.0846 - val_precision: 0.4032 - val_recall: 0.0475\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2947 - accuracy: 0.3947 - f1: 0.1384 - precision: 0.5803 - recall: 0.0793 - val_loss: 1.3659 - val_accuracy: 0.3292 - val_f1: 0.0726 - val_precision: 0.4387 - val_recall: 0.0400\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2841 - accuracy: 0.4171 - f1: 0.1274 - precision: 0.6013 - recall: 0.0726 - val_loss: 1.3756 - val_accuracy: 0.3300 - val_f1: 0.1016 - val_precision: 0.4513 - val_recall: 0.0575\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2860 - accuracy: 0.4156 - f1: 0.1531 - precision: 0.6026 - recall: 0.0890 - val_loss: 1.3910 - val_accuracy: 0.3167 - val_f1: 0.0388 - val_precision: 0.3100 - val_recall: 0.0208\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2985 - accuracy: 0.3942 - f1: 0.1104 - precision: 0.5553 - recall: 0.0633 - val_loss: 1.3667 - val_accuracy: 0.3383 - val_f1: 0.0453 - val_precision: 0.3945 - val_recall: 0.0242\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 7s 143ms/step - loss: 1.2807 - accuracy: 0.4172 - f1: 0.1100 - precision: 0.5788 - recall: 0.0637 - val_loss: 1.3774 - val_accuracy: 0.3317 - val_f1: 0.0389 - val_precision: 0.3304 - val_recall: 0.0208\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2787 - accuracy: 0.4191 - f1: 0.1004 - precision: 0.5308 - recall: 0.0565 - val_loss: 1.3875 - val_accuracy: 0.3425 - val_f1: 0.1301 - val_precision: 0.3901 - val_recall: 0.0783\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2629 - accuracy: 0.4351 - f1: 0.1912 - precision: 0.5927 - recall: 0.1151 - val_loss: 1.3813 - val_accuracy: 0.3292 - val_f1: 0.0736 - val_precision: 0.3499 - val_recall: 0.0417\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2734 - accuracy: 0.4211 - f1: 0.1524 - precision: 0.5409 - recall: 0.0908 - val_loss: 1.3920 - val_accuracy: 0.3333 - val_f1: 0.1393 - val_precision: 0.3829 - val_recall: 0.0858\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2612 - accuracy: 0.4350 - f1: 0.2065 - precision: 0.5602 - recall: 0.1275 - val_loss: 1.4237 - val_accuracy: 0.3033 - val_f1: 0.1314 - val_precision: 0.3854 - val_recall: 0.0800\n",
            "14/14 [==============================] - 1s 85ms/step - loss: 1.4547 - accuracy: 0.2893 - f1: 0.1433 - precision: 0.3671 - recall: 0.0893\n",
            "[1.4546726942062378, 0.28928571939468384, 0.14325760304927826, 0.36708900332450867, 0.08928573131561279]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebfbc847-078a-4b7d-d855-783302ad3983"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(64))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 129ms/step - loss: 1.3817 - accuracy: 0.2974 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3725 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3792 - accuracy: 0.3054 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3704 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3774 - accuracy: 0.3105 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3703 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3747 - accuracy: 0.3164 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3705 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3762 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3691 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.3768 - accuracy: 0.3119 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3718 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3755 - accuracy: 0.3144 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3726 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3772 - accuracy: 0.3113 - f1: 1.6007e-04 - precision: 0.0081 - recall: 8.0836e-05 - val_loss: 1.3693 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3747 - accuracy: 0.3150 - f1: 1.7324e-04 - precision: 0.0087 - recall: 8.7485e-05 - val_loss: 1.3714 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3769 - accuracy: 0.3110 - f1: 1.8683e-04 - precision: 0.0094 - recall: 9.4349e-05 - val_loss: 1.3706 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.3754 - accuracy: 0.3147 - f1: 2.0087e-04 - precision: 0.0101 - recall: 1.0144e-04 - val_loss: 1.3710 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3758 - accuracy: 0.3107 - f1: 2.1540e-04 - precision: 0.0109 - recall: 1.0878e-04 - val_loss: 1.3710 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3760 - accuracy: 0.3104 - f1: 2.3045e-04 - precision: 0.0116 - recall: 1.1638e-04 - val_loss: 1.3686 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3768 - accuracy: 0.3105 - f1: 2.4605e-04 - precision: 0.0124 - recall: 1.2426e-04 - val_loss: 1.3701 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3742 - accuracy: 0.3187 - f1: 2.6226e-04 - precision: 0.0132 - recall: 1.3244e-04 - val_loss: 1.3697 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3758 - accuracy: 0.3118 - f1: 2.7911e-04 - precision: 0.0141 - recall: 1.4095e-04 - val_loss: 1.3690 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.3758 - accuracy: 0.3137 - f1: 2.9667e-04 - precision: 0.0150 - recall: 1.4982e-04 - val_loss: 1.3711 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3744 - accuracy: 0.3159 - f1: 3.1498e-04 - precision: 0.0159 - recall: 1.5907e-04 - val_loss: 1.3714 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3687 - accuracy: 0.3294 - f1: 0.0068 - precision: 0.0426 - recall: 0.0038 - val_loss: 1.3549 - val_accuracy: 0.3517 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3567 - accuracy: 0.3311 - f1: 0.0539 - precision: 0.3165 - recall: 0.0314 - val_loss: 1.3601 - val_accuracy: 0.3117 - val_f1: 0.0728 - val_precision: 0.5706 - val_recall: 0.0392\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3288 - accuracy: 0.3610 - f1: 0.1213 - precision: 0.6218 - recall: 0.0727 - val_loss: 1.3525 - val_accuracy: 0.3300 - val_f1: 0.1071 - val_precision: 0.5808 - val_recall: 0.0592\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.3157 - accuracy: 0.3682 - f1: 0.1907 - precision: 0.6428 - recall: 0.1147 - val_loss: 1.3556 - val_accuracy: 0.3092 - val_f1: 0.1187 - val_precision: 0.5658 - val_recall: 0.0667\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3109 - accuracy: 0.3660 - f1: 0.2074 - precision: 0.5933 - recall: 0.1298 - val_loss: 1.3524 - val_accuracy: 0.3800 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3378 - accuracy: 0.3608 - f1: 0.0444 - precision: 0.1770 - recall: 0.0272 - val_loss: 1.3385 - val_accuracy: 0.3517 - val_f1: 0.1203 - val_precision: 0.6403 - val_recall: 0.0667\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2899 - accuracy: 0.3949 - f1: 0.2374 - precision: 0.6100 - recall: 0.1505 - val_loss: 1.3531 - val_accuracy: 0.3158 - val_f1: 0.0481 - val_precision: 0.7236 - val_recall: 0.0250\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.2885 - accuracy: 0.4010 - f1: 0.2289 - precision: 0.6243 - recall: 0.1475 - val_loss: 1.3362 - val_accuracy: 0.3592 - val_f1: 0.0229 - val_precision: 0.7500 - val_recall: 0.0117\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2854 - accuracy: 0.4025 - f1: 0.1683 - precision: 0.6017 - recall: 0.1034 - val_loss: 1.3251 - val_accuracy: 0.3667 - val_f1: 0.1572 - val_precision: 0.6188 - val_recall: 0.0908\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2539 - accuracy: 0.4274 - f1: 0.2512 - precision: 0.6536 - recall: 0.1585 - val_loss: 1.3166 - val_accuracy: 0.3767 - val_f1: 0.1759 - val_precision: 0.6120 - val_recall: 0.1033\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.2521 - accuracy: 0.4256 - f1: 0.2549 - precision: 0.6896 - recall: 0.1594 - val_loss: 1.3220 - val_accuracy: 0.3692 - val_f1: 0.2071 - val_precision: 0.5193 - val_recall: 0.1300\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.2429 - accuracy: 0.4293 - f1: 0.2741 - precision: 0.6823 - recall: 0.1736 - val_loss: 1.3190 - val_accuracy: 0.3875 - val_f1: 0.2154 - val_precision: 0.5535 - val_recall: 0.1342\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.2408 - accuracy: 0.4223 - f1: 0.2729 - precision: 0.6768 - recall: 0.1725 - val_loss: 1.3196 - val_accuracy: 0.3800 - val_f1: 0.1607 - val_precision: 0.5988 - val_recall: 0.0933\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.2317 - accuracy: 0.4363 - f1: 0.2835 - precision: 0.6919 - recall: 0.1811 - val_loss: 1.3020 - val_accuracy: 0.3958 - val_f1: 0.1853 - val_precision: 0.6093 - val_recall: 0.1100\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.2216 - accuracy: 0.4320 - f1: 0.2965 - precision: 0.6937 - recall: 0.1910 - val_loss: 1.3010 - val_accuracy: 0.3742 - val_f1: 0.2311 - val_precision: 0.5620 - val_recall: 0.1458\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.2098 - accuracy: 0.4524 - f1: 0.3127 - precision: 0.7022 - recall: 0.2033 - val_loss: 1.3206 - val_accuracy: 0.3733 - val_f1: 0.2307 - val_precision: 0.5564 - val_recall: 0.1458\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.1890 - accuracy: 0.4509 - f1: 0.3347 - precision: 0.6989 - recall: 0.2250 - val_loss: 1.3051 - val_accuracy: 0.4067 - val_f1: 0.2643 - val_precision: 0.6137 - val_recall: 0.1692\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.1562 - accuracy: 0.4743 - f1: 0.3817 - precision: 0.7478 - recall: 0.2618 - val_loss: 1.3302 - val_accuracy: 0.3925 - val_f1: 0.2561 - val_precision: 0.5930 - val_recall: 0.1642\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.1295 - accuracy: 0.5033 - f1: 0.4031 - precision: 0.7569 - recall: 0.2805 - val_loss: 1.3111 - val_accuracy: 0.3992 - val_f1: 0.2915 - val_precision: 0.5911 - val_recall: 0.1942\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.1148 - accuracy: 0.5147 - f1: 0.4324 - precision: 0.7643 - recall: 0.3067 - val_loss: 1.3855 - val_accuracy: 0.3767 - val_f1: 0.2996 - val_precision: 0.5163 - val_recall: 0.2117\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.1386 - accuracy: 0.5117 - f1: 0.4206 - precision: 0.7233 - recall: 0.3003 - val_loss: 1.2956 - val_accuracy: 0.4208 - val_f1: 0.3315 - val_precision: 0.5822 - val_recall: 0.2325\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.0722 - accuracy: 0.5468 - f1: 0.4503 - precision: 0.7741 - recall: 0.3206 - val_loss: 1.3083 - val_accuracy: 0.4358 - val_f1: 0.3392 - val_precision: 0.6027 - val_recall: 0.2367\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.0435 - accuracy: 0.5559 - f1: 0.4694 - precision: 0.8006 - recall: 0.3354 - val_loss: 1.2883 - val_accuracy: 0.4467 - val_f1: 0.3458 - val_precision: 0.6158 - val_recall: 0.2408\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.0262 - accuracy: 0.5740 - f1: 0.4782 - precision: 0.7937 - recall: 0.3455 - val_loss: 1.3195 - val_accuracy: 0.4275 - val_f1: 0.3639 - val_precision: 0.5650 - val_recall: 0.2692\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.0253 - accuracy: 0.5618 - f1: 0.4783 - precision: 0.8077 - recall: 0.3427 - val_loss: 1.3370 - val_accuracy: 0.4383 - val_f1: 0.3555 - val_precision: 0.5968 - val_recall: 0.2542\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.9975 - accuracy: 0.5805 - f1: 0.5094 - precision: 0.8193 - recall: 0.3726 - val_loss: 1.3244 - val_accuracy: 0.4408 - val_f1: 0.3615 - val_precision: 0.5921 - val_recall: 0.2608\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.9847 - accuracy: 0.5900 - f1: 0.5099 - precision: 0.8375 - recall: 0.3707 - val_loss: 1.3777 - val_accuracy: 0.4367 - val_f1: 0.3618 - val_precision: 0.5696 - val_recall: 0.2658\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.9799 - accuracy: 0.5850 - f1: 0.5095 - precision: 0.8056 - recall: 0.3749 - val_loss: 1.3882 - val_accuracy: 0.4308 - val_f1: 0.3408 - val_precision: 0.6233 - val_recall: 0.2350\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.9606 - accuracy: 0.5980 - f1: 0.5138 - precision: 0.8364 - recall: 0.3736 - val_loss: 1.3333 - val_accuracy: 0.4217 - val_f1: 0.3126 - val_precision: 0.6673 - val_recall: 0.2050\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.9487 - accuracy: 0.5995 - f1: 0.5192 - precision: 0.8451 - recall: 0.3788 - val_loss: 1.3284 - val_accuracy: 0.4300 - val_f1: 0.3414 - val_precision: 0.6627 - val_recall: 0.2308\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.9615 - accuracy: 0.5987 - f1: 0.5199 - precision: 0.8481 - recall: 0.3808 - val_loss: 1.2939 - val_accuracy: 0.4458 - val_f1: 0.3776 - val_precision: 0.6012 - val_recall: 0.2758\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.9681 - accuracy: 0.5943 - f1: 0.5140 - precision: 0.8276 - recall: 0.3759 - val_loss: 1.3597 - val_accuracy: 0.4425 - val_f1: 0.3491 - val_precision: 0.6376 - val_recall: 0.2408\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.8997 - accuracy: 0.6104 - f1: 0.5521 - precision: 0.8566 - recall: 0.4091 - val_loss: 1.3679 - val_accuracy: 0.4525 - val_f1: 0.3567 - val_precision: 0.6293 - val_recall: 0.2492\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.9510 - accuracy: 0.6006 - f1: 0.5236 - precision: 0.8380 - recall: 0.3831 - val_loss: 1.3750 - val_accuracy: 0.4558 - val_f1: 0.3600 - val_precision: 0.6013 - val_recall: 0.2575\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.8856 - accuracy: 0.6270 - f1: 0.5563 - precision: 0.8477 - recall: 0.4157 - val_loss: 1.4320 - val_accuracy: 0.4483 - val_f1: 0.3657 - val_precision: 0.5840 - val_recall: 0.2667\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.8737 - accuracy: 0.6461 - f1: 0.5665 - precision: 0.8547 - recall: 0.4257 - val_loss: 1.5113 - val_accuracy: 0.4300 - val_f1: 0.3534 - val_precision: 0.5607 - val_recall: 0.2583\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.8682 - accuracy: 0.6446 - f1: 0.5687 - precision: 0.8487 - recall: 0.4289 - val_loss: 1.4629 - val_accuracy: 0.4475 - val_f1: 0.3764 - val_precision: 0.5982 - val_recall: 0.2750\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.8798 - accuracy: 0.6332 - f1: 0.5674 - precision: 0.8405 - recall: 0.4302 - val_loss: 1.4096 - val_accuracy: 0.4617 - val_f1: 0.3798 - val_precision: 0.6344 - val_recall: 0.2717\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.8549 - accuracy: 0.6359 - f1: 0.5674 - precision: 0.8432 - recall: 0.4287 - val_loss: 1.4013 - val_accuracy: 0.4517 - val_f1: 0.3787 - val_precision: 0.6137 - val_recall: 0.2742\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.8730 - accuracy: 0.6338 - f1: 0.5676 - precision: 0.8575 - recall: 0.4270 - val_loss: 1.4003 - val_accuracy: 0.4775 - val_f1: 0.4088 - val_precision: 0.5909 - val_recall: 0.3133\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.8452 - accuracy: 0.6600 - f1: 0.5805 - precision: 0.8542 - recall: 0.4415 - val_loss: 1.3804 - val_accuracy: 0.4742 - val_f1: 0.4245 - val_precision: 0.6077 - val_recall: 0.3267\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.8208 - accuracy: 0.6550 - f1: 0.5906 - precision: 0.8616 - recall: 0.4512 - val_loss: 1.4123 - val_accuracy: 0.4750 - val_f1: 0.4232 - val_precision: 0.6030 - val_recall: 0.3267\n",
            "14/14 [==============================] - 1s 82ms/step - loss: 1.3800 - accuracy: 0.4971 - f1: 0.4381 - precision: 0.6133 - recall: 0.3414\n",
            "[1.3800264596939087, 0.49714285135269165, 0.43806618452072144, 0.6133258938789368, 0.3414285480976105]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab62e3c-7a9f-4696-a9ff-03edbb138cb6"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 10s 139ms/step - loss: 1.3810 - accuracy: 0.2923 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3638 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3598 - accuracy: 0.3248 - f1: 4.5999e-05 - precision: 0.0023 - recall: 2.3230e-05 - val_loss: 1.3307 - val_accuracy: 0.3792 - val_f1: 0.0033 - val_precision: 0.1667 - val_recall: 0.0017\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3095 - accuracy: 0.3857 - f1: 0.0449 - precision: 0.6210 - recall: 0.0240 - val_loss: 1.2958 - val_accuracy: 0.3967 - val_f1: 0.0858 - val_precision: 0.5664 - val_recall: 0.0467\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2435 - accuracy: 0.4429 - f1: 0.1654 - precision: 0.6726 - recall: 0.0959 - val_loss: 1.2835 - val_accuracy: 0.3950 - val_f1: 0.1942 - val_precision: 0.5663 - val_recall: 0.1183\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.1875 - accuracy: 0.4809 - f1: 0.2562 - precision: 0.6856 - recall: 0.1606 - val_loss: 1.2765 - val_accuracy: 0.4025 - val_f1: 0.2629 - val_precision: 0.5715 - val_recall: 0.1717\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1398 - accuracy: 0.5101 - f1: 0.3536 - precision: 0.7059 - recall: 0.2378 - val_loss: 1.2909 - val_accuracy: 0.4167 - val_f1: 0.3255 - val_precision: 0.5558 - val_recall: 0.2308\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.1016 - accuracy: 0.5385 - f1: 0.4161 - precision: 0.6748 - recall: 0.3021 - val_loss: 1.2846 - val_accuracy: 0.4492 - val_f1: 0.3391 - val_precision: 0.5197 - val_recall: 0.2525\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0770 - accuracy: 0.5540 - f1: 0.4490 - precision: 0.6812 - recall: 0.3361 - val_loss: 1.2764 - val_accuracy: 0.4433 - val_f1: 0.3621 - val_precision: 0.5369 - val_recall: 0.2742\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.0376 - accuracy: 0.5819 - f1: 0.4877 - precision: 0.7091 - recall: 0.3727 - val_loss: 1.2976 - val_accuracy: 0.4492 - val_f1: 0.3748 - val_precision: 0.5220 - val_recall: 0.2942\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0157 - accuracy: 0.5914 - f1: 0.5032 - precision: 0.7124 - recall: 0.3907 - val_loss: 1.2952 - val_accuracy: 0.4608 - val_f1: 0.4002 - val_precision: 0.5295 - val_recall: 0.3225\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.9973 - accuracy: 0.5959 - f1: 0.5234 - precision: 0.7094 - recall: 0.4154 - val_loss: 1.2866 - val_accuracy: 0.4675 - val_f1: 0.4035 - val_precision: 0.5278 - val_recall: 0.3275\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.9467 - accuracy: 0.6203 - f1: 0.5609 - precision: 0.7102 - recall: 0.4642 - val_loss: 1.3038 - val_accuracy: 0.4658 - val_f1: 0.4256 - val_precision: 0.5401 - val_recall: 0.3517\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9114 - accuracy: 0.6467 - f1: 0.5908 - precision: 0.7376 - recall: 0.4933 - val_loss: 1.3056 - val_accuracy: 0.4842 - val_f1: 0.4316 - val_precision: 0.5476 - val_recall: 0.3567\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.8757 - accuracy: 0.6621 - f1: 0.6183 - precision: 0.7607 - recall: 0.5213 - val_loss: 1.3434 - val_accuracy: 0.4758 - val_f1: 0.4327 - val_precision: 0.5415 - val_recall: 0.3608\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.8403 - accuracy: 0.6759 - f1: 0.6365 - precision: 0.7538 - recall: 0.5514 - val_loss: 1.3533 - val_accuracy: 0.4933 - val_f1: 0.4462 - val_precision: 0.5519 - val_recall: 0.3750\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.7963 - accuracy: 0.6975 - f1: 0.6587 - precision: 0.7762 - recall: 0.5730 - val_loss: 1.3614 - val_accuracy: 0.4908 - val_f1: 0.4482 - val_precision: 0.5428 - val_recall: 0.3833\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.7656 - accuracy: 0.7141 - f1: 0.6801 - precision: 0.7920 - recall: 0.5965 - val_loss: 1.3421 - val_accuracy: 0.4858 - val_f1: 0.4644 - val_precision: 0.5630 - val_recall: 0.3958\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.7392 - accuracy: 0.7229 - f1: 0.6848 - precision: 0.7940 - recall: 0.6029 - val_loss: 1.4002 - val_accuracy: 0.4992 - val_f1: 0.4752 - val_precision: 0.5584 - val_recall: 0.4142\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.7175 - accuracy: 0.7201 - f1: 0.7062 - precision: 0.7980 - recall: 0.6339 - val_loss: 1.4223 - val_accuracy: 0.5142 - val_f1: 0.4870 - val_precision: 0.5657 - val_recall: 0.4283\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.6822 - accuracy: 0.7376 - f1: 0.7263 - precision: 0.8180 - recall: 0.6538 - val_loss: 1.3777 - val_accuracy: 0.5092 - val_f1: 0.4755 - val_precision: 0.5601 - val_recall: 0.4133\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.6718 - accuracy: 0.7419 - f1: 0.7288 - precision: 0.8140 - recall: 0.6608 - val_loss: 1.3708 - val_accuracy: 0.5200 - val_f1: 0.4931 - val_precision: 0.5672 - val_recall: 0.4367\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.6352 - accuracy: 0.7623 - f1: 0.7417 - precision: 0.8199 - recall: 0.6777 - val_loss: 1.4932 - val_accuracy: 0.5058 - val_f1: 0.4915 - val_precision: 0.5520 - val_recall: 0.4433\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.5869 - accuracy: 0.7842 - f1: 0.7699 - precision: 0.8407 - recall: 0.7106 - val_loss: 1.5275 - val_accuracy: 0.5192 - val_f1: 0.5015 - val_precision: 0.5576 - val_recall: 0.4558\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.5461 - accuracy: 0.8023 - f1: 0.7961 - precision: 0.8510 - recall: 0.7482 - val_loss: 1.6062 - val_accuracy: 0.5417 - val_f1: 0.5175 - val_precision: 0.5741 - val_recall: 0.4717\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.5144 - accuracy: 0.8107 - f1: 0.8060 - precision: 0.8626 - recall: 0.7569 - val_loss: 1.6199 - val_accuracy: 0.5442 - val_f1: 0.5268 - val_precision: 0.5785 - val_recall: 0.4842\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.5023 - accuracy: 0.8217 - f1: 0.8167 - precision: 0.8621 - recall: 0.7763 - val_loss: 1.5844 - val_accuracy: 0.5492 - val_f1: 0.5377 - val_precision: 0.5864 - val_recall: 0.4967\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.4709 - accuracy: 0.8286 - f1: 0.8264 - precision: 0.8734 - recall: 0.7845 - val_loss: 1.6896 - val_accuracy: 0.5567 - val_f1: 0.5421 - val_precision: 0.5878 - val_recall: 0.5033\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.4354 - accuracy: 0.8403 - f1: 0.8402 - precision: 0.8776 - recall: 0.8062 - val_loss: 1.7668 - val_accuracy: 0.5392 - val_f1: 0.5340 - val_precision: 0.5711 - val_recall: 0.5017\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.4523 - accuracy: 0.8314 - f1: 0.8280 - precision: 0.8706 - recall: 0.7898 - val_loss: 1.7590 - val_accuracy: 0.5683 - val_f1: 0.5544 - val_precision: 0.5857 - val_recall: 0.5267\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.3789 - accuracy: 0.8670 - f1: 0.8600 - precision: 0.8939 - recall: 0.8290 - val_loss: 1.7815 - val_accuracy: 0.5675 - val_f1: 0.5575 - val_precision: 0.5884 - val_recall: 0.5300\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.3971 - accuracy: 0.8491 - f1: 0.8482 - precision: 0.8811 - recall: 0.8178 - val_loss: 1.8724 - val_accuracy: 0.5583 - val_f1: 0.5443 - val_precision: 0.5756 - val_recall: 0.5167\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3728 - accuracy: 0.8686 - f1: 0.8668 - precision: 0.8962 - recall: 0.8395 - val_loss: 1.7932 - val_accuracy: 0.5792 - val_f1: 0.5747 - val_precision: 0.6012 - val_recall: 0.5508\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3605 - accuracy: 0.8669 - f1: 0.8684 - precision: 0.8937 - recall: 0.8447 - val_loss: 1.8102 - val_accuracy: 0.5783 - val_f1: 0.5759 - val_precision: 0.6076 - val_recall: 0.5475\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.3095 - accuracy: 0.8871 - f1: 0.8853 - precision: 0.9084 - recall: 0.8636 - val_loss: 1.8766 - val_accuracy: 0.5767 - val_f1: 0.5784 - val_precision: 0.6003 - val_recall: 0.5583\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2916 - accuracy: 0.8934 - f1: 0.8952 - precision: 0.9142 - recall: 0.8771 - val_loss: 1.8518 - val_accuracy: 0.5767 - val_f1: 0.5697 - val_precision: 0.5939 - val_recall: 0.5475\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2877 - accuracy: 0.8962 - f1: 0.8982 - precision: 0.9191 - recall: 0.8784 - val_loss: 2.1973 - val_accuracy: 0.5792 - val_f1: 0.5733 - val_precision: 0.5891 - val_recall: 0.5583\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3279 - accuracy: 0.8868 - f1: 0.8829 - precision: 0.9049 - recall: 0.8621 - val_loss: 2.0587 - val_accuracy: 0.5858 - val_f1: 0.5868 - val_precision: 0.6029 - val_recall: 0.5717\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.2890 - accuracy: 0.8895 - f1: 0.8912 - precision: 0.9109 - recall: 0.8725 - val_loss: 1.9718 - val_accuracy: 0.5883 - val_f1: 0.5819 - val_precision: 0.5999 - val_recall: 0.5650\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.2550 - accuracy: 0.9067 - f1: 0.9082 - precision: 0.9266 - recall: 0.8907 - val_loss: 2.0051 - val_accuracy: 0.5925 - val_f1: 0.5909 - val_precision: 0.6069 - val_recall: 0.5758\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3602 - accuracy: 0.8716 - f1: 0.8709 - precision: 0.8908 - recall: 0.8522 - val_loss: 1.7822 - val_accuracy: 0.5825 - val_f1: 0.5735 - val_precision: 0.5944 - val_recall: 0.5542\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.2688 - accuracy: 0.9065 - f1: 0.9060 - precision: 0.9284 - recall: 0.8849 - val_loss: 1.7445 - val_accuracy: 0.5750 - val_f1: 0.5706 - val_precision: 0.5930 - val_recall: 0.5500\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.2654 - accuracy: 0.9049 - f1: 0.9030 - precision: 0.9186 - recall: 0.8882 - val_loss: 1.8720 - val_accuracy: 0.5925 - val_f1: 0.5929 - val_precision: 0.6122 - val_recall: 0.5750\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.2132 - accuracy: 0.9245 - f1: 0.9239 - precision: 0.9365 - recall: 0.9118 - val_loss: 2.1418 - val_accuracy: 0.5933 - val_f1: 0.5921 - val_precision: 0.6066 - val_recall: 0.5783\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.1865 - accuracy: 0.9349 - f1: 0.9343 - precision: 0.9445 - recall: 0.9245 - val_loss: 2.1055 - val_accuracy: 0.5983 - val_f1: 0.5919 - val_precision: 0.6101 - val_recall: 0.5750\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1892 - accuracy: 0.9407 - f1: 0.9403 - precision: 0.9481 - recall: 0.9327 - val_loss: 2.1148 - val_accuracy: 0.5975 - val_f1: 0.5979 - val_precision: 0.6125 - val_recall: 0.5842\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1772 - accuracy: 0.9454 - f1: 0.9450 - precision: 0.9513 - recall: 0.9388 - val_loss: 2.2321 - val_accuracy: 0.5992 - val_f1: 0.6030 - val_precision: 0.6140 - val_recall: 0.5925\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.1631 - accuracy: 0.9384 - f1: 0.9395 - precision: 0.9462 - recall: 0.9329 - val_loss: 2.2799 - val_accuracy: 0.5992 - val_f1: 0.5985 - val_precision: 0.6091 - val_recall: 0.5883\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1495 - accuracy: 0.9453 - f1: 0.9471 - precision: 0.9540 - recall: 0.9403 - val_loss: 2.3246 - val_accuracy: 0.5942 - val_f1: 0.5876 - val_precision: 0.5972 - val_recall: 0.5783\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1444 - accuracy: 0.9490 - f1: 0.9501 - precision: 0.9573 - recall: 0.9431 - val_loss: 2.3818 - val_accuracy: 0.5958 - val_f1: 0.5937 - val_precision: 0.6037 - val_recall: 0.5842\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1490 - accuracy: 0.9444 - f1: 0.9453 - precision: 0.9526 - recall: 0.9382 - val_loss: 2.5937 - val_accuracy: 0.5883 - val_f1: 0.5846 - val_precision: 0.5919 - val_recall: 0.5775\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1472 - accuracy: 0.9518 - f1: 0.9516 - precision: 0.9566 - recall: 0.9468 - val_loss: 2.4041 - val_accuracy: 0.5917 - val_f1: 0.5899 - val_precision: 0.5975 - val_recall: 0.5825\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.1366 - accuracy: 0.9498 - f1: 0.9497 - precision: 0.9542 - recall: 0.9454 - val_loss: 2.7718 - val_accuracy: 0.5925 - val_f1: 0.5926 - val_precision: 0.6005 - val_recall: 0.5850\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1534 - accuracy: 0.9463 - f1: 0.9458 - precision: 0.9505 - recall: 0.9412 - val_loss: 2.8232 - val_accuracy: 0.5817 - val_f1: 0.5788 - val_precision: 0.5863 - val_recall: 0.5717\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1552 - accuracy: 0.9443 - f1: 0.9436 - precision: 0.9485 - recall: 0.9389 - val_loss: 2.5228 - val_accuracy: 0.5867 - val_f1: 0.5851 - val_precision: 0.5947 - val_recall: 0.5758\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 0.1483 - accuracy: 0.9471 - f1: 0.9465 - precision: 0.9511 - recall: 0.9420 - val_loss: 2.7170 - val_accuracy: 0.5975 - val_f1: 0.5920 - val_precision: 0.6019 - val_recall: 0.5825\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.1300 - accuracy: 0.9551 - f1: 0.9558 - precision: 0.9602 - recall: 0.9515 - val_loss: 2.6151 - val_accuracy: 0.5925 - val_f1: 0.5909 - val_precision: 0.6006 - val_recall: 0.5817\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.1166 - accuracy: 0.9608 - f1: 0.9624 - precision: 0.9664 - recall: 0.9584 - val_loss: 2.7510 - val_accuracy: 0.5992 - val_f1: 0.5925 - val_precision: 0.6020 - val_recall: 0.5833\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1333 - accuracy: 0.9513 - f1: 0.9513 - precision: 0.9557 - recall: 0.9469 - val_loss: 2.6462 - val_accuracy: 0.5783 - val_f1: 0.5774 - val_precision: 0.5870 - val_recall: 0.5683\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1466 - accuracy: 0.9490 - f1: 0.9485 - precision: 0.9528 - recall: 0.9444 - val_loss: 2.3848 - val_accuracy: 0.5942 - val_f1: 0.5923 - val_precision: 0.5989 - val_recall: 0.5858\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1249 - accuracy: 0.9569 - f1: 0.9579 - precision: 0.9622 - recall: 0.9536 - val_loss: 2.5275 - val_accuracy: 0.5867 - val_f1: 0.5837 - val_precision: 0.5936 - val_recall: 0.5742\n",
            "14/14 [==============================] - 1s 86ms/step - loss: 2.5516 - accuracy: 0.6021 - f1: 0.6007 - precision: 0.6082 - recall: 0.5936\n",
            "[2.551596164703369, 0.602142870426178, 0.6007444262504578, 0.608150064945221, 0.5935714840888977]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ae637c-54f9-496c-f302-69737acf2975"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 154ms/step - loss: 1.3914 - accuracy: 0.2743 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3731 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3768 - accuracy: 0.2963 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3647 - val_accuracy: 0.3267 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3608 - accuracy: 0.3302 - f1: 7.0338e-04 - precision: 0.0355 - recall: 3.5521e-04 - val_loss: 1.3446 - val_accuracy: 0.3600 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3458 - accuracy: 0.3497 - f1: 0.0172 - precision: 0.2762 - recall: 0.0093 - val_loss: 1.3422 - val_accuracy: 0.3708 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.3201 - accuracy: 0.3762 - f1: 0.0503 - precision: 0.5196 - recall: 0.0278 - val_loss: 1.3245 - val_accuracy: 0.3733 - val_f1: 0.0847 - val_precision: 0.5119 - val_recall: 0.0467\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3017 - accuracy: 0.3936 - f1: 0.1115 - precision: 0.5516 - recall: 0.0645 - val_loss: 1.3231 - val_accuracy: 0.3683 - val_f1: 0.1127 - val_precision: 0.4825 - val_recall: 0.0642\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2938 - accuracy: 0.3980 - f1: 0.1003 - precision: 0.5031 - recall: 0.0563 - val_loss: 1.3766 - val_accuracy: 0.3025 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3414 - accuracy: 0.3402 - f1: 0.0046 - precision: 0.1395 - recall: 0.0023 - val_loss: 1.3881 - val_accuracy: 0.3058 - val_f1: 0.0194 - val_precision: 0.4333 - val_recall: 0.0100\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3305 - accuracy: 0.3526 - f1: 0.0447 - precision: 0.5333 - recall: 0.0236 - val_loss: 1.3908 - val_accuracy: 0.3108 - val_f1: 0.0033 - val_precision: 0.1111 - val_recall: 0.0017\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3435 - accuracy: 0.3459 - f1: 0.0404 - precision: 0.5003 - recall: 0.0215 - val_loss: 1.3922 - val_accuracy: 0.2758 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3448 - accuracy: 0.3489 - f1: 0.0153 - precision: 0.4366 - recall: 0.0078 - val_loss: 1.3652 - val_accuracy: 0.3392 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2978 - accuracy: 0.3937 - f1: 0.0811 - precision: 0.5348 - recall: 0.0469 - val_loss: 1.3951 - val_accuracy: 0.2942 - val_f1: 0.0131 - val_precision: 0.4444 - val_recall: 0.0067\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.3051 - accuracy: 0.3964 - f1: 0.0564 - precision: 0.4959 - recall: 0.0307 - val_loss: 1.3675 - val_accuracy: 0.3517 - val_f1: 0.1145 - val_precision: 0.4267 - val_recall: 0.0667\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2628 - accuracy: 0.4099 - f1: 0.1819 - precision: 0.5192 - recall: 0.1127 - val_loss: 1.4480 - val_accuracy: 0.2883 - val_f1: 0.0817 - val_precision: 0.3424 - val_recall: 0.0467\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.3148 - accuracy: 0.3763 - f1: 0.1226 - precision: 0.5579 - recall: 0.0714 - val_loss: 1.3961 - val_accuracy: 0.3250 - val_f1: 0.0559 - val_precision: 0.4393 - val_recall: 0.0300\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.3269 - accuracy: 0.3593 - f1: 0.0449 - precision: 0.5707 - recall: 0.0239 - val_loss: 1.4131 - val_accuracy: 0.3125 - val_f1: 0.0924 - val_precision: 0.3333 - val_recall: 0.0542\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3178 - accuracy: 0.3883 - f1: 0.1230 - precision: 0.5180 - recall: 0.0705 - val_loss: 1.3922 - val_accuracy: 0.3025 - val_f1: 0.0514 - val_precision: 0.4677 - val_recall: 0.0275\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2691 - accuracy: 0.4163 - f1: 0.1275 - precision: 0.5774 - recall: 0.0736 - val_loss: 1.4383 - val_accuracy: 0.2883 - val_f1: 0.1024 - val_precision: 0.3568 - val_recall: 0.0600\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2836 - accuracy: 0.4129 - f1: 0.1963 - precision: 0.5314 - recall: 0.1219 - val_loss: 1.4058 - val_accuracy: 0.2942 - val_f1: 0.0660 - val_precision: 0.4564 - val_recall: 0.0358\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2882 - accuracy: 0.4042 - f1: 0.1499 - precision: 0.5637 - recall: 0.0884 - val_loss: 1.3958 - val_accuracy: 0.2842 - val_f1: 0.0260 - val_precision: 0.6806 - val_recall: 0.0133\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.3269 - accuracy: 0.3681 - f1: 0.0920 - precision: 0.5275 - recall: 0.0510 - val_loss: 1.4100 - val_accuracy: 0.3000 - val_f1: 0.0795 - val_precision: 0.4103 - val_recall: 0.0442\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2664 - accuracy: 0.4100 - f1: 0.1865 - precision: 0.5598 - recall: 0.1140 - val_loss: 1.3785 - val_accuracy: 0.3283 - val_f1: 0.0653 - val_precision: 0.5762 - val_recall: 0.0350\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2272 - accuracy: 0.4421 - f1: 0.2247 - precision: 0.6626 - recall: 0.1381 - val_loss: 1.4269 - val_accuracy: 0.3050 - val_f1: 0.1025 - val_precision: 0.3626 - val_recall: 0.0600\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2451 - accuracy: 0.4398 - f1: 0.2208 - precision: 0.5850 - recall: 0.1368 - val_loss: 1.4587 - val_accuracy: 0.2942 - val_f1: 0.1399 - val_precision: 0.3691 - val_recall: 0.0867\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2849 - accuracy: 0.4117 - f1: 0.2290 - precision: 0.5539 - recall: 0.1458 - val_loss: 1.4382 - val_accuracy: 0.2942 - val_f1: 0.0603 - val_precision: 0.3395 - val_recall: 0.0333\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2933 - accuracy: 0.3985 - f1: 0.1762 - precision: 0.5232 - recall: 0.1076 - val_loss: 1.3856 - val_accuracy: 0.3267 - val_f1: 0.0585 - val_precision: 0.3153 - val_recall: 0.0325\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2533 - accuracy: 0.4276 - f1: 0.1901 - precision: 0.5896 - recall: 0.1151 - val_loss: 1.4356 - val_accuracy: 0.3150 - val_f1: 0.0968 - val_precision: 0.3410 - val_recall: 0.0567\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2578 - accuracy: 0.4358 - f1: 0.2417 - precision: 0.5736 - recall: 0.1556 - val_loss: 1.4283 - val_accuracy: 0.2883 - val_f1: 0.0918 - val_precision: 0.3335 - val_recall: 0.0533\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2882 - accuracy: 0.4159 - f1: 0.2006 - precision: 0.5438 - recall: 0.1238 - val_loss: 1.3972 - val_accuracy: 0.3358 - val_f1: 0.1365 - val_precision: 0.3691 - val_recall: 0.0842\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2211 - accuracy: 0.4621 - f1: 0.2826 - precision: 0.6267 - recall: 0.1841 - val_loss: 1.4615 - val_accuracy: 0.2908 - val_f1: 0.0930 - val_precision: 0.3356 - val_recall: 0.0542\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.2297 - accuracy: 0.4555 - f1: 0.2775 - precision: 0.5999 - recall: 0.1842 - val_loss: 1.4189 - val_accuracy: 0.3050 - val_f1: 0.1532 - val_precision: 0.4191 - val_recall: 0.0942\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2441 - accuracy: 0.4429 - f1: 0.2808 - precision: 0.5594 - recall: 0.1908 - val_loss: 1.4059 - val_accuracy: 0.3150 - val_f1: 0.1251 - val_precision: 0.3947 - val_recall: 0.0750\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2515 - accuracy: 0.4364 - f1: 0.2021 - precision: 0.5784 - recall: 0.1232 - val_loss: 1.4253 - val_accuracy: 0.3183 - val_f1: 0.1480 - val_precision: 0.3524 - val_recall: 0.0942\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2149 - accuracy: 0.4509 - f1: 0.2711 - precision: 0.5929 - recall: 0.1768 - val_loss: 1.4952 - val_accuracy: 0.3042 - val_f1: 0.1229 - val_precision: 0.3011 - val_recall: 0.0775\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.2572 - accuracy: 0.4157 - f1: 0.2221 - precision: 0.5698 - recall: 0.1388 - val_loss: 1.4808 - val_accuracy: 0.2950 - val_f1: 0.1966 - val_precision: 0.3495 - val_recall: 0.1375\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2741 - accuracy: 0.4171 - f1: 0.2298 - precision: 0.5334 - recall: 0.1499 - val_loss: 1.4487 - val_accuracy: 0.3183 - val_f1: 0.1328 - val_precision: 0.3843 - val_recall: 0.0808\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.2214 - accuracy: 0.4671 - f1: 0.2691 - precision: 0.6035 - recall: 0.1747 - val_loss: 1.4738 - val_accuracy: 0.2967 - val_f1: 0.1464 - val_precision: 0.3307 - val_recall: 0.0942\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.2387 - accuracy: 0.4383 - f1: 0.2621 - precision: 0.5718 - recall: 0.1715 - val_loss: 1.4825 - val_accuracy: 0.3217 - val_f1: 0.1737 - val_precision: 0.3781 - val_recall: 0.1133\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.2183 - accuracy: 0.4660 - f1: 0.3042 - precision: 0.5959 - recall: 0.2058 - val_loss: 1.4398 - val_accuracy: 0.3167 - val_f1: 0.0978 - val_precision: 0.3620 - val_recall: 0.0575\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2700 - accuracy: 0.4067 - f1: 0.2190 - precision: 0.5520 - recall: 0.1380 - val_loss: 1.4262 - val_accuracy: 0.3158 - val_f1: 0.1055 - val_precision: 0.3853 - val_recall: 0.0617\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.1975 - accuracy: 0.4600 - f1: 0.2616 - precision: 0.6227 - recall: 0.1692 - val_loss: 1.4699 - val_accuracy: 0.2942 - val_f1: 0.0946 - val_precision: 0.2979 - val_recall: 0.0567\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 7s 144ms/step - loss: 1.2550 - accuracy: 0.4129 - f1: 0.2152 - precision: 0.5423 - recall: 0.1355 - val_loss: 1.4117 - val_accuracy: 0.3067 - val_f1: 0.1249 - val_precision: 0.3831 - val_recall: 0.0750\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.2086 - accuracy: 0.4536 - f1: 0.2723 - precision: 0.6400 - recall: 0.1750 - val_loss: 1.4248 - val_accuracy: 0.3133 - val_f1: 0.1197 - val_precision: 0.4284 - val_recall: 0.0700\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.1978 - accuracy: 0.4701 - f1: 0.2701 - precision: 0.6361 - recall: 0.1729 - val_loss: 1.4606 - val_accuracy: 0.2983 - val_f1: 0.1149 - val_precision: 0.3282 - val_recall: 0.0700\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.1790 - accuracy: 0.4825 - f1: 0.3115 - precision: 0.6426 - recall: 0.2078 - val_loss: 1.4699 - val_accuracy: 0.2983 - val_f1: 0.1497 - val_precision: 0.3286 - val_recall: 0.0975\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.1332 - accuracy: 0.5103 - f1: 0.3700 - precision: 0.6678 - recall: 0.2580 - val_loss: 1.4730 - val_accuracy: 0.3300 - val_f1: 0.1863 - val_precision: 0.3606 - val_recall: 0.1258\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.1184 - accuracy: 0.5115 - f1: 0.3862 - precision: 0.6723 - recall: 0.2722 - val_loss: 1.4941 - val_accuracy: 0.3267 - val_f1: 0.2158 - val_precision: 0.3935 - val_recall: 0.1492\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.1131 - accuracy: 0.5130 - f1: 0.3983 - precision: 0.6500 - recall: 0.2885 - val_loss: 1.4910 - val_accuracy: 0.3200 - val_f1: 0.2104 - val_precision: 0.3868 - val_recall: 0.1450\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.0977 - accuracy: 0.5200 - f1: 0.4017 - precision: 0.6539 - recall: 0.2910 - val_loss: 1.5307 - val_accuracy: 0.3108 - val_f1: 0.2013 - val_precision: 0.3565 - val_recall: 0.1408\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.0468 - accuracy: 0.5396 - f1: 0.4514 - precision: 0.6860 - recall: 0.3374 - val_loss: 1.5251 - val_accuracy: 0.3117 - val_f1: 0.1843 - val_precision: 0.3569 - val_recall: 0.1250\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.0653 - accuracy: 0.5348 - f1: 0.4424 - precision: 0.6736 - recall: 0.3308 - val_loss: 1.5774 - val_accuracy: 0.3158 - val_f1: 0.2007 - val_precision: 0.3512 - val_recall: 0.1408\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.1070 - accuracy: 0.5249 - f1: 0.4188 - precision: 0.6412 - recall: 0.3126 - val_loss: 1.5213 - val_accuracy: 0.3475 - val_f1: 0.2228 - val_precision: 0.3833 - val_recall: 0.1575\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.0444 - accuracy: 0.5504 - f1: 0.4459 - precision: 0.6882 - recall: 0.3322 - val_loss: 1.5534 - val_accuracy: 0.3400 - val_f1: 0.2566 - val_precision: 0.3972 - val_recall: 0.1900\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.0752 - accuracy: 0.5309 - f1: 0.4488 - precision: 0.6702 - recall: 0.3394 - val_loss: 1.5643 - val_accuracy: 0.3375 - val_f1: 0.2382 - val_precision: 0.3739 - val_recall: 0.1750\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.0495 - accuracy: 0.5569 - f1: 0.4582 - precision: 0.6731 - recall: 0.3488 - val_loss: 1.5363 - val_accuracy: 0.3625 - val_f1: 0.2530 - val_precision: 0.4068 - val_recall: 0.1842\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.0594 - accuracy: 0.5461 - f1: 0.4391 - precision: 0.6798 - recall: 0.3259 - val_loss: 1.5473 - val_accuracy: 0.3483 - val_f1: 0.2517 - val_precision: 0.4097 - val_recall: 0.1825\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 1.0432 - accuracy: 0.5533 - f1: 0.4621 - precision: 0.6881 - recall: 0.3487 - val_loss: 1.5775 - val_accuracy: 0.3358 - val_f1: 0.2590 - val_precision: 0.3915 - val_recall: 0.1942\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.0259 - accuracy: 0.5701 - f1: 0.4935 - precision: 0.6982 - recall: 0.3830 - val_loss: 1.6051 - val_accuracy: 0.3433 - val_f1: 0.2653 - val_precision: 0.3825 - val_recall: 0.2033\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.1050 - accuracy: 0.5251 - f1: 0.4456 - precision: 0.6510 - recall: 0.3402 - val_loss: 1.5439 - val_accuracy: 0.3258 - val_f1: 0.2411 - val_precision: 0.3777 - val_recall: 0.1775\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 7s 146ms/step - loss: 1.1515 - accuracy: 0.5043 - f1: 0.3987 - precision: 0.6368 - recall: 0.2916 - val_loss: 1.5268 - val_accuracy: 0.3433 - val_f1: 0.2615 - val_precision: 0.3881 - val_recall: 0.1975\n",
            "14/14 [==============================] - 1s 83ms/step - loss: 1.5591 - accuracy: 0.3443 - f1: 0.2477 - precision: 0.3621 - recall: 0.1886\n",
            "[1.5591201782226562, 0.3442857265472412, 0.24767284095287323, 0.3621430993080139, 0.18857142329216003]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7476f9e8-c97c-4891-d0a9-d9914fa30fe0"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(64))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 4s 38ms/step - loss: 1.3820 - accuracy: 0.2913 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3720 - val_accuracy: 0.3325 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 1.3291 - accuracy: 0.3559 - f1: 0.0844 - precision: 0.2532 - recall: 0.0528 - val_loss: 1.1898 - val_accuracy: 0.4617 - val_f1: 0.3739 - val_precision: 0.6162 - val_recall: 0.2692\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 1.2015 - accuracy: 0.4562 - f1: 0.3352 - precision: 0.6341 - recall: 0.2421 - val_loss: 1.1461 - val_accuracy: 0.4733 - val_f1: 0.3943 - val_precision: 0.6470 - val_recall: 0.2842\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 1.1196 - accuracy: 0.4835 - f1: 0.4075 - precision: 0.6926 - recall: 0.2908 - val_loss: 1.0920 - val_accuracy: 0.4975 - val_f1: 0.4215 - val_precision: 0.7190 - val_recall: 0.3000\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 1.0533 - accuracy: 0.5206 - f1: 0.4263 - precision: 0.8038 - recall: 0.2940 - val_loss: 1.0586 - val_accuracy: 0.5275 - val_f1: 0.4010 - val_precision: 0.7768 - val_recall: 0.2725\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 1.0017 - accuracy: 0.5459 - f1: 0.4415 - precision: 0.8571 - recall: 0.3035 - val_loss: 1.1729 - val_accuracy: 0.5375 - val_f1: 0.4627 - val_precision: 0.7157 - val_recall: 0.3433\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.9389 - accuracy: 0.5785 - f1: 0.5001 - precision: 0.8614 - recall: 0.3562 - val_loss: 1.1014 - val_accuracy: 0.5658 - val_f1: 0.4937 - val_precision: 0.7120 - val_recall: 0.3792\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.8497 - accuracy: 0.6268 - f1: 0.5401 - precision: 0.8964 - recall: 0.3890 - val_loss: 1.0802 - val_accuracy: 0.5767 - val_f1: 0.5141 - val_precision: 0.7236 - val_recall: 0.4000\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.8206 - accuracy: 0.6427 - f1: 0.5716 - precision: 0.8643 - recall: 0.4304 - val_loss: 0.9828 - val_accuracy: 0.5767 - val_f1: 0.5634 - val_precision: 0.7290 - val_recall: 0.4600\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.7732 - accuracy: 0.6657 - f1: 0.5980 - precision: 0.8244 - recall: 0.4721 - val_loss: 1.0137 - val_accuracy: 0.5633 - val_f1: 0.5429 - val_precision: 0.6978 - val_recall: 0.4450\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.7328 - accuracy: 0.6732 - f1: 0.6133 - precision: 0.8203 - recall: 0.4921 - val_loss: 0.9553 - val_accuracy: 0.5992 - val_f1: 0.5724 - val_precision: 0.7441 - val_recall: 0.4667\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.6752 - accuracy: 0.6964 - f1: 0.6572 - precision: 0.8307 - recall: 0.5451 - val_loss: 1.0675 - val_accuracy: 0.6058 - val_f1: 0.5599 - val_precision: 0.7122 - val_recall: 0.4625\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.7024 - accuracy: 0.6730 - f1: 0.6256 - precision: 0.8205 - recall: 0.5080 - val_loss: 0.9718 - val_accuracy: 0.6317 - val_f1: 0.5891 - val_precision: 0.7409 - val_recall: 0.4908\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.7128 - accuracy: 0.6687 - f1: 0.6407 - precision: 0.7993 - recall: 0.5373 - val_loss: 0.9104 - val_accuracy: 0.6117 - val_f1: 0.5709 - val_precision: 0.7786 - val_recall: 0.4525\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.6360 - accuracy: 0.7041 - f1: 0.6619 - precision: 0.8478 - recall: 0.5465 - val_loss: 0.9095 - val_accuracy: 0.6167 - val_f1: 0.5963 - val_precision: 0.7558 - val_recall: 0.4942\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.6073 - accuracy: 0.7177 - f1: 0.6839 - precision: 0.8401 - recall: 0.5799 - val_loss: 0.9729 - val_accuracy: 0.6225 - val_f1: 0.6157 - val_precision: 0.7120 - val_recall: 0.5425\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.5677 - accuracy: 0.7405 - f1: 0.7169 - precision: 0.8310 - recall: 0.6319 - val_loss: 1.0777 - val_accuracy: 0.5967 - val_f1: 0.5924 - val_precision: 0.6813 - val_recall: 0.5242\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.5723 - accuracy: 0.7389 - f1: 0.7316 - precision: 0.7949 - recall: 0.6784 - val_loss: 0.9242 - val_accuracy: 0.6733 - val_f1: 0.6720 - val_precision: 0.7398 - val_recall: 0.6158\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.4756 - accuracy: 0.8279 - f1: 0.8230 - precision: 0.8681 - recall: 0.7833 - val_loss: 0.9086 - val_accuracy: 0.7042 - val_f1: 0.6971 - val_precision: 0.7533 - val_recall: 0.6492\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.4171 - accuracy: 0.8595 - f1: 0.8571 - precision: 0.8932 - recall: 0.8244 - val_loss: 0.8693 - val_accuracy: 0.7242 - val_f1: 0.7294 - val_precision: 0.7687 - val_recall: 0.6942\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3726 - accuracy: 0.8759 - f1: 0.8788 - precision: 0.9113 - recall: 0.8493 - val_loss: 0.9024 - val_accuracy: 0.7317 - val_f1: 0.7295 - val_precision: 0.7587 - val_recall: 0.7025\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3555 - accuracy: 0.8880 - f1: 0.8875 - precision: 0.9098 - recall: 0.8665 - val_loss: 1.0058 - val_accuracy: 0.7317 - val_f1: 0.7295 - val_precision: 0.7504 - val_recall: 0.7100\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3563 - accuracy: 0.8900 - f1: 0.8875 - precision: 0.9114 - recall: 0.8652 - val_loss: 0.7949 - val_accuracy: 0.7333 - val_f1: 0.7355 - val_precision: 0.7857 - val_recall: 0.6917\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3792 - accuracy: 0.8838 - f1: 0.8788 - precision: 0.9154 - recall: 0.8458 - val_loss: 0.8408 - val_accuracy: 0.7500 - val_f1: 0.7505 - val_precision: 0.7920 - val_recall: 0.7133\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.3870 - accuracy: 0.8809 - f1: 0.8843 - precision: 0.9141 - recall: 0.8565 - val_loss: 0.8793 - val_accuracy: 0.7567 - val_f1: 0.7606 - val_precision: 0.7921 - val_recall: 0.7317\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3250 - accuracy: 0.9035 - f1: 0.9032 - precision: 0.9261 - recall: 0.8819 - val_loss: 0.7334 - val_accuracy: 0.7567 - val_f1: 0.7403 - val_precision: 0.8203 - val_recall: 0.6750\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.3330 - accuracy: 0.9063 - f1: 0.8983 - precision: 0.9298 - recall: 0.8696 - val_loss: 0.7701 - val_accuracy: 0.7758 - val_f1: 0.7757 - val_precision: 0.8036 - val_recall: 0.7500\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2683 - accuracy: 0.9224 - f1: 0.9226 - precision: 0.9387 - recall: 0.9072 - val_loss: 0.7552 - val_accuracy: 0.7758 - val_f1: 0.7741 - val_precision: 0.8059 - val_recall: 0.7450\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2729 - accuracy: 0.9233 - f1: 0.9239 - precision: 0.9410 - recall: 0.9075 - val_loss: 0.8619 - val_accuracy: 0.7758 - val_f1: 0.7752 - val_precision: 0.7922 - val_recall: 0.7592\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2440 - accuracy: 0.9352 - f1: 0.9321 - precision: 0.9454 - recall: 0.9193 - val_loss: 0.8448 - val_accuracy: 0.7867 - val_f1: 0.7885 - val_precision: 0.8063 - val_recall: 0.7717\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2384 - accuracy: 0.9331 - f1: 0.9351 - precision: 0.9478 - recall: 0.9227 - val_loss: 0.8569 - val_accuracy: 0.7717 - val_f1: 0.7708 - val_precision: 0.7977 - val_recall: 0.7458\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2374 - accuracy: 0.9363 - f1: 0.9356 - precision: 0.9502 - recall: 0.9216 - val_loss: 0.8961 - val_accuracy: 0.7750 - val_f1: 0.7774 - val_precision: 0.7912 - val_recall: 0.7642\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2162 - accuracy: 0.9397 - f1: 0.9424 - precision: 0.9537 - recall: 0.9315 - val_loss: 0.9013 - val_accuracy: 0.7700 - val_f1: 0.7717 - val_precision: 0.7939 - val_recall: 0.7508\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1965 - accuracy: 0.9470 - f1: 0.9484 - precision: 0.9612 - recall: 0.9361 - val_loss: 0.9039 - val_accuracy: 0.7900 - val_f1: 0.7923 - val_precision: 0.8044 - val_recall: 0.7808\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1810 - accuracy: 0.9541 - f1: 0.9525 - precision: 0.9636 - recall: 0.9417 - val_loss: 0.9425 - val_accuracy: 0.7767 - val_f1: 0.7785 - val_precision: 0.7918 - val_recall: 0.7658\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1715 - accuracy: 0.9568 - f1: 0.9560 - precision: 0.9621 - recall: 0.9500 - val_loss: 0.9447 - val_accuracy: 0.7733 - val_f1: 0.7733 - val_precision: 0.7864 - val_recall: 0.7608\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1815 - accuracy: 0.9528 - f1: 0.9540 - precision: 0.9654 - recall: 0.9431 - val_loss: 0.9450 - val_accuracy: 0.7908 - val_f1: 0.7882 - val_precision: 0.7959 - val_recall: 0.7808\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1671 - accuracy: 0.9591 - f1: 0.9574 - precision: 0.9637 - recall: 0.9513 - val_loss: 0.9555 - val_accuracy: 0.7783 - val_f1: 0.7791 - val_precision: 0.7884 - val_recall: 0.7700\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1703 - accuracy: 0.9551 - f1: 0.9537 - precision: 0.9628 - recall: 0.9450 - val_loss: 0.8918 - val_accuracy: 0.7783 - val_f1: 0.7788 - val_precision: 0.7923 - val_recall: 0.7658\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1681 - accuracy: 0.9562 - f1: 0.9571 - precision: 0.9656 - recall: 0.9489 - val_loss: 0.8400 - val_accuracy: 0.7867 - val_f1: 0.7833 - val_precision: 0.8000 - val_recall: 0.7675\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1529 - accuracy: 0.9610 - f1: 0.9615 - precision: 0.9693 - recall: 0.9539 - val_loss: 0.8548 - val_accuracy: 0.7892 - val_f1: 0.7910 - val_precision: 0.8061 - val_recall: 0.7767\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1438 - accuracy: 0.9662 - f1: 0.9663 - precision: 0.9739 - recall: 0.9589 - val_loss: 0.7855 - val_accuracy: 0.7742 - val_f1: 0.7830 - val_precision: 0.8106 - val_recall: 0.7575\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1943 - accuracy: 0.9501 - f1: 0.9503 - precision: 0.9621 - recall: 0.9390 - val_loss: 0.8989 - val_accuracy: 0.7717 - val_f1: 0.7720 - val_precision: 0.7845 - val_recall: 0.7600\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1777 - accuracy: 0.9577 - f1: 0.9574 - precision: 0.9659 - recall: 0.9491 - val_loss: 0.9273 - val_accuracy: 0.7683 - val_f1: 0.7705 - val_precision: 0.7814 - val_recall: 0.7600\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1531 - accuracy: 0.9614 - f1: 0.9629 - precision: 0.9689 - recall: 0.9570 - val_loss: 0.9698 - val_accuracy: 0.7700 - val_f1: 0.7655 - val_precision: 0.7773 - val_recall: 0.7542\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1475 - accuracy: 0.9629 - f1: 0.9645 - precision: 0.9721 - recall: 0.9572 - val_loss: 0.9901 - val_accuracy: 0.7775 - val_f1: 0.7708 - val_precision: 0.7883 - val_recall: 0.7542\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1581 - accuracy: 0.9649 - f1: 0.9636 - precision: 0.9712 - recall: 0.9561 - val_loss: 1.0243 - val_accuracy: 0.7775 - val_f1: 0.7771 - val_precision: 0.7844 - val_recall: 0.7700\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1700 - accuracy: 0.9571 - f1: 0.9584 - precision: 0.9647 - recall: 0.9523 - val_loss: 0.9975 - val_accuracy: 0.7808 - val_f1: 0.7819 - val_precision: 0.7898 - val_recall: 0.7742\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1875 - accuracy: 0.9524 - f1: 0.9538 - precision: 0.9612 - recall: 0.9465 - val_loss: 0.9835 - val_accuracy: 0.7758 - val_f1: 0.7744 - val_precision: 0.7840 - val_recall: 0.7650\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1268 - accuracy: 0.9676 - f1: 0.9692 - precision: 0.9758 - recall: 0.9627 - val_loss: 1.0626 - val_accuracy: 0.7708 - val_f1: 0.7765 - val_precision: 0.7885 - val_recall: 0.7650\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1326 - accuracy: 0.9649 - f1: 0.9677 - precision: 0.9750 - recall: 0.9606 - val_loss: 0.9037 - val_accuracy: 0.7575 - val_f1: 0.7553 - val_precision: 0.7742 - val_recall: 0.7375\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2134 - accuracy: 0.9450 - f1: 0.9446 - precision: 0.9549 - recall: 0.9347 - val_loss: 0.9060 - val_accuracy: 0.7900 - val_f1: 0.7900 - val_precision: 0.8086 - val_recall: 0.7725\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1969 - accuracy: 0.9501 - f1: 0.9507 - precision: 0.9611 - recall: 0.9405 - val_loss: 0.9742 - val_accuracy: 0.7625 - val_f1: 0.7616 - val_precision: 0.7810 - val_recall: 0.7433\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.4115 - accuracy: 0.8986 - f1: 0.8920 - precision: 0.9219 - recall: 0.8650 - val_loss: 1.0514 - val_accuracy: 0.7333 - val_f1: 0.7398 - val_precision: 0.7491 - val_recall: 0.7308\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.2201 - accuracy: 0.9414 - f1: 0.9440 - precision: 0.9563 - recall: 0.9321 - val_loss: 0.9793 - val_accuracy: 0.7658 - val_f1: 0.7612 - val_precision: 0.7720 - val_recall: 0.7508\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1584 - accuracy: 0.9603 - f1: 0.9597 - precision: 0.9663 - recall: 0.9533 - val_loss: 1.1648 - val_accuracy: 0.7567 - val_f1: 0.7534 - val_precision: 0.7621 - val_recall: 0.7450\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.4315 - accuracy: 0.8867 - f1: 0.8797 - precision: 0.8944 - recall: 0.8666 - val_loss: 0.9509 - val_accuracy: 0.7625 - val_f1: 0.7638 - val_precision: 0.7719 - val_recall: 0.7558\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 27ms/step - loss: 0.2267 - accuracy: 0.9371 - f1: 0.9394 - precision: 0.9500 - recall: 0.9292 - val_loss: 0.9477 - val_accuracy: 0.7733 - val_f1: 0.7730 - val_precision: 0.7874 - val_recall: 0.7592\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 0.1671 - accuracy: 0.9564 - f1: 0.9566 - precision: 0.9674 - recall: 0.9461 - val_loss: 0.9055 - val_accuracy: 0.7708 - val_f1: 0.7749 - val_precision: 0.7879 - val_recall: 0.7625\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 26ms/step - loss: 0.1723 - accuracy: 0.9517 - f1: 0.9534 - precision: 0.9651 - recall: 0.9422 - val_loss: 0.8609 - val_accuracy: 0.7958 - val_f1: 0.7951 - val_precision: 0.8146 - val_recall: 0.7767\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.8570 - accuracy: 0.8021 - f1: 0.8014 - precision: 0.8236 - recall: 0.7807\n",
            "[0.8569522500038147, 0.802142858505249, 0.8014446496963501, 0.8235778212547302, 0.7807143330574036]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92ffc64-4127-44e0-e860-aca48821ebe9"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 6s 63ms/step - loss: 1.3755 - accuracy: 0.3135 - f1: 5.0661e-04 - precision: 0.0144 - recall: 2.6090e-04 - val_loss: 1.2692 - val_accuracy: 0.4133 - val_f1: 0.0789 - val_precision: 0.8118 - val_recall: 0.0417\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 1.2295 - accuracy: 0.4488 - f1: 0.1879 - precision: 0.6785 - recall: 0.1151 - val_loss: 1.1402 - val_accuracy: 0.5142 - val_f1: 0.3919 - val_precision: 0.6040 - val_recall: 0.2908\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 1.0489 - accuracy: 0.5668 - f1: 0.4592 - precision: 0.6901 - recall: 0.3462 - val_loss: 1.0065 - val_accuracy: 0.5933 - val_f1: 0.5208 - val_precision: 0.6825 - val_recall: 0.4217\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.8935 - accuracy: 0.6451 - f1: 0.5898 - precision: 0.7346 - recall: 0.4945 - val_loss: 0.9000 - val_accuracy: 0.6392 - val_f1: 0.5990 - val_precision: 0.7167 - val_recall: 0.5150\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.7549 - accuracy: 0.7257 - f1: 0.6932 - precision: 0.7877 - recall: 0.6200 - val_loss: 0.8446 - val_accuracy: 0.6600 - val_f1: 0.6347 - val_precision: 0.7201 - val_recall: 0.5683\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.6263 - accuracy: 0.7684 - f1: 0.7498 - precision: 0.8202 - recall: 0.6912 - val_loss: 0.8185 - val_accuracy: 0.6833 - val_f1: 0.6709 - val_precision: 0.7316 - val_recall: 0.6200\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.5501 - accuracy: 0.7992 - f1: 0.7907 - precision: 0.8468 - recall: 0.7422 - val_loss: 0.7962 - val_accuracy: 0.6967 - val_f1: 0.6899 - val_precision: 0.7355 - val_recall: 0.6500\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.4677 - accuracy: 0.8283 - f1: 0.8229 - precision: 0.8644 - recall: 0.7856 - val_loss: 0.7991 - val_accuracy: 0.6992 - val_f1: 0.6981 - val_precision: 0.7433 - val_recall: 0.6583\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.4463 - accuracy: 0.8383 - f1: 0.8338 - precision: 0.8774 - recall: 0.7950 - val_loss: 0.7589 - val_accuracy: 0.7183 - val_f1: 0.7117 - val_precision: 0.7658 - val_recall: 0.6650\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.3980 - accuracy: 0.8600 - f1: 0.8570 - precision: 0.8914 - recall: 0.8256 - val_loss: 0.7666 - val_accuracy: 0.7242 - val_f1: 0.7293 - val_precision: 0.7537 - val_recall: 0.7067\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.3337 - accuracy: 0.8791 - f1: 0.8797 - precision: 0.9031 - recall: 0.8577 - val_loss: 0.7634 - val_accuracy: 0.7367 - val_f1: 0.7327 - val_precision: 0.7561 - val_recall: 0.7108\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.2682 - accuracy: 0.9013 - f1: 0.9000 - precision: 0.9204 - recall: 0.8807 - val_loss: 0.7177 - val_accuracy: 0.7492 - val_f1: 0.7440 - val_precision: 0.7726 - val_recall: 0.7175\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.2734 - accuracy: 0.9004 - f1: 0.9040 - precision: 0.9223 - recall: 0.8866 - val_loss: 0.8135 - val_accuracy: 0.7483 - val_f1: 0.7440 - val_precision: 0.7606 - val_recall: 0.7283\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 2s 38ms/step - loss: 0.2546 - accuracy: 0.9110 - f1: 0.9144 - precision: 0.9298 - recall: 0.8997 - val_loss: 0.7893 - val_accuracy: 0.7675 - val_f1: 0.7648 - val_precision: 0.7776 - val_recall: 0.7525\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.2249 - accuracy: 0.9191 - f1: 0.9180 - precision: 0.9285 - recall: 0.9077 - val_loss: 0.8042 - val_accuracy: 0.7475 - val_f1: 0.7474 - val_precision: 0.7658 - val_recall: 0.7300\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.2009 - accuracy: 0.9250 - f1: 0.9239 - precision: 0.9324 - recall: 0.9156 - val_loss: 0.8612 - val_accuracy: 0.7375 - val_f1: 0.7443 - val_precision: 0.7564 - val_recall: 0.7325\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1593 - accuracy: 0.9379 - f1: 0.9403 - precision: 0.9501 - recall: 0.9309 - val_loss: 1.0517 - val_accuracy: 0.7492 - val_f1: 0.7515 - val_precision: 0.7590 - val_recall: 0.7442\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1599 - accuracy: 0.9411 - f1: 0.9429 - precision: 0.9503 - recall: 0.9357 - val_loss: 0.9566 - val_accuracy: 0.7575 - val_f1: 0.7573 - val_precision: 0.7668 - val_recall: 0.7483\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1407 - accuracy: 0.9467 - f1: 0.9475 - precision: 0.9536 - recall: 0.9416 - val_loss: 0.9912 - val_accuracy: 0.7533 - val_f1: 0.7539 - val_precision: 0.7641 - val_recall: 0.7442\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1482 - accuracy: 0.9460 - f1: 0.9447 - precision: 0.9503 - recall: 0.9391 - val_loss: 1.0303 - val_accuracy: 0.7450 - val_f1: 0.7473 - val_precision: 0.7566 - val_recall: 0.7383\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1637 - accuracy: 0.9385 - f1: 0.9388 - precision: 0.9445 - recall: 0.9332 - val_loss: 0.9169 - val_accuracy: 0.7658 - val_f1: 0.7659 - val_precision: 0.7746 - val_recall: 0.7575\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1256 - accuracy: 0.9526 - f1: 0.9540 - precision: 0.9598 - recall: 0.9484 - val_loss: 0.9569 - val_accuracy: 0.7600 - val_f1: 0.7608 - val_precision: 0.7704 - val_recall: 0.7517\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.1253 - accuracy: 0.9526 - f1: 0.9533 - precision: 0.9603 - recall: 0.9465 - val_loss: 0.9803 - val_accuracy: 0.7683 - val_f1: 0.7660 - val_precision: 0.7738 - val_recall: 0.7583\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0919 - accuracy: 0.9687 - f1: 0.9661 - precision: 0.9701 - recall: 0.9623 - val_loss: 1.0160 - val_accuracy: 0.7642 - val_f1: 0.7664 - val_precision: 0.7746 - val_recall: 0.7583\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0706 - accuracy: 0.9742 - f1: 0.9720 - precision: 0.9753 - recall: 0.9687 - val_loss: 1.0576 - val_accuracy: 0.7658 - val_f1: 0.7665 - val_precision: 0.7698 - val_recall: 0.7633\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0750 - accuracy: 0.9739 - f1: 0.9730 - precision: 0.9754 - recall: 0.9706 - val_loss: 1.0776 - val_accuracy: 0.7717 - val_f1: 0.7689 - val_precision: 0.7763 - val_recall: 0.7617\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0721 - accuracy: 0.9718 - f1: 0.9711 - precision: 0.9735 - recall: 0.9688 - val_loss: 1.1361 - val_accuracy: 0.7492 - val_f1: 0.7481 - val_precision: 0.7556 - val_recall: 0.7408\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0672 - accuracy: 0.9729 - f1: 0.9738 - precision: 0.9766 - recall: 0.9711 - val_loss: 1.2079 - val_accuracy: 0.7517 - val_f1: 0.7509 - val_precision: 0.7596 - val_recall: 0.7425\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0728 - accuracy: 0.9737 - f1: 0.9739 - precision: 0.9760 - recall: 0.9718 - val_loss: 1.0927 - val_accuracy: 0.7600 - val_f1: 0.7588 - val_precision: 0.7678 - val_recall: 0.7500\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0830 - accuracy: 0.9640 - f1: 0.9646 - precision: 0.9684 - recall: 0.9608 - val_loss: 1.1340 - val_accuracy: 0.7517 - val_f1: 0.7529 - val_precision: 0.7593 - val_recall: 0.7467\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0689 - accuracy: 0.9717 - f1: 0.9720 - precision: 0.9744 - recall: 0.9697 - val_loss: 1.1501 - val_accuracy: 0.7375 - val_f1: 0.7416 - val_precision: 0.7492 - val_recall: 0.7342\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0786 - accuracy: 0.9620 - f1: 0.9627 - precision: 0.9655 - recall: 0.9599 - val_loss: 1.2112 - val_accuracy: 0.7550 - val_f1: 0.7589 - val_precision: 0.7664 - val_recall: 0.7517\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0704 - accuracy: 0.9701 - f1: 0.9707 - precision: 0.9735 - recall: 0.9680 - val_loss: 1.1813 - val_accuracy: 0.7583 - val_f1: 0.7591 - val_precision: 0.7632 - val_recall: 0.7550\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0529 - accuracy: 0.9782 - f1: 0.9783 - precision: 0.9798 - recall: 0.9768 - val_loss: 1.2749 - val_accuracy: 0.7483 - val_f1: 0.7472 - val_precision: 0.7504 - val_recall: 0.7442\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0521 - accuracy: 0.9751 - f1: 0.9751 - precision: 0.9768 - recall: 0.9734 - val_loss: 1.2014 - val_accuracy: 0.7650 - val_f1: 0.7651 - val_precision: 0.7677 - val_recall: 0.7625\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0497 - accuracy: 0.9762 - f1: 0.9760 - precision: 0.9788 - recall: 0.9733 - val_loss: 1.2361 - val_accuracy: 0.7567 - val_f1: 0.7553 - val_precision: 0.7573 - val_recall: 0.7533\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0472 - accuracy: 0.9761 - f1: 0.9774 - precision: 0.9803 - recall: 0.9746 - val_loss: 1.2160 - val_accuracy: 0.7692 - val_f1: 0.7673 - val_precision: 0.7705 - val_recall: 0.7642\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0596 - accuracy: 0.9802 - f1: 0.9802 - precision: 0.9807 - recall: 0.9797 - val_loss: 1.2537 - val_accuracy: 0.7450 - val_f1: 0.7475 - val_precision: 0.7527 - val_recall: 0.7425\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0621 - accuracy: 0.9727 - f1: 0.9733 - precision: 0.9757 - recall: 0.9710 - val_loss: 1.2093 - val_accuracy: 0.7608 - val_f1: 0.7615 - val_precision: 0.7647 - val_recall: 0.7583\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0489 - accuracy: 0.9762 - f1: 0.9758 - precision: 0.9776 - recall: 0.9741 - val_loss: 1.3381 - val_accuracy: 0.7592 - val_f1: 0.7619 - val_precision: 0.7664 - val_recall: 0.7575\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0489 - accuracy: 0.9744 - f1: 0.9748 - precision: 0.9778 - recall: 0.9718 - val_loss: 1.2752 - val_accuracy: 0.7658 - val_f1: 0.7674 - val_precision: 0.7706 - val_recall: 0.7642\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0518 - accuracy: 0.9758 - f1: 0.9763 - precision: 0.9787 - recall: 0.9739 - val_loss: 1.4461 - val_accuracy: 0.7442 - val_f1: 0.7452 - val_precision: 0.7480 - val_recall: 0.7425\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0457 - accuracy: 0.9784 - f1: 0.9782 - precision: 0.9805 - recall: 0.9759 - val_loss: 1.3078 - val_accuracy: 0.7733 - val_f1: 0.7733 - val_precision: 0.7750 - val_recall: 0.7717\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0509 - accuracy: 0.9724 - f1: 0.9725 - precision: 0.9736 - recall: 0.9713 - val_loss: 1.3703 - val_accuracy: 0.7742 - val_f1: 0.7736 - val_precision: 0.7765 - val_recall: 0.7708\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0452 - accuracy: 0.9771 - f1: 0.9772 - precision: 0.9781 - recall: 0.9763 - val_loss: 1.4042 - val_accuracy: 0.7708 - val_f1: 0.7722 - val_precision: 0.7761 - val_recall: 0.7683\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0465 - accuracy: 0.9763 - f1: 0.9773 - precision: 0.9790 - recall: 0.9755 - val_loss: 1.3789 - val_accuracy: 0.7642 - val_f1: 0.7664 - val_precision: 0.7703 - val_recall: 0.7625\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0651 - accuracy: 0.9670 - f1: 0.9670 - precision: 0.9688 - recall: 0.9652 - val_loss: 1.1712 - val_accuracy: 0.7750 - val_f1: 0.7747 - val_precision: 0.7786 - val_recall: 0.7708\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0533 - accuracy: 0.9742 - f1: 0.9737 - precision: 0.9755 - recall: 0.9718 - val_loss: 1.3669 - val_accuracy: 0.7775 - val_f1: 0.7790 - val_precision: 0.7823 - val_recall: 0.7758\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0615 - accuracy: 0.9739 - f1: 0.9752 - precision: 0.9777 - recall: 0.9727 - val_loss: 1.4607 - val_accuracy: 0.7667 - val_f1: 0.7674 - val_precision: 0.7689 - val_recall: 0.7658\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0392 - accuracy: 0.9787 - f1: 0.9792 - precision: 0.9801 - recall: 0.9782 - val_loss: 1.5232 - val_accuracy: 0.7633 - val_f1: 0.7635 - val_precision: 0.7654 - val_recall: 0.7617\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0426 - accuracy: 0.9794 - f1: 0.9787 - precision: 0.9803 - recall: 0.9771 - val_loss: 1.5037 - val_accuracy: 0.7850 - val_f1: 0.7855 - val_precision: 0.7868 - val_recall: 0.7842\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0401 - accuracy: 0.9781 - f1: 0.9783 - precision: 0.9795 - recall: 0.9771 - val_loss: 1.3781 - val_accuracy: 0.7867 - val_f1: 0.7865 - val_precision: 0.7888 - val_recall: 0.7842\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0449 - accuracy: 0.9747 - f1: 0.9745 - precision: 0.9761 - recall: 0.9729 - val_loss: 1.5213 - val_accuracy: 0.7717 - val_f1: 0.7721 - val_precision: 0.7750 - val_recall: 0.7692\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0379 - accuracy: 0.9828 - f1: 0.9824 - precision: 0.9847 - recall: 0.9801 - val_loss: 1.5094 - val_accuracy: 0.7867 - val_f1: 0.7858 - val_precision: 0.7874 - val_recall: 0.7842\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0370 - accuracy: 0.9761 - f1: 0.9767 - precision: 0.9790 - recall: 0.9745 - val_loss: 1.6102 - val_accuracy: 0.7742 - val_f1: 0.7737 - val_precision: 0.7766 - val_recall: 0.7708\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0326 - accuracy: 0.9826 - f1: 0.9814 - precision: 0.9852 - recall: 0.9776 - val_loss: 1.6208 - val_accuracy: 0.7800 - val_f1: 0.7785 - val_precision: 0.7812 - val_recall: 0.7758\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0387 - accuracy: 0.9797 - f1: 0.9801 - precision: 0.9815 - recall: 0.9787 - val_loss: 1.6633 - val_accuracy: 0.7558 - val_f1: 0.7550 - val_precision: 0.7576 - val_recall: 0.7525\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0450 - accuracy: 0.9781 - f1: 0.9776 - precision: 0.9801 - recall: 0.9752 - val_loss: 1.5349 - val_accuracy: 0.7817 - val_f1: 0.7829 - val_precision: 0.7867 - val_recall: 0.7792\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0438 - accuracy: 0.9772 - f1: 0.9778 - precision: 0.9802 - recall: 0.9755 - val_loss: 1.6975 - val_accuracy: 0.7583 - val_f1: 0.7584 - val_precision: 0.7594 - val_recall: 0.7575\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0527 - accuracy: 0.9738 - f1: 0.9740 - precision: 0.9759 - recall: 0.9721 - val_loss: 1.5154 - val_accuracy: 0.7592 - val_f1: 0.7593 - val_precision: 0.7602 - val_recall: 0.7583\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 1.3073 - accuracy: 0.7879 - f1: 0.7869 - precision: 0.7903 - recall: 0.7836\n",
            "[1.3073341846466064, 0.7878571152687073, 0.7869313359260559, 0.7903481125831604, 0.7835714221000671]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca00d92-5835-45b9-d43d-3b4e8f3d91ab"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 5s 84ms/step - loss: 1.3864 - accuracy: 0.2606 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3659 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.3799 - accuracy: 0.3014 - f1: 0.0018 - precision: 0.0747 - recall: 9.3571e-04 - val_loss: 1.3688 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.3878 - accuracy: 0.2772 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3667 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.3762 - accuracy: 0.3083 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3673 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.3709 - accuracy: 0.3117 - f1: 4.3697e-04 - precision: 0.0075 - recall: 2.2504e-04 - val_loss: 1.3717 - val_accuracy: 0.3142 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.3576 - accuracy: 0.3473 - f1: 0.0082 - precision: 0.1045 - recall: 0.0044 - val_loss: 1.3225 - val_accuracy: 0.3583 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.2973 - accuracy: 0.3877 - f1: 0.0901 - precision: 0.5907 - recall: 0.0512 - val_loss: 1.3279 - val_accuracy: 0.3692 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.2849 - accuracy: 0.4118 - f1: 0.1064 - precision: 0.4788 - recall: 0.0630 - val_loss: 1.3060 - val_accuracy: 0.3558 - val_f1: 0.0179 - val_precision: 0.4306 - val_recall: 0.0092\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.2765 - accuracy: 0.4141 - f1: 0.1311 - precision: 0.5694 - recall: 0.0800 - val_loss: 1.2873 - val_accuracy: 0.4142 - val_f1: 0.1228 - val_precision: 0.5418 - val_recall: 0.0700\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 1.2098 - accuracy: 0.4764 - f1: 0.2597 - precision: 0.5917 - recall: 0.1725 - val_loss: 1.3123 - val_accuracy: 0.3783 - val_f1: 0.1244 - val_precision: 0.5001 - val_recall: 0.0717\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.2087 - accuracy: 0.4735 - f1: 0.2779 - precision: 0.5561 - recall: 0.1908 - val_loss: 1.2738 - val_accuracy: 0.4350 - val_f1: 0.3234 - val_precision: 0.4709 - val_recall: 0.2467\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.1434 - accuracy: 0.5225 - f1: 0.3895 - precision: 0.5968 - recall: 0.2920 - val_loss: 1.2844 - val_accuracy: 0.4217 - val_f1: 0.3169 - val_precision: 0.4748 - val_recall: 0.2383\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.1138 - accuracy: 0.5449 - f1: 0.4446 - precision: 0.6149 - recall: 0.3509 - val_loss: 1.2502 - val_accuracy: 0.4600 - val_f1: 0.2689 - val_precision: 0.5213 - val_recall: 0.1817\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 3s 73ms/step - loss: 1.1049 - accuracy: 0.5593 - f1: 0.4032 - precision: 0.6136 - recall: 0.3041 - val_loss: 1.2208 - val_accuracy: 0.4650 - val_f1: 0.3638 - val_precision: 0.5698 - val_recall: 0.2675\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 1.1014 - accuracy: 0.5457 - f1: 0.4421 - precision: 0.6260 - recall: 0.3443 - val_loss: 1.2646 - val_accuracy: 0.4650 - val_f1: 0.3088 - val_precision: 0.5044 - val_recall: 0.2233\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 76ms/step - loss: 1.1693 - accuracy: 0.5164 - f1: 0.3545 - precision: 0.5829 - recall: 0.2563 - val_loss: 1.2512 - val_accuracy: 0.4625 - val_f1: 0.3188 - val_precision: 0.5080 - val_recall: 0.2325\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 1.1150 - accuracy: 0.5435 - f1: 0.4420 - precision: 0.6193 - recall: 0.3481 - val_loss: 1.2606 - val_accuracy: 0.4642 - val_f1: 0.3867 - val_precision: 0.5426 - val_recall: 0.3008\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 1.1054 - accuracy: 0.5571 - f1: 0.4390 - precision: 0.6194 - recall: 0.3416 - val_loss: 1.2350 - val_accuracy: 0.4942 - val_f1: 0.4164 - val_precision: 0.5367 - val_recall: 0.3408\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.0637 - accuracy: 0.5653 - f1: 0.4873 - precision: 0.6271 - recall: 0.4019 - val_loss: 1.2641 - val_accuracy: 0.4642 - val_f1: 0.3404 - val_precision: 0.5464 - val_recall: 0.2475\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.0752 - accuracy: 0.5659 - f1: 0.4269 - precision: 0.6549 - recall: 0.3201 - val_loss: 1.2295 - val_accuracy: 0.4900 - val_f1: 0.4430 - val_precision: 0.5259 - val_recall: 0.3833\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.0046 - accuracy: 0.5978 - f1: 0.5478 - precision: 0.6688 - recall: 0.4645 - val_loss: 1.2570 - val_accuracy: 0.4800 - val_f1: 0.4385 - val_precision: 0.5131 - val_recall: 0.3833\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.0285 - accuracy: 0.5965 - f1: 0.5358 - precision: 0.6625 - recall: 0.4507 - val_loss: 1.2824 - val_accuracy: 0.4658 - val_f1: 0.4269 - val_precision: 0.5167 - val_recall: 0.3642\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.9901 - accuracy: 0.6085 - f1: 0.5615 - precision: 0.6882 - recall: 0.4753 - val_loss: 1.2743 - val_accuracy: 0.4892 - val_f1: 0.4610 - val_precision: 0.5208 - val_recall: 0.4142\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.9793 - accuracy: 0.6121 - f1: 0.5690 - precision: 0.6814 - recall: 0.4897 - val_loss: 1.2821 - val_accuracy: 0.5000 - val_f1: 0.4570 - val_precision: 0.5192 - val_recall: 0.4083\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.9430 - accuracy: 0.6304 - f1: 0.5840 - precision: 0.6890 - recall: 0.5082 - val_loss: 1.3557 - val_accuracy: 0.4883 - val_f1: 0.4637 - val_precision: 0.5023 - val_recall: 0.4308\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.9399 - accuracy: 0.6427 - f1: 0.6037 - precision: 0.6981 - recall: 0.5336 - val_loss: 1.3032 - val_accuracy: 0.4708 - val_f1: 0.4488 - val_precision: 0.5091 - val_recall: 0.4017\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.9554 - accuracy: 0.6299 - f1: 0.5811 - precision: 0.6811 - recall: 0.5089 - val_loss: 1.3230 - val_accuracy: 0.4825 - val_f1: 0.4577 - val_precision: 0.5120 - val_recall: 0.4142\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.9922 - accuracy: 0.5962 - f1: 0.5481 - precision: 0.6752 - recall: 0.4676 - val_loss: 1.3231 - val_accuracy: 0.4667 - val_f1: 0.4390 - val_precision: 0.4997 - val_recall: 0.3917\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 1.0402 - accuracy: 0.5836 - f1: 0.5392 - precision: 0.6558 - recall: 0.4634 - val_loss: 1.3058 - val_accuracy: 0.4392 - val_f1: 0.3126 - val_precision: 0.4897 - val_recall: 0.2300\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 1.0727 - accuracy: 0.5646 - f1: 0.4927 - precision: 0.6730 - recall: 0.3934 - val_loss: 1.2608 - val_accuracy: 0.4725 - val_f1: 0.4309 - val_precision: 0.5321 - val_recall: 0.3625\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.9679 - accuracy: 0.6116 - f1: 0.5560 - precision: 0.6996 - recall: 0.4646 - val_loss: 1.3308 - val_accuracy: 0.4542 - val_f1: 0.4250 - val_precision: 0.5107 - val_recall: 0.3642\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.9680 - accuracy: 0.6243 - f1: 0.5743 - precision: 0.6989 - recall: 0.4897 - val_loss: 1.3110 - val_accuracy: 0.4858 - val_f1: 0.4635 - val_precision: 0.5253 - val_recall: 0.4150\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.9148 - accuracy: 0.6562 - f1: 0.6035 - precision: 0.7054 - recall: 0.5299 - val_loss: 1.3038 - val_accuracy: 0.4683 - val_f1: 0.4398 - val_precision: 0.5291 - val_recall: 0.3767\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.9303 - accuracy: 0.6340 - f1: 0.5774 - precision: 0.7101 - recall: 0.4885 - val_loss: 1.3273 - val_accuracy: 0.4675 - val_f1: 0.4555 - val_precision: 0.5093 - val_recall: 0.4125\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.8671 - accuracy: 0.6725 - f1: 0.6280 - precision: 0.7243 - recall: 0.5565 - val_loss: 1.4132 - val_accuracy: 0.4875 - val_f1: 0.4774 - val_precision: 0.5134 - val_recall: 0.4467\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.8656 - accuracy: 0.6703 - f1: 0.6426 - precision: 0.7210 - recall: 0.5811 - val_loss: 1.4010 - val_accuracy: 0.4975 - val_f1: 0.4838 - val_precision: 0.5115 - val_recall: 0.4592\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.8475 - accuracy: 0.6759 - f1: 0.6522 - precision: 0.7313 - recall: 0.5903 - val_loss: 1.3777 - val_accuracy: 0.4733 - val_f1: 0.4643 - val_precision: 0.5133 - val_recall: 0.4242\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.7982 - accuracy: 0.7020 - f1: 0.6819 - precision: 0.7521 - recall: 0.6248 - val_loss: 1.3434 - val_accuracy: 0.4833 - val_f1: 0.4621 - val_precision: 0.5180 - val_recall: 0.4175\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.7303 - accuracy: 0.7342 - f1: 0.7097 - precision: 0.7777 - recall: 0.6536 - val_loss: 1.4535 - val_accuracy: 0.4767 - val_f1: 0.4659 - val_precision: 0.4922 - val_recall: 0.4425\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.7977 - accuracy: 0.7004 - f1: 0.6803 - precision: 0.7479 - recall: 0.6267 - val_loss: 1.2915 - val_accuracy: 0.4900 - val_f1: 0.4545 - val_precision: 0.5237 - val_recall: 0.4017\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.7487 - accuracy: 0.7197 - f1: 0.7027 - precision: 0.7787 - recall: 0.6419 - val_loss: 1.3608 - val_accuracy: 0.4825 - val_f1: 0.4675 - val_precision: 0.5224 - val_recall: 0.4233\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.6857 - accuracy: 0.7489 - f1: 0.7371 - precision: 0.8054 - recall: 0.6803 - val_loss: 1.3637 - val_accuracy: 0.4992 - val_f1: 0.4813 - val_precision: 0.5369 - val_recall: 0.4367\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 0.6668 - accuracy: 0.7591 - f1: 0.7415 - precision: 0.7994 - recall: 0.6924 - val_loss: 1.3595 - val_accuracy: 0.5100 - val_f1: 0.4876 - val_precision: 0.5278 - val_recall: 0.4533\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.6514 - accuracy: 0.7691 - f1: 0.7613 - precision: 0.8071 - recall: 0.7208 - val_loss: 1.4032 - val_accuracy: 0.4800 - val_f1: 0.4706 - val_precision: 0.5130 - val_recall: 0.4350\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.6045 - accuracy: 0.7903 - f1: 0.7821 - precision: 0.8215 - recall: 0.7467 - val_loss: 1.4306 - val_accuracy: 0.4733 - val_f1: 0.4532 - val_precision: 0.4971 - val_recall: 0.4167\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.6885 - accuracy: 0.7494 - f1: 0.7465 - precision: 0.7890 - recall: 0.7090 - val_loss: 1.5147 - val_accuracy: 0.4658 - val_f1: 0.4694 - val_precision: 0.4979 - val_recall: 0.4442\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.7071 - accuracy: 0.7461 - f1: 0.7354 - precision: 0.7726 - recall: 0.7022 - val_loss: 1.3848 - val_accuracy: 0.4675 - val_f1: 0.4537 - val_precision: 0.4983 - val_recall: 0.4167\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 3s 74ms/step - loss: 0.6911 - accuracy: 0.7573 - f1: 0.7387 - precision: 0.7937 - recall: 0.6919 - val_loss: 1.4883 - val_accuracy: 0.4775 - val_f1: 0.4683 - val_precision: 0.4924 - val_recall: 0.4467\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 0.5880 - accuracy: 0.8014 - f1: 0.7991 - precision: 0.8360 - recall: 0.7659 - val_loss: 1.4454 - val_accuracy: 0.4783 - val_f1: 0.4678 - val_precision: 0.5158 - val_recall: 0.4283\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.6549 - accuracy: 0.7725 - f1: 0.7691 - precision: 0.8098 - recall: 0.7328 - val_loss: 1.4047 - val_accuracy: 0.4808 - val_f1: 0.4654 - val_precision: 0.5077 - val_recall: 0.4300\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.6826 - accuracy: 0.7584 - f1: 0.7448 - precision: 0.8054 - recall: 0.6931 - val_loss: 1.4297 - val_accuracy: 0.4825 - val_f1: 0.4764 - val_precision: 0.5066 - val_recall: 0.4500\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.6765 - accuracy: 0.7642 - f1: 0.7522 - precision: 0.7924 - recall: 0.7163 - val_loss: 1.4211 - val_accuracy: 0.4833 - val_f1: 0.4762 - val_precision: 0.5150 - val_recall: 0.4433\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 0.6395 - accuracy: 0.7766 - f1: 0.7735 - precision: 0.8192 - recall: 0.7331 - val_loss: 1.4794 - val_accuracy: 0.5033 - val_f1: 0.4903 - val_precision: 0.5156 - val_recall: 0.4675\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.6254 - accuracy: 0.7926 - f1: 0.7831 - precision: 0.8222 - recall: 0.7482 - val_loss: 1.5118 - val_accuracy: 0.4975 - val_f1: 0.4890 - val_precision: 0.5169 - val_recall: 0.4642\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 76ms/step - loss: 0.7399 - accuracy: 0.7497 - f1: 0.7407 - precision: 0.7830 - recall: 0.7034 - val_loss: 1.4867 - val_accuracy: 0.5000 - val_f1: 0.4975 - val_precision: 0.5156 - val_recall: 0.4808\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.6021 - accuracy: 0.8031 - f1: 0.7974 - precision: 0.8344 - recall: 0.7639 - val_loss: 1.5013 - val_accuracy: 0.4950 - val_f1: 0.4866 - val_precision: 0.5148 - val_recall: 0.4617\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 3s 76ms/step - loss: 0.5774 - accuracy: 0.8099 - f1: 0.8023 - precision: 0.8393 - recall: 0.7690 - val_loss: 1.4830 - val_accuracy: 0.4833 - val_f1: 0.4795 - val_precision: 0.5058 - val_recall: 0.4558\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.5930 - accuracy: 0.8137 - f1: 0.8042 - precision: 0.8460 - recall: 0.7666 - val_loss: 1.4858 - val_accuracy: 0.4992 - val_f1: 0.4835 - val_precision: 0.5171 - val_recall: 0.4542\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 0.5543 - accuracy: 0.8237 - f1: 0.8188 - precision: 0.8488 - recall: 0.7912 - val_loss: 1.7458 - val_accuracy: 0.4717 - val_f1: 0.4684 - val_precision: 0.4829 - val_recall: 0.4550\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.5895 - accuracy: 0.8053 - f1: 0.8007 - precision: 0.8293 - recall: 0.7743 - val_loss: 1.5555 - val_accuracy: 0.5108 - val_f1: 0.5087 - val_precision: 0.5300 - val_recall: 0.4892\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.6049 - accuracy: 0.5071 - f1: 0.5072 - precision: 0.5327 - recall: 0.4843\n",
            "[1.6049323081970215, 0.5071428418159485, 0.5072462558746338, 0.5326724648475647, 0.48428574204444885]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "b7239858-6e23-4f01-a6bc-c1c4c9f89385"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "print(history_sg_bi.history)\r\n",
        "plt.title(\"Skip-Gram + Bi-LSTM accuracy, val accuracy vs epochs Graph\")\r\n",
        "plt.plot(history_sg_bi.history['val_accuracy'],c='b',label='val_accuracy')\r\n",
        "plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()\r\n",
        "plt.title(\"GloVe + Bi-LSTM accuracy, val accuracy vs epochs Graph\")\r\n",
        "plt.plot(history_glove_bi.history['val_accuracy'],c='b',label='val_accuracy')\r\n",
        "plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.4871428608894348, 0.5964285731315613, 0.28928571939468384]\n",
            "cbow [0.49714285135269165, 0.602142870426178, 0.3442857265472412]\n",
            "glove [0.802142858505249, 0.7878571152687073, 0.5071428418159485]\n",
            "{'loss': [1.3783377408981323, 1.3613847494125366, 1.3077408075332642, 1.2208513021469116, 1.1554948091506958, 1.1048213243484497, 1.0709242820739746, 1.0345042943954468, 1.0128980875015259, 0.9775596261024475, 0.9548526406288147, 0.9357914328575134, 0.9308721423149109, 0.8856363296508789, 0.8606762886047363, 0.8119085431098938, 0.7764349579811096, 0.7502379417419434, 0.7323710918426514, 0.6955975890159607, 0.662286639213562, 0.6362349390983582, 0.6550601124763489, 0.5821753740310669, 0.5488330721855164, 0.5501968264579773, 0.49769100546836853, 0.495244562625885, 0.4580874741077423, 0.43802571296691895, 0.44183555245399475, 0.38913992047309875, 0.35995596647262573, 0.38384565711021423, 0.343825101852417, 0.3257773220539093, 0.3032320439815521, 0.271626353263855, 0.27126869559288025, 0.2589530646800995, 0.24110940098762512, 0.23187176883220673, 0.21255293488502502, 0.23075848817825317, 0.19946809113025665, 0.21608597040176392, 0.21901513636112213, 0.19274035096168518, 0.1892232447862625, 0.1866060346364975, 0.16842235624790192, 0.14064545929431915, 0.15709513425827026, 0.15743055939674377, 0.1476302146911621, 0.15043827891349792, 0.12747274339199066, 0.12411996722221375, 0.155667245388031, 0.14462825655937195], 'accuracy': [0.2991304397583008, 0.3204347789287567, 0.3928260803222656, 0.46934783458709717, 0.511956512928009, 0.5445652008056641, 0.5645652413368225, 0.5860869288444519, 0.6010869741439819, 0.6080434918403625, 0.6178261041641235, 0.6273912787437439, 0.622826099395752, 0.6508695483207703, 0.6628260612487793, 0.6854347586631775, 0.6986956596374512, 0.7095652222633362, 0.7121739387512207, 0.729347825050354, 0.748913049697876, 0.751086950302124, 0.7530434727668762, 0.7756521701812744, 0.7910869717597961, 0.7895652055740356, 0.8128260970115662, 0.8110869526863098, 0.8286956548690796, 0.8360869288444519, 0.833695650100708, 0.8584782481193542, 0.8684782385826111, 0.8567391037940979, 0.877173900604248, 0.8806521892547607, 0.8891304135322571, 0.9026086926460266, 0.9082608819007874, 0.908695638179779, 0.9134782552719116, 0.916304349899292, 0.9276086688041687, 0.9167391061782837, 0.9291304349899292, 0.9223912954330444, 0.926086962223053, 0.9356521964073181, 0.9345652461051941, 0.9328261017799377, 0.9408695697784424, 0.9519565105438232, 0.946739137172699, 0.9452173709869385, 0.9502173662185669, 0.9465217590332031, 0.95652174949646, 0.9589130282402039, 0.9495652318000793, 0.9480434656143188], 'f1': [0.0, 0.0, 0.018824642524123192, 0.19968672096729279, 0.32489868998527527, 0.41580066084861755, 0.45091572403907776, 0.4980599880218506, 0.5151415467262268, 0.538800060749054, 0.5659878253936768, 0.571678638458252, 0.5740697979927063, 0.6081278324127197, 0.6231256723403931, 0.655026376247406, 0.6650884747505188, 0.6832863092422485, 0.6898671388626099, 0.7153005003929138, 0.7333545684814453, 0.7423657774925232, 0.7315915822982788, 0.7706550359725952, 0.7817908525466919, 0.7841061353683472, 0.8073899745941162, 0.8128104209899902, 0.8252781629562378, 0.833381175994873, 0.8301355242729187, 0.8572383522987366, 0.8666453957557678, 0.8593525886535645, 0.8731066584587097, 0.8826904296875, 0.8880668878555298, 0.9041467905044556, 0.9047247171401978, 0.906899631023407, 0.9129371643066406, 0.9145037531852722, 0.9261780381202698, 0.9171444177627563, 0.9317075610160828, 0.9240738153457642, 0.923038125038147, 0.9349079728126526, 0.9342218637466431, 0.9321091771125793, 0.9414544105529785, 0.9526640772819519, 0.9474266171455383, 0.9456848502159119, 0.95017009973526, 0.9455967545509338, 0.9558262825012207, 0.959566056728363, 0.9507842659950256, 0.9479361176490784], 'precision': [0.0, 0.0, 0.24277269840240479, 0.6803163290023804, 0.6578745245933533, 0.6770671010017395, 0.686642587184906, 0.7052183747291565, 0.7074637413024902, 0.7355988621711731, 0.7356061339378357, 0.7528116703033447, 0.7494249939918518, 0.7633174657821655, 0.7730125784873962, 0.7903299927711487, 0.7968955039978027, 0.8129375576972961, 0.8100427389144897, 0.8136257529258728, 0.8286480903625488, 0.8345565795898438, 0.8308696746826172, 0.8558052182197571, 0.8565493822097778, 0.8527401089668274, 0.8745636343955994, 0.8763837218284607, 0.8799439072608948, 0.8793390393257141, 0.8676952123641968, 0.8943266868591309, 0.9008013010025024, 0.8905194401741028, 0.9028252959251404, 0.9054164290428162, 0.9114284515380859, 0.9243444204330444, 0.9244660139083862, 0.9275352358818054, 0.9320499300956726, 0.9320304989814758, 0.9400864243507385, 0.9323336482048035, 0.9433807730674744, 0.9371873736381531, 0.9356637001037598, 0.9439796805381775, 0.9453057646751404, 0.9416614770889282, 0.9491838216781616, 0.960756778717041, 0.9543610215187073, 0.9537416696548462, 0.9572438597679138, 0.9519962668418884, 0.9627087712287903, 0.9664581418037415, 0.9562674164772034, 0.954734206199646], 'recall': [0.0, 0.0, 0.010217390954494476, 0.12239132076501846, 0.2191304713487625, 0.30195650458335876, 0.3384782671928406, 0.38695651292800903, 0.40804341435432434, 0.427173912525177, 0.4630434513092041, 0.46478259563446045, 0.46956509351730347, 0.5100000500679016, 0.525869607925415, 0.5615217685699463, 0.5723912715911865, 0.5915217399597168, 0.6032609343528748, 0.6397826075553894, 0.6597825288772583, 0.6704347133636475, 0.656521737575531, 0.7021738886833191, 0.720434844493866, 0.7269565463066101, 0.7513042092323303, 0.7599999904632568, 0.7782609462738037, 0.7928261756896973, 0.79630446434021, 0.8234782218933105, 0.8352175354957581, 0.8306521773338318, 0.8456521034240723, 0.8613044023513794, 0.866087019443512, 0.8850001692771912, 0.8860868215560913, 0.8873911499977112, 0.894782543182373, 0.897826075553894, 0.9128261208534241, 0.9026084542274475, 0.9204347729682922, 0.9115215539932251, 0.9108695387840271, 0.9260867834091187, 0.9234781861305237, 0.9228259325027466, 0.9339128732681274, 0.9447824358940125, 0.9406520128250122, 0.9378259181976318, 0.9432607889175415, 0.9393477439880371, 0.9491304159164429, 0.9528261423110962, 0.9454349875450134, 0.9413043856620789], 'val_loss': [1.3662834167480469, 1.3491543531417847, 1.317291021347046, 1.3429855108261108, 1.2968502044677734, 1.288368582725525, 1.253438115119934, 1.2343616485595703, 1.2596049308776855, 1.2236783504486084, 1.2324042320251465, 1.2405879497528076, 1.2188751697540283, 1.2362624406814575, 1.2487424612045288, 1.2532330751419067, 1.2733843326568604, 1.283897876739502, 1.2704458236694336, 1.294669508934021, 1.3031117916107178, 1.375368356704712, 1.276659607887268, 1.313222885131836, 1.3594714403152466, 1.4168214797973633, 1.387901782989502, 1.336929440498352, 1.3796725273132324, 1.3967230319976807, 1.4259058237075806, 1.4148883819580078, 1.5049314498901367, 1.502932071685791, 1.6094266176223755, 1.7210620641708374, 1.6557585000991821, 1.7232156991958618, 1.6681392192840576, 1.8130464553833008, 1.8901454210281372, 2.0437850952148438, 1.840912103652954, 2.166069746017456, 2.1725828647613525, 2.153472423553467, 2.1982030868530273, 2.0958938598632812, 2.035825490951538, 2.0019330978393555, 2.1470298767089844, 2.381943464279175, 2.3351516723632812, 2.230849504470825, 2.3513972759246826, 2.183500051498413, 2.245600461959839, 2.3752081394195557, 2.2575037479400635, 2.260071039199829], 'val_accuracy': [0.3291666805744171, 0.3675000071525574, 0.3891666531562805, 0.39750000834465027, 0.41333332657814026, 0.4308333396911621, 0.45750001072883606, 0.4699999988079071, 0.4675000011920929, 0.49000000953674316, 0.4858333468437195, 0.4308333396911621, 0.46000000834465027, 0.47749999165534973, 0.4841666519641876, 0.49916666746139526, 0.5049999952316284, 0.5149999856948853, 0.5083333253860474, 0.49166667461395264, 0.5066666603088379, 0.512499988079071, 0.5291666388511658, 0.5416666865348816, 0.5191666483879089, 0.5350000262260437, 0.5224999785423279, 0.5408333539962769, 0.5533333420753479, 0.5425000190734863, 0.5416666865348816, 0.574999988079071, 0.5608333349227905, 0.5691666603088379, 0.574999988079071, 0.5600000023841858, 0.5774999856948853, 0.5816666483879089, 0.6041666865348816, 0.5991666913032532, 0.5991666913032532, 0.5824999809265137, 0.5883333086967468, 0.5816666483879089, 0.5941666960716248, 0.5774999856948853, 0.6066666841506958, 0.5983333587646484, 0.6000000238418579, 0.6000000238418579, 0.6050000190734863, 0.5950000286102295, 0.5975000262260437, 0.621666669845581, 0.6050000190734863, 0.612500011920929, 0.6083333492279053, 0.6225000023841858, 0.6158333420753479, 0.628333330154419], 'val_f1': [0.0, 0.0, 0.08395419269800186, 0.26725146174430847, 0.2942238748073578, 0.3166956901550293, 0.343854695558548, 0.3481461703777313, 0.37151601910591125, 0.3648933470249176, 0.36147749423980713, 0.30110421776771545, 0.33912816643714905, 0.3912096917629242, 0.4340115785598755, 0.4205847680568695, 0.43475791811943054, 0.4494635760784149, 0.45030033588409424, 0.462054044008255, 0.4647041857242584, 0.47023794054985046, 0.48837316036224365, 0.5084659457206726, 0.49058929085731506, 0.5253132581710815, 0.49802374839782715, 0.525873601436615, 0.5309076905250549, 0.5183307528495789, 0.5313423275947571, 0.5650932788848877, 0.5538289546966553, 0.5514429211616516, 0.5631750226020813, 0.553521990776062, 0.573782742023468, 0.574851930141449, 0.5977548956871033, 0.5957074761390686, 0.5939798355102539, 0.575175940990448, 0.5891178250312805, 0.5734819173812866, 0.5906901359558105, 0.5765364766120911, 0.5979477167129517, 0.5961545705795288, 0.5934072136878967, 0.5954300761222839, 0.6047072410583496, 0.5925208330154419, 0.5937466621398926, 0.6208584904670715, 0.6078607439994812, 0.609298050403595, 0.6126720309257507, 0.6203014850616455, 0.6137986779212952, 0.6262767910957336], 'val_precision': [0.0, 0.0, 0.5328115820884705, 0.47585079073905945, 0.5072368383407593, 0.5089443922042847, 0.5343165397644043, 0.5708966851234436, 0.5313923358917236, 0.5710141062736511, 0.5699062347412109, 0.5889295339584351, 0.6062950491905212, 0.5705670714378357, 0.5791541934013367, 0.5663682818412781, 0.5794218182563782, 0.5775224566459656, 0.5807340741157532, 0.5772091746330261, 0.5889728665351868, 0.5748457312583923, 0.5888944268226624, 0.5865495204925537, 0.5766445994377136, 0.5936101675033569, 0.5895536541938782, 0.5958836674690247, 0.5918264985084534, 0.5794965624809265, 0.5902853608131409, 0.6155649423599243, 0.5914167761802673, 0.5996840596199036, 0.5979236960411072, 0.5865185260772705, 0.6019191741943359, 0.6072447896003723, 0.6265568137168884, 0.6191515326499939, 0.6115300059318542, 0.5951645374298096, 0.6077809929847717, 0.5923933982849121, 0.6103300452232361, 0.5914555788040161, 0.6161799430847168, 0.6125009655952454, 0.6103292107582092, 0.6129236221313477, 0.6184527277946472, 0.6085278391838074, 0.6064359545707703, 0.6304079294204712, 0.619566798210144, 0.6180526614189148, 0.625932514667511, 0.6274710893630981, 0.6247168183326721, 0.6336521506309509], 'val_recall': [0.0, 0.0, 0.04583333060145378, 0.18666668236255646, 0.2083333283662796, 0.23083335161209106, 0.2541666626930237, 0.2516666650772095, 0.2866666615009308, 0.26916664838790894, 0.26583331823349, 0.20333333313465118, 0.23666667938232422, 0.2983333170413971, 0.3474999964237213, 0.33500000834465027, 0.34833332896232605, 0.3683333396911621, 0.3683333098888397, 0.38583335280418396, 0.3841666281223297, 0.398333340883255, 0.41749998927116394, 0.4491666853427887, 0.42750000953674316, 0.47166669368743896, 0.43166670203208923, 0.47083333134651184, 0.4816666543483734, 0.46916672587394714, 0.4833333492279053, 0.5225000381469727, 0.5208333134651184, 0.5108333230018616, 0.5324999690055847, 0.5241666436195374, 0.5483333468437195, 0.54583340883255, 0.5716666579246521, 0.5741665959358215, 0.57750004529953, 0.5566666722297668, 0.5716666579246521, 0.5558333396911621, 0.5724999904632568, 0.5625, 0.580833375453949, 0.580833375453949, 0.57750004529953, 0.5791667103767395, 0.5916666984558105, 0.57750004529953, 0.5816666483879089, 0.6116666197776794, 0.5966667532920837, 0.6008333563804626, 0.6000000238418579, 0.6133333444595337, 0.6033332943916321, 0.6191666722297668]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}