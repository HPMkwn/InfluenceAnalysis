{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointnegpos4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointnegpos4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478f2a3b-2c9b-4220-930a-aca99517e0e4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bce8ad4-7ee8-4b1e-dbd0-6ff379cd45cb"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['oh', 'goodness', 'i', 'am', 'loving', 'this', 'rainy', 'day', 'it', 'is', 'a', 'head', 'down', 'creative', 'cap', 'on', 'and', 'get', 'lost', 'in', 'your', 'thoughts', 'kind', 'of', 'day', 'thought', 'balloon', 'smiling', 'faceÔ∏è', 'art', 'create', 'biz'], ['uplift', 'if', 'you', 'are', 'still', 'discouraged', 'it', 'means', 'ur', 'listening', 'to', 'the', 'wrong', 'voices', 'and', 'loking', 'to', 'the', 'wrong', 'source', 'lok', 'to', 'the', 'lord'], ['never', 'a', 'dull', 'moment', 'when', 'talking', 'to', 'nell', 'face', 'with', 'tears', 'of', 'joyface', 'with', 'tears', 'of', 'joyface', 'with', 'tears', 'of', 'joyface', 'with', 'tears', 'of', 'joyface', 'savouring', 'delicious', 'food'], ['i', 'added', 'paul', 'walker', 'on', 'xbox', 'but', 'he', 'just', 'spends', 'all', 'of', 'his', 'time', 'on', 'the', 'dashboard', 'humor', 'funny'], ['sorry', 'to', 'hear', 'about', 'your', 'experience', 'however', 'please', 'do', 'not', 'hesitate', 'to', 'contact', 'us', 'via', 'live', 'chat', 'or', 'email', 'at', 'askdysonus', 'com']]\n",
            "[[0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [0 1 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor=1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYsrqGsXqbjZ"
      },
      "source": [
        "positive = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/positive-words.csv')\r\n",
        "positive = list(positive['0'][1:])\r\n",
        "negative = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/negative-words.csv')\r\n",
        "negative = list(negative['0'][2:])\r\n",
        "pn_dict = {}\r\n",
        "for i in range(len(positive)):\r\n",
        "  pn_dict[positive[i]] = 1\r\n",
        "\r\n",
        "for i in range(len(negative)):\r\n",
        "  pn_dict[negative[i]] = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xYyXPmkpgpP"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.30, random_state=42)\r\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\r\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\r\n",
        "X_train = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_train]\r\n",
        "X_test = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_test]\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          tweet[i]=((tweet[i][0]*tweet[i][2]),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='RB':\r\n",
        "            tweet[i]=((tweet[i][0]*mul_factor),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0]),tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0]),tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          tweet[i]=((tweet[i][0]*tweet[i][2]),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='RB':\r\n",
        "            tweet[i]=((tweet[i][0]*mul_factor),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0]),tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0]),tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad183da-e484-400c-9a19-d94b1ceeeeb8"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history_sg_lstm = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-f0e292bfb240>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3786 - accuracy: 0.3149\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.3758 - accuracy: 0.3171\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3757 - accuracy: 0.3173\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.3754 - accuracy: 0.3173\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3752 - accuracy: 0.3173\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3751 - accuracy: 0.3173\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.3750 - accuracy: 0.3173\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3749 - accuracy: 0.3173\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3748 - accuracy: 0.3173\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.3747 - accuracy: 0.3173\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.3747 - accuracy: 0.3173\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.3751 - accuracy: 0.3165\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3747 - accuracy: 0.3173\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3745 - accuracy: 0.3173\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.3730 - accuracy: 0.3173\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.3701 - accuracy: 0.3214\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.3587 - accuracy: 0.3443\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.3491 - accuracy: 0.3586\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.3426 - accuracy: 0.3671\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.3271 - accuracy: 0.3865\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3198 - accuracy: 0.3941\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3061 - accuracy: 0.4031\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.2985 - accuracy: 0.4137\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.2913 - accuracy: 0.4178\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.2853 - accuracy: 0.4243\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.2632 - accuracy: 0.4373\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.2554 - accuracy: 0.4420\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.2383 - accuracy: 0.4488\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.2149 - accuracy: 0.4594\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 1.1955 - accuracy: 0.4729\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.1755 - accuracy: 0.4818\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.1613 - accuracy: 0.4916\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.1404 - accuracy: 0.4998\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.1238 - accuracy: 0.5139\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.1102 - accuracy: 0.5224\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.0856 - accuracy: 0.5400\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.0656 - accuracy: 0.5596\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.0480 - accuracy: 0.5676\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.0456 - accuracy: 0.5651\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.0355 - accuracy: 0.5696\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.0571 - accuracy: 0.5578\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.0324 - accuracy: 0.5639\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.0002 - accuracy: 0.5851\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 0.9751 - accuracy: 0.6057\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.9374 - accuracy: 0.6314\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.9134 - accuracy: 0.6382\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 0.9108 - accuracy: 0.6380\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.8830 - accuracy: 0.6563\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.9084 - accuracy: 0.6378\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.8626 - accuracy: 0.6669\n",
            "21/21 [==============================] - 2s 73ms/step - loss: 1.4948 - accuracy: 0.4338\n",
            "[1.494768738746643, 0.4338095188140869]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c51f8a-2b26-4b70-cd89-75955d375b49"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.3750 - accuracy: 0.3127\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3509 - accuracy: 0.3359\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3158 - accuracy: 0.3669\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.2850 - accuracy: 0.3955\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.2574 - accuracy: 0.4233\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.2279 - accuracy: 0.4406\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.2031 - accuracy: 0.4637\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.1782 - accuracy: 0.4816\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.1542 - accuracy: 0.4955\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.1306 - accuracy: 0.5129\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.1045 - accuracy: 0.5273\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.0764 - accuracy: 0.5392\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.0466 - accuracy: 0.5510\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.0232 - accuracy: 0.5680\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.1100 - accuracy: 0.5276\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.0312 - accuracy: 0.5665\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.9778 - accuracy: 0.5957\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.9437 - accuracy: 0.6104\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.9108 - accuracy: 0.6249\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.8778 - accuracy: 0.6378\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.8700 - accuracy: 0.6447\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.8370 - accuracy: 0.6592\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.8044 - accuracy: 0.6759\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.7845 - accuracy: 0.6845\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.7778 - accuracy: 0.6880\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.7237 - accuracy: 0.7108\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.7029 - accuracy: 0.7182\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.7187 - accuracy: 0.7122\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 0.6684 - accuracy: 0.7371\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6539 - accuracy: 0.7433\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6394 - accuracy: 0.7467\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.6119 - accuracy: 0.7631\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.5947 - accuracy: 0.7690\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.5819 - accuracy: 0.7767\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.5340 - accuracy: 0.7933\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.5868 - accuracy: 0.7808\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 0.5254 - accuracy: 0.8061\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.5513 - accuracy: 0.7922\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 0.5585 - accuracy: 0.7880\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 0.5251 - accuracy: 0.8039\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4719 - accuracy: 0.8288\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 0.4603 - accuracy: 0.8327\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 4s 85ms/step - loss: 0.5159 - accuracy: 0.8080\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 4s 84ms/step - loss: 0.4345 - accuracy: 0.8369\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 83ms/step - loss: 0.4168 - accuracy: 0.8441\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 0.3977 - accuracy: 0.8555\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 4s 83ms/step - loss: 0.3758 - accuracy: 0.8657\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.3930 - accuracy: 0.8565\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.4131 - accuracy: 0.8551\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 0.3712 - accuracy: 0.8682\n",
            "21/21 [==============================] - 1s 71ms/step - loss: 2.1063 - accuracy: 0.5019\n",
            "[2.106285333633423, 0.5019047856330872]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7682ede6-4550-4fad-f90d-9f8925f91213"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.3801 - accuracy: 0.3086\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.3750 - accuracy: 0.3124\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.3679 - accuracy: 0.3190\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.3604 - accuracy: 0.3224\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.3516 - accuracy: 0.3404\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.3284 - accuracy: 0.3639\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.2996 - accuracy: 0.3867\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.2798 - accuracy: 0.4106\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.2490 - accuracy: 0.4302\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.2263 - accuracy: 0.4455\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.2291 - accuracy: 0.4384\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.2277 - accuracy: 0.4427\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.2132 - accuracy: 0.4467\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 5s 110ms/step - loss: 1.2116 - accuracy: 0.4578\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.2294 - accuracy: 0.4365\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.2342 - accuracy: 0.4367\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.1982 - accuracy: 0.4667\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.1898 - accuracy: 0.4690\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.2133 - accuracy: 0.4594\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.1888 - accuracy: 0.4761\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.1610 - accuracy: 0.5022\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.1368 - accuracy: 0.5076\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.1437 - accuracy: 0.5000\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1702 - accuracy: 0.4863\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.2062 - accuracy: 0.4657\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.1778 - accuracy: 0.4716\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.1224 - accuracy: 0.5169\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1236 - accuracy: 0.5155\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1487 - accuracy: 0.4949\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1526 - accuracy: 0.5022\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 5s 109ms/step - loss: 1.1528 - accuracy: 0.5002\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1311 - accuracy: 0.5165\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1604 - accuracy: 0.4984\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.1663 - accuracy: 0.4843\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.1084 - accuracy: 0.5298\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.0561 - accuracy: 0.5584\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.0233 - accuracy: 0.5735\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.0021 - accuracy: 0.5849\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.0311 - accuracy: 0.5708\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.1140 - accuracy: 0.5296\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.0879 - accuracy: 0.5378\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.0976 - accuracy: 0.5329\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.0702 - accuracy: 0.5567\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.0692 - accuracy: 0.5555\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.0183 - accuracy: 0.5833\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 0.9687 - accuracy: 0.6096\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 0.9562 - accuracy: 0.6106\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 0.9380 - accuracy: 0.6218\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 0.9655 - accuracy: 0.6108\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 0.9942 - accuracy: 0.5871\n",
            "21/21 [==============================] - 2s 72ms/step - loss: 1.6965 - accuracy: 0.2895\n",
            "[1.6964794397354126, 0.28952381014823914]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49c6496-887d-441e-9cc7-8842743f9bc0"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3787 - accuracy: 0.3153\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.3760 - accuracy: 0.3171\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3756 - accuracy: 0.3173\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3754 - accuracy: 0.3173\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3752 - accuracy: 0.3173\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3750 - accuracy: 0.3173\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3749 - accuracy: 0.3173\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3749 - accuracy: 0.3173\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3748 - accuracy: 0.3173\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3747 - accuracy: 0.3173\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3746 - accuracy: 0.3173\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3743 - accuracy: 0.3173\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.3634 - accuracy: 0.3371\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.3437 - accuracy: 0.3537\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.3214 - accuracy: 0.3710\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.2975 - accuracy: 0.3812\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.2787 - accuracy: 0.3959\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.2603 - accuracy: 0.4051\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 1.2407 - accuracy: 0.4169\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 1.2185 - accuracy: 0.4357\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.1967 - accuracy: 0.4422\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.1848 - accuracy: 0.4588\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 1.1652 - accuracy: 0.4716\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 1.1279 - accuracy: 0.4886\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.0931 - accuracy: 0.5055\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 1.0767 - accuracy: 0.5131\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 1.0451 - accuracy: 0.5320\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.9859 - accuracy: 0.5745\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 0.9555 - accuracy: 0.5894\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.9325 - accuracy: 0.6039\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.9164 - accuracy: 0.6112\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.9050 - accuracy: 0.6212\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 0.8549 - accuracy: 0.6394\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 0.8855 - accuracy: 0.6290\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 0.8345 - accuracy: 0.6555\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.7872 - accuracy: 0.6771\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 0.7615 - accuracy: 0.6949\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.7753 - accuracy: 0.6871\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 0.7745 - accuracy: 0.6904\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 0.7507 - accuracy: 0.6957\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.7576 - accuracy: 0.6967\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 4s 71ms/step - loss: 0.7413 - accuracy: 0.7012\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 4s 73ms/step - loss: 0.6912 - accuracy: 0.7320\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.6456 - accuracy: 0.7504\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 0.6179 - accuracy: 0.7647\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 0.6102 - accuracy: 0.7686\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 0.5976 - accuracy: 0.7700\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 4s 74ms/step - loss: 0.6210 - accuracy: 0.7567\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.6665 - accuracy: 0.7339\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6338 - accuracy: 0.7567\n",
            "21/21 [==============================] - 1s 69ms/step - loss: 1.6276 - accuracy: 0.4695\n",
            "[1.6275826692581177, 0.4695238173007965]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906839e1-300f-4e52-a0c2-635d393cd215"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 2/49 [>.............................] - ETA: 1s - loss: 1.3880 - accuracy: 0.2350WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0275s vs `on_train_batch_end` time: 0.0647s). Check your callbacks.\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.3725 - accuracy: 0.3167\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.3471 - accuracy: 0.3414\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.3166 - accuracy: 0.3692\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.2885 - accuracy: 0.3971\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.2638 - accuracy: 0.4100\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.2335 - accuracy: 0.4369\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.2103 - accuracy: 0.4541\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.1859 - accuracy: 0.4686\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 1.1612 - accuracy: 0.4896\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.1365 - accuracy: 0.5069\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.1095 - accuracy: 0.5247\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.0847 - accuracy: 0.5347\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.0568 - accuracy: 0.5471\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 1.0335 - accuracy: 0.5633\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.9977 - accuracy: 0.5786\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.9607 - accuracy: 0.6024\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.9408 - accuracy: 0.6088\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.9057 - accuracy: 0.6243\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.8970 - accuracy: 0.6337\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.9064 - accuracy: 0.6378\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.8651 - accuracy: 0.6524\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.8243 - accuracy: 0.6716\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.7964 - accuracy: 0.6820\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.7636 - accuracy: 0.6953\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.7368 - accuracy: 0.7059\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.7244 - accuracy: 0.7149\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.7144 - accuracy: 0.7151\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.6923 - accuracy: 0.7302\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.6656 - accuracy: 0.7382\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.6576 - accuracy: 0.7443\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.6468 - accuracy: 0.7427\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6926 - accuracy: 0.7310\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.6402 - accuracy: 0.7520\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 0.5829 - accuracy: 0.7733\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 4s 83ms/step - loss: 0.5493 - accuracy: 0.7867\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 0.5297 - accuracy: 0.7943\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.5373 - accuracy: 0.7965\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 4s 75ms/step - loss: 0.5187 - accuracy: 0.7953\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.5270 - accuracy: 0.7988\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.5036 - accuracy: 0.8110\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4801 - accuracy: 0.8210\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4591 - accuracy: 0.8278\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4366 - accuracy: 0.8351\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4232 - accuracy: 0.8410\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4114 - accuracy: 0.8478\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.4217 - accuracy: 0.8433\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4217 - accuracy: 0.8392\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.4562 - accuracy: 0.8314\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.4531 - accuracy: 0.8355\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 0.3851 - accuracy: 0.8576\n",
            "21/21 [==============================] - 1s 70ms/step - loss: 1.9851 - accuracy: 0.4910\n",
            "[1.9851311445236206, 0.49095237255096436]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d6960b-d80e-4ef3-9586-1c06e20a92e2"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.3800 - accuracy: 0.3096\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.3749 - accuracy: 0.3104\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.3689 - accuracy: 0.3122\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.3618 - accuracy: 0.3108\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.3639 - accuracy: 0.3337\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.3544 - accuracy: 0.3365\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.3436 - accuracy: 0.3467\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 5s 101ms/step - loss: 1.3298 - accuracy: 0.3651\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.3281 - accuracy: 0.3645\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 5s 102ms/step - loss: 1.3186 - accuracy: 0.3653\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.3139 - accuracy: 0.3767\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 5s 102ms/step - loss: 1.3058 - accuracy: 0.3833\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.3099 - accuracy: 0.3814\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 5s 102ms/step - loss: 1.3013 - accuracy: 0.3900\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.2958 - accuracy: 0.3933\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.2990 - accuracy: 0.3863\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.2961 - accuracy: 0.3896\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.2678 - accuracy: 0.4118\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.2527 - accuracy: 0.4198\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.2313 - accuracy: 0.4392\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.2071 - accuracy: 0.4508\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.1869 - accuracy: 0.4618\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.1716 - accuracy: 0.4767\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1834 - accuracy: 0.4733\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.2001 - accuracy: 0.4618\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.1997 - accuracy: 0.4612\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.1458 - accuracy: 0.4965\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1228 - accuracy: 0.5084\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1547 - accuracy: 0.4861\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1903 - accuracy: 0.4676\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.2264 - accuracy: 0.4502\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.1708 - accuracy: 0.4812\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.2040 - accuracy: 0.4588\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 5s 102ms/step - loss: 1.1969 - accuracy: 0.4659\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1865 - accuracy: 0.4667\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.1807 - accuracy: 0.4665\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1549 - accuracy: 0.4967\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.1684 - accuracy: 0.4794\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1613 - accuracy: 0.4910\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 5s 103ms/step - loss: 1.1621 - accuracy: 0.4861\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.1009 - accuracy: 0.5265\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 5s 102ms/step - loss: 1.0645 - accuracy: 0.5520\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.0747 - accuracy: 0.5369\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 5s 107ms/step - loss: 1.1288 - accuracy: 0.5084\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 5s 102ms/step - loss: 1.1335 - accuracy: 0.5086\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.0749 - accuracy: 0.5308\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 5s 106ms/step - loss: 1.0505 - accuracy: 0.5496\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 5s 108ms/step - loss: 1.0567 - accuracy: 0.5482\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 5s 105ms/step - loss: 1.0666 - accuracy: 0.5392\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 5s 104ms/step - loss: 1.1588 - accuracy: 0.4922\n",
            "21/21 [==============================] - 1s 69ms/step - loss: 1.6105 - accuracy: 0.2848\n",
            "[1.6104556322097778, 0.284761905670166]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd09dbb-fa9e-4a25-c933-8b72187e950c"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_lstm = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 1.3788 - accuracy: 0.3133\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 1.3439 - accuracy: 0.3339\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 1.2244 - accuracy: 0.4263\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 1.1176 - accuracy: 0.4847\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 1.0319 - accuracy: 0.5410\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.9798 - accuracy: 0.5627\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.9357 - accuracy: 0.5782\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.8732 - accuracy: 0.6147\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.8782 - accuracy: 0.6018\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.8289 - accuracy: 0.6288\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.7678 - accuracy: 0.6573\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.7316 - accuracy: 0.6727\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.7109 - accuracy: 0.6820\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.7252 - accuracy: 0.6620\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.7203 - accuracy: 0.6782\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.7105 - accuracy: 0.6890\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.7085 - accuracy: 0.6871\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.6615 - accuracy: 0.7096\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.6525 - accuracy: 0.7137\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.6208 - accuracy: 0.7292\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.6238 - accuracy: 0.7239\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.6564 - accuracy: 0.7278\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.6301 - accuracy: 0.7308\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.6245 - accuracy: 0.7316\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.5582 - accuracy: 0.7712\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.5025 - accuracy: 0.8016\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.4606 - accuracy: 0.8255\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.4416 - accuracy: 0.8386\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.4339 - accuracy: 0.8439\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.4152 - accuracy: 0.8563\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.4779 - accuracy: 0.8278\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.4477 - accuracy: 0.8502\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.4210 - accuracy: 0.8573\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.3780 - accuracy: 0.8790\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.3411 - accuracy: 0.8910\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.4163 - accuracy: 0.8676\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.3259 - accuracy: 0.9004\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.3135 - accuracy: 0.9053\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.3343 - accuracy: 0.8969\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.3453 - accuracy: 0.8986\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.3598 - accuracy: 0.8882\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.3676 - accuracy: 0.8871\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.3042 - accuracy: 0.9078\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.2736 - accuracy: 0.9202\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 0.2945 - accuracy: 0.9116\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.2911 - accuracy: 0.9155\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.2520 - accuracy: 0.9292\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 0.2373 - accuracy: 0.9310\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 0.2281 - accuracy: 0.9331\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 1s 25ms/step - loss: 0.2268 - accuracy: 0.9367\n",
            "21/21 [==============================] - 0s 18ms/step - loss: 1.2309 - accuracy: 0.6771\n",
            "[1.2309032678604126, 0.677142858505249]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0e73b9-0290-4013-d136-a87bf3ef9c54"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 1.3490 - accuracy: 0.3441\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 1.2153 - accuracy: 0.4394\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 1.1289 - accuracy: 0.4949\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 1.0409 - accuracy: 0.5618\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.9486 - accuracy: 0.6112\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.8333 - accuracy: 0.6776\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.7272 - accuracy: 0.7229\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.6389 - accuracy: 0.7590\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.5574 - accuracy: 0.7976\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.4871 - accuracy: 0.8282\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.4328 - accuracy: 0.8492\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.3866 - accuracy: 0.8618\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.3459 - accuracy: 0.8757\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.3405 - accuracy: 0.8778\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.3381 - accuracy: 0.8804\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2732 - accuracy: 0.9067\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2652 - accuracy: 0.9045\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2592 - accuracy: 0.9084\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2110 - accuracy: 0.9276\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1730 - accuracy: 0.9451\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1631 - accuracy: 0.9478\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1491 - accuracy: 0.9547\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1414 - accuracy: 0.9545\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1475 - accuracy: 0.9478\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1651 - accuracy: 0.9376\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1761 - accuracy: 0.9376\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1956 - accuracy: 0.9276\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1647 - accuracy: 0.9414\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1349 - accuracy: 0.9555\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1254 - accuracy: 0.9602\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1252 - accuracy: 0.9582\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.1102 - accuracy: 0.9631\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0899 - accuracy: 0.9655\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0770 - accuracy: 0.9710\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0779 - accuracy: 0.9702\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0716 - accuracy: 0.9716\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0792 - accuracy: 0.9684\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0755 - accuracy: 0.9700\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0708 - accuracy: 0.9724\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0695 - accuracy: 0.9722\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0662 - accuracy: 0.9722\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0661 - accuracy: 0.9720\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0678 - accuracy: 0.9724\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0655 - accuracy: 0.9714\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0671 - accuracy: 0.9735\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0640 - accuracy: 0.9739\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0581 - accuracy: 0.9741\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0556 - accuracy: 0.9757\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0610 - accuracy: 0.9724\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.0545 - accuracy: 0.9755\n",
            "21/21 [==============================] - 0s 21ms/step - loss: 1.4681 - accuracy: 0.6995\n",
            "[1.4680954217910767, 0.6995238065719604]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c466c2-cd91-44a5-abf0-7dce031a9fb0"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3692 - accuracy: 0.3171\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3184 - accuracy: 0.3865\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.2916 - accuracy: 0.4047\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.2911 - accuracy: 0.4086\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3772 - accuracy: 0.2949\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.3772 - accuracy: 0.3102\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3759 - accuracy: 0.3129\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3748 - accuracy: 0.3129\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3737 - accuracy: 0.3131\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3748 - accuracy: 0.3141\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3742 - accuracy: 0.3122\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3738 - accuracy: 0.3122\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3735 - accuracy: 0.3122\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3731 - accuracy: 0.3135\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3726 - accuracy: 0.3135\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3724 - accuracy: 0.3133\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3723 - accuracy: 0.3133\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3721 - accuracy: 0.3133\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3720 - accuracy: 0.3133\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3713 - accuracy: 0.3116\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3697 - accuracy: 0.3127\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3693 - accuracy: 0.3127\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3673 - accuracy: 0.3406\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3671 - accuracy: 0.3353\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3560 - accuracy: 0.3586\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3484 - accuracy: 0.3708\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3436 - accuracy: 0.3706\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3399 - accuracy: 0.3706\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3372 - accuracy: 0.3704\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3355 - accuracy: 0.3700\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3344 - accuracy: 0.3694\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.3328 - accuracy: 0.3700\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3319 - accuracy: 0.3698\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3309 - accuracy: 0.3702\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3304 - accuracy: 0.3700\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3303 - accuracy: 0.3698\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3295 - accuracy: 0.3700\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3293 - accuracy: 0.3698\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3292 - accuracy: 0.3696\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.3287 - accuracy: 0.3698\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3293 - accuracy: 0.3692\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3294 - accuracy: 0.3690\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 1.3300 - accuracy: 0.3684\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 1.3299 - accuracy: 0.3682\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 72ms/step - loss: 1.3296 - accuracy: 0.3686\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3302 - accuracy: 0.3684\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3299 - accuracy: 0.3686\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3300 - accuracy: 0.3686\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 1.3305 - accuracy: 0.3684\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3303 - accuracy: 0.3684\n",
            "21/21 [==============================] - 0s 21ms/step - loss: 1.3310 - accuracy: 0.3710\n",
            "[1.3309882879257202, 0.3709523677825928]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "eb96ad76-0fba-40eb-9483-17c26b5d57c6"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "print(history_sg_bi.history)\r\n",
        "plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()\r\n",
        "plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.4338095188140869, 0.5019047856330872, 0.28952381014823914]\n",
            "cbow [0.4695238173007965, 0.49095237255096436, 0.284761905670166]\n",
            "glove [0.677142858505249, 0.6995238065719604, 0.3709523677825928]\n",
            "{'loss': [1.3749839067459106, 1.3508803844451904, 1.3157767057418823, 1.2849596738815308, 1.2573941946029663, 1.2278952598571777, 1.2031220197677612, 1.1781833171844482, 1.1541719436645508, 1.1306248903274536, 1.1044827699661255, 1.0764265060424805, 1.046570062637329, 1.0231599807739258, 1.1099931001663208, 1.03123939037323, 0.9778082966804504, 0.9436726570129395, 0.9107797741889954, 0.8777619004249573, 0.8700494170188904, 0.8369972705841064, 0.8043699860572815, 0.7844513058662415, 0.7778325080871582, 0.7237404584884644, 0.7029338479042053, 0.7187485694885254, 0.6683573126792908, 0.6539459824562073, 0.6393826007843018, 0.6118651032447815, 0.5946564078330994, 0.5819164514541626, 0.5340105295181274, 0.586824893951416, 0.5254393219947815, 0.5512738227844238, 0.5584926605224609, 0.5251038670539856, 0.47190040349960327, 0.4602905213832855, 0.5159158706665039, 0.43449264764785767, 0.41681694984436035, 0.3977237343788147, 0.3757546842098236, 0.39295366406440735, 0.41310998797416687, 0.37116459012031555], 'accuracy': [0.3126530647277832, 0.3359183669090271, 0.3669387698173523, 0.395510196685791, 0.4232653081417084, 0.440612256526947, 0.46367347240448, 0.48163264989852905, 0.49551019072532654, 0.5128571391105652, 0.5273469090461731, 0.5391836762428284, 0.5510203838348389, 0.567959189414978, 0.527550995349884, 0.5665305852890015, 0.595714271068573, 0.6104081869125366, 0.6248979568481445, 0.6377550959587097, 0.6446938514709473, 0.6591836810112, 0.6759183406829834, 0.6844897866249084, 0.6879591941833496, 0.7108163237571716, 0.718163251876831, 0.7122448682785034, 0.7371428608894348, 0.7432653307914734, 0.7467346787452698, 0.7630612254142761, 0.7689796090126038, 0.7767347097396851, 0.7932652831077576, 0.7808163166046143, 0.8061224222183228, 0.7922449111938477, 0.7879591584205627, 0.8038775324821472, 0.8287755250930786, 0.8326530456542969, 0.8079591989517212, 0.8369387984275818, 0.8440816402435303, 0.8555101752281189, 0.8657143115997314, 0.8565306067466736, 0.8551020622253418, 0.8681632876396179]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}