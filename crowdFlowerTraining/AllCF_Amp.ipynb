{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllCF_Amp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllCF_Amp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUUbZlzjtM4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a645f43c-1d3a-43e3-b534-1bad78354f8d"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from gensim.models import Word2Vec\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from google.colab import  drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('all')\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "print(device_name)\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/InfluenceAnalysis/crowdFlower/crowdFlower_clean.csv')\r\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Z9hWlpvu-l"
      },
      "source": [
        "category_dict = {0:[1,0,0,0,0,0,0,0,0,0,0,0,0],1:[0,1,0,0,0,0,0,0,0,0,0,0,0],2:[0,0,1,0,0,0,0,0,0,0,0,0,0],3:[0,0,0,1,0,0,0,0,0,0,0,0,0],4:[0,0,0,0,1,0,0,0,0,0,0,0,0],5:[0,0,0,0,0,1,0,0,0,0,0,0,0],6:[0,0,0,0,0,0,1,0,0,0,0,0,0],7:[0,0,0,0,0,0,0,1,0,0,0,0,0],8:[0,0,0,0,0,0,0,0,1,0,0,0,0],9:[0,0,0,0,0,0,0,0,0,1,0,0,0],10:[0,0,0,0,0,0,0,0,0,0,1,0,0],11:[0,0,0,0,0,0,0,0,0,0,0,1,0],12:[0,0,0,0,0,0,0,0,0,0,0,0,1]}\r\n",
        "data_tweet = [x.lower().split() for x in df['tweets']]\r\n",
        "data_cat = np.array([category_dict[x] for x in df['emotion']])\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ik2nYnWvu7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fdccb2-4195-40ba-f5b9-587de29d33c7"
      },
      "source": [
        "print(data_tweet[:5])\r\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['katofawesome', 'praying', 'for', 'love', 'in', 'a', 'lap', 'dance', 'and', 'paying', 'in', 'naivety', 'lt--one', 'of', 'my', 'fav', 'songs'], ['trying', 'to', 'fix', 'my', 'internet', 'connectionguess', 'my', 'prayers', 'have', 'been', 'answered', 'and', 'i', 'wont', 'have', 'any', 'study', 'distractions', 'ugh'], ['starting', 'an', 'account', 'here', 'on', 'twitter'], ['dipfico', 'hmm', 'wrong', 'link', 'ignore', 'my', 'tweet'], ['mzunyque', 'thanks', 'before', 'the', 'major', 'chop']]\n",
            "[[0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTQD20vNvu3x"
      },
      "source": [
        "# Parameters\r\n",
        "Min_count = 0\r\n",
        "Embedding_size = 200\r\n",
        "Window_size = 5\r\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmvFJUlXvu1S"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\r\n",
        "                     window=Window_size,\r\n",
        "                     size=Embedding_size,\r\n",
        "                     negative=Negative_sampling,sg=1)\r\n",
        "w2v_sg.build_vocab(data_tweet)\r\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\r\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\r\n",
        "                     window=Window_size,\r\n",
        "                     size=Embedding_size,\r\n",
        "                     negative=Negative_sampling,sg=0)\r\n",
        "w2v_cbow.build_vocab(data_tweet)\r\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\r\n",
        "w2v_sg.wv.init_sims(True)\r\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWlSeP2Xvuy7"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\r\n",
        "vocab_sg = [x for x in vocab_sg]\r\n",
        "vocab_cbow = w2v_cbow.wv.vocab\r\n",
        "vocab_cbow = [x for x in vocab_cbow] \r\n",
        "vocab_glove = {}\r\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\r\n",
        "  for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      vector = np.asarray(values[1:], \"float32\")\r\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2dmTA7mv2rW"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "data_tweet = [nltk.pos_tag(x) for x in data_tweet]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMXBVvGUv9q3"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='RB':\r\n",
        "            tweet[i]=((tweet[i][0])*mul_factor,tweet[i][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='RB':\r\n",
        "            tweet[i]=((tweet[i][0])*mul_factor,tweet[i][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpLVu-Jv9oA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L46j8Lgv9lS"
      },
      "source": [
        "epochs = 40"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRdVKc6av2oZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdae0ed8-b1a4-433c-84fb-71d70a691997"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\r\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\r\n",
        "model_sg.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[0]=history[1]\r\n",
        "print(\"SG_LSTM\")\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 67s 254ms/step - loss: 2.2616 - accuracy: 0.2081 - val_loss: 2.1251 - val_accuracy: 0.2702\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 2.1220 - accuracy: 0.2565 - val_loss: 2.0738 - val_accuracy: 0.2651\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 2.0763 - accuracy: 0.2655 - val_loss: 2.0544 - val_accuracy: 0.2780\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 2.0524 - accuracy: 0.2784 - val_loss: 2.0442 - val_accuracy: 0.2855\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 65s 254ms/step - loss: 2.0342 - accuracy: 0.2928 - val_loss: 2.0349 - val_accuracy: 0.2903\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 2.0205 - accuracy: 0.2994 - val_loss: 2.0195 - val_accuracy: 0.3018\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 66s 256ms/step - loss: 2.0012 - accuracy: 0.3109 - val_loss: 2.0111 - val_accuracy: 0.3055\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.9879 - accuracy: 0.3188 - val_loss: 2.0019 - val_accuracy: 0.3115\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.9736 - accuracy: 0.3266 - val_loss: 1.9967 - val_accuracy: 0.3112\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.9584 - accuracy: 0.3332 - val_loss: 1.9918 - val_accuracy: 0.3112\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.9450 - accuracy: 0.3407 - val_loss: 1.9909 - val_accuracy: 0.3118\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.9344 - accuracy: 0.3434 - val_loss: 1.9896 - val_accuracy: 0.3135\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.9182 - accuracy: 0.3493 - val_loss: 1.9852 - val_accuracy: 0.3149\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.9035 - accuracy: 0.3562 - val_loss: 1.9845 - val_accuracy: 0.3175\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.8940 - accuracy: 0.3586 - val_loss: 1.9798 - val_accuracy: 0.3195\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.8790 - accuracy: 0.3654 - val_loss: 1.9866 - val_accuracy: 0.3174\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.8691 - accuracy: 0.3697 - val_loss: 1.9897 - val_accuracy: 0.3163\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 66s 257ms/step - loss: 1.8548 - accuracy: 0.3744 - val_loss: 1.9918 - val_accuracy: 0.3132\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.8438 - accuracy: 0.3766 - val_loss: 1.9980 - val_accuracy: 0.3126\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.8397 - accuracy: 0.3779 - val_loss: 1.9934 - val_accuracy: 0.3152\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.8282 - accuracy: 0.3813 - val_loss: 1.9844 - val_accuracy: 0.3195\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.8195 - accuracy: 0.3866 - val_loss: 1.9942 - val_accuracy: 0.3214\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.8053 - accuracy: 0.3928 - val_loss: 1.9882 - val_accuracy: 0.3211\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 65s 252ms/step - loss: 1.7976 - accuracy: 0.3945 - val_loss: 1.9940 - val_accuracy: 0.3214\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 65s 254ms/step - loss: 1.7829 - accuracy: 0.4052 - val_loss: 2.0011 - val_accuracy: 0.3226\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.7728 - accuracy: 0.4085 - val_loss: 2.0028 - val_accuracy: 0.3237\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.7734 - accuracy: 0.4113 - val_loss: 2.0018 - val_accuracy: 0.3185\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.7622 - accuracy: 0.4145 - val_loss: 2.0060 - val_accuracy: 0.3198\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.7625 - accuracy: 0.4132 - val_loss: 2.0148 - val_accuracy: 0.3237\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.7490 - accuracy: 0.4170 - val_loss: 2.0280 - val_accuracy: 0.3198\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.7390 - accuracy: 0.4198 - val_loss: 2.0323 - val_accuracy: 0.3225\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.7282 - accuracy: 0.4275 - val_loss: 2.0294 - val_accuracy: 0.3272\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.7228 - accuracy: 0.4274 - val_loss: 2.0387 - val_accuracy: 0.3235\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.7040 - accuracy: 0.4359 - val_loss: 2.0496 - val_accuracy: 0.3243\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.6959 - accuracy: 0.4397 - val_loss: 2.0818 - val_accuracy: 0.3131\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.6841 - accuracy: 0.4446 - val_loss: 2.0890 - val_accuracy: 0.3166\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.6724 - accuracy: 0.4492 - val_loss: 2.0879 - val_accuracy: 0.3172\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.6640 - accuracy: 0.4523 - val_loss: 2.0925 - val_accuracy: 0.3172\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 66s 257ms/step - loss: 1.6600 - accuracy: 0.4532 - val_loss: 2.1151 - val_accuracy: 0.3157\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.6593 - accuracy: 0.4527 - val_loss: 2.1028 - val_accuracy: 0.3182\n",
            "80/80 [==============================] - 15s 185ms/step - loss: 2.1498 - accuracy: 0.3135\n",
            "[2.1497995853424072, 0.31349998712539673]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg66ju0qwE_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62bdaa6-eb96-4540-9fb2-e6d3f7cf1631"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 78s 292ms/step - loss: 2.2400 - accuracy: 0.2269 - val_loss: 2.0609 - val_accuracy: 0.2788\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 2.0581 - accuracy: 0.2855 - val_loss: 2.0290 - val_accuracy: 0.2992\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 2.0202 - accuracy: 0.3027 - val_loss: 2.0133 - val_accuracy: 0.3105\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 78s 303ms/step - loss: 1.9978 - accuracy: 0.3153 - val_loss: 2.0014 - val_accuracy: 0.3149\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 79s 309ms/step - loss: 1.9762 - accuracy: 0.3251 - val_loss: 1.9939 - val_accuracy: 0.3168\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 78s 302ms/step - loss: 1.9562 - accuracy: 0.3302 - val_loss: 1.9880 - val_accuracy: 0.3168\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 80s 314ms/step - loss: 1.9369 - accuracy: 0.3367 - val_loss: 1.9857 - val_accuracy: 0.3143\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 80s 312ms/step - loss: 1.9156 - accuracy: 0.3474 - val_loss: 1.9906 - val_accuracy: 0.3140\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 80s 310ms/step - loss: 1.8999 - accuracy: 0.3530 - val_loss: 1.9886 - val_accuracy: 0.3180\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 80s 311ms/step - loss: 1.8772 - accuracy: 0.3595 - val_loss: 1.9878 - val_accuracy: 0.3178\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 80s 312ms/step - loss: 1.8559 - accuracy: 0.3655 - val_loss: 1.9945 - val_accuracy: 0.3160\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 80s 310ms/step - loss: 1.8371 - accuracy: 0.3745 - val_loss: 2.0033 - val_accuracy: 0.3157\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 80s 311ms/step - loss: 1.8159 - accuracy: 0.3832 - val_loss: 2.0092 - val_accuracy: 0.3145\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 80s 313ms/step - loss: 1.7965 - accuracy: 0.3894 - val_loss: 2.0212 - val_accuracy: 0.3163\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 79s 308ms/step - loss: 1.7728 - accuracy: 0.3990 - val_loss: 2.0319 - val_accuracy: 0.3117\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 77s 301ms/step - loss: 1.7498 - accuracy: 0.4054 - val_loss: 2.0447 - val_accuracy: 0.3128\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.7313 - accuracy: 0.4152 - val_loss: 2.0562 - val_accuracy: 0.3098\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 75s 294ms/step - loss: 1.7117 - accuracy: 0.4210 - val_loss: 2.0690 - val_accuracy: 0.3105\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.6910 - accuracy: 0.4274 - val_loss: 2.0754 - val_accuracy: 0.3094\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 75s 293ms/step - loss: 1.6682 - accuracy: 0.4347 - val_loss: 2.1033 - val_accuracy: 0.3083\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 73s 283ms/step - loss: 1.6492 - accuracy: 0.4439 - val_loss: 2.1141 - val_accuracy: 0.3057\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 73s 284ms/step - loss: 1.6309 - accuracy: 0.4511 - val_loss: 2.1417 - val_accuracy: 0.3068\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 73s 284ms/step - loss: 1.6238 - accuracy: 0.4537 - val_loss: 2.1585 - val_accuracy: 0.3017\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.5913 - accuracy: 0.4628 - val_loss: 2.1554 - val_accuracy: 0.3025\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 73s 286ms/step - loss: 1.5767 - accuracy: 0.4725 - val_loss: 2.1743 - val_accuracy: 0.2988\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.5554 - accuracy: 0.4791 - val_loss: 2.1992 - val_accuracy: 0.3051\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.5382 - accuracy: 0.4857 - val_loss: 2.2320 - val_accuracy: 0.2962\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.5192 - accuracy: 0.4902 - val_loss: 2.2377 - val_accuracy: 0.2980\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 76s 296ms/step - loss: 1.4943 - accuracy: 0.5014 - val_loss: 2.2300 - val_accuracy: 0.2954\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 76s 295ms/step - loss: 1.4724 - accuracy: 0.5086 - val_loss: 2.2434 - val_accuracy: 0.2946\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.4648 - accuracy: 0.5149 - val_loss: 2.2731 - val_accuracy: 0.2935\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 73s 286ms/step - loss: 1.4562 - accuracy: 0.5151 - val_loss: 2.3082 - val_accuracy: 0.2931\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 73s 285ms/step - loss: 1.4479 - accuracy: 0.5159 - val_loss: 2.3411 - val_accuracy: 0.2923\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 80s 311ms/step - loss: 1.4283 - accuracy: 0.5239 - val_loss: 2.3568 - val_accuracy: 0.2900\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 80s 314ms/step - loss: 1.4031 - accuracy: 0.5351 - val_loss: 2.3913 - val_accuracy: 0.2852\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 79s 310ms/step - loss: 1.3942 - accuracy: 0.5382 - val_loss: 2.4262 - val_accuracy: 0.2835\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 80s 310ms/step - loss: 1.3747 - accuracy: 0.5430 - val_loss: 2.3879 - val_accuracy: 0.2903\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 79s 309ms/step - loss: 1.3818 - accuracy: 0.5411 - val_loss: 2.4316 - val_accuracy: 0.2888\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 80s 312ms/step - loss: 1.3556 - accuracy: 0.5513 - val_loss: 2.4753 - val_accuracy: 0.2840\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 80s 312ms/step - loss: 1.3447 - accuracy: 0.5533 - val_loss: 2.5109 - val_accuracy: 0.2848\n",
            "80/80 [==============================] - 17s 216ms/step - loss: 2.5724 - accuracy: 0.2756\n",
            "[2.572416067123413, 0.27562499046325684]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMK-RBywE7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606813e2-447f-4d70-9ab8-7a42db915f73"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 64s 247ms/step - loss: 2.2526 - accuracy: 0.1851 - val_loss: 2.1396 - val_accuracy: 0.2157\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 62s 241ms/step - loss: 2.1423 - accuracy: 0.2149 - val_loss: 2.1367 - val_accuracy: 0.2154\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 61s 238ms/step - loss: 2.1433 - accuracy: 0.2167 - val_loss: 2.1379 - val_accuracy: 0.2158\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 61s 239ms/step - loss: 2.1440 - accuracy: 0.2165 - val_loss: 2.1396 - val_accuracy: 0.2152\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 63s 245ms/step - loss: 2.1408 - accuracy: 0.2157 - val_loss: 2.1383 - val_accuracy: 0.2157\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 62s 242ms/step - loss: 2.1405 - accuracy: 0.2165 - val_loss: 2.1380 - val_accuracy: 0.2143\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 2.1374 - accuracy: 0.2211 - val_loss: 2.1384 - val_accuracy: 0.2146\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 60s 233ms/step - loss: 2.1344 - accuracy: 0.2253 - val_loss: 2.1373 - val_accuracy: 0.2134\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 58s 227ms/step - loss: 2.1357 - accuracy: 0.2237 - val_loss: 2.1386 - val_accuracy: 0.2128\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 60s 232ms/step - loss: 2.1379 - accuracy: 0.2195 - val_loss: 2.1409 - val_accuracy: 0.2068\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 59s 229ms/step - loss: 2.1338 - accuracy: 0.2241 - val_loss: 2.1479 - val_accuracy: 0.2108\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 59s 230ms/step - loss: 2.1258 - accuracy: 0.2297 - val_loss: 2.1538 - val_accuracy: 0.2035\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 59s 230ms/step - loss: 2.1207 - accuracy: 0.2336 - val_loss: 2.1698 - val_accuracy: 0.2011\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 59s 229ms/step - loss: 2.1176 - accuracy: 0.2351 - val_loss: 2.1869 - val_accuracy: 0.1957\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 58s 228ms/step - loss: 2.1070 - accuracy: 0.2396 - val_loss: 2.2130 - val_accuracy: 0.1931\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 56s 220ms/step - loss: 2.1015 - accuracy: 0.2407 - val_loss: 2.2122 - val_accuracy: 0.1985\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 57s 221ms/step - loss: 2.0982 - accuracy: 0.2449 - val_loss: 2.1893 - val_accuracy: 0.2066\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 2.0969 - accuracy: 0.2468 - val_loss: 2.1627 - val_accuracy: 0.2157\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0955 - accuracy: 0.2507 - val_loss: 2.1576 - val_accuracy: 0.2197\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 57s 222ms/step - loss: 2.0872 - accuracy: 0.2558 - val_loss: 2.1795 - val_accuracy: 0.2129\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 56s 220ms/step - loss: 2.0791 - accuracy: 0.2632 - val_loss: 2.1716 - val_accuracy: 0.2232\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 56s 218ms/step - loss: 2.0739 - accuracy: 0.2688 - val_loss: 2.1708 - val_accuracy: 0.2282\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 2.0662 - accuracy: 0.2703 - val_loss: 2.1830 - val_accuracy: 0.2183\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 2.0607 - accuracy: 0.2771 - val_loss: 2.1933 - val_accuracy: 0.2137\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 2.0486 - accuracy: 0.2842 - val_loss: 2.2173 - val_accuracy: 0.2032\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 57s 222ms/step - loss: 2.0440 - accuracy: 0.2862 - val_loss: 2.2269 - val_accuracy: 0.2015\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 59s 231ms/step - loss: 2.0404 - accuracy: 0.2890 - val_loss: 2.2272 - val_accuracy: 0.1998\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.0358 - accuracy: 0.2922 - val_loss: 2.2186 - val_accuracy: 0.2034\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.0322 - accuracy: 0.2951 - val_loss: 2.2115 - val_accuracy: 0.2075\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.0306 - accuracy: 0.2904 - val_loss: 2.2072 - val_accuracy: 0.2155\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.0311 - accuracy: 0.2923 - val_loss: 2.2117 - val_accuracy: 0.2203\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0281 - accuracy: 0.2991 - val_loss: 2.2133 - val_accuracy: 0.2168\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0275 - accuracy: 0.3005 - val_loss: 2.2256 - val_accuracy: 0.2177\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 56s 220ms/step - loss: 2.0292 - accuracy: 0.2976 - val_loss: 2.2256 - val_accuracy: 0.2177\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 2.0315 - accuracy: 0.2941 - val_loss: 2.2105 - val_accuracy: 0.2291\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 56s 220ms/step - loss: 2.0112 - accuracy: 0.3024 - val_loss: 2.2095 - val_accuracy: 0.2255\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 57s 222ms/step - loss: 1.9983 - accuracy: 0.3055 - val_loss: 2.2188 - val_accuracy: 0.2194\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 1.9958 - accuracy: 0.3084 - val_loss: 2.2226 - val_accuracy: 0.2206\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 56s 219ms/step - loss: 1.9938 - accuracy: 0.3100 - val_loss: 2.2320 - val_accuracy: 0.2206\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 56s 220ms/step - loss: 1.9955 - accuracy: 0.3160 - val_loss: 2.2264 - val_accuracy: 0.2260\n",
            "80/80 [==============================] - 14s 173ms/step - loss: 2.2599 - accuracy: 0.2180\n",
            "[2.259915351867676, 0.21799999475479126]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UrOK83UwE4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5ddcf3-36b7-43bf-899a-443c51cd3718"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 66s 252ms/step - loss: 2.2525 - accuracy: 0.2127 - val_loss: 2.1392 - val_accuracy: 0.2149\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 2.1467 - accuracy: 0.2147 - val_loss: 2.1363 - val_accuracy: 0.2158\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 65s 252ms/step - loss: 2.1443 - accuracy: 0.2215 - val_loss: 2.1002 - val_accuracy: 0.2588\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 2.0978 - accuracy: 0.2539 - val_loss: 2.0572 - val_accuracy: 0.2826\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 2.0547 - accuracy: 0.2763 - val_loss: 2.0430 - val_accuracy: 0.2912\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 2.0337 - accuracy: 0.2913 - val_loss: 2.0282 - val_accuracy: 0.3034\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 2.0115 - accuracy: 0.3063 - val_loss: 2.0222 - val_accuracy: 0.3046\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.9976 - accuracy: 0.3141 - val_loss: 2.0130 - val_accuracy: 0.3077\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.9834 - accuracy: 0.3188 - val_loss: 2.0082 - val_accuracy: 0.3055\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.9680 - accuracy: 0.3259 - val_loss: 2.0010 - val_accuracy: 0.3112\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.9541 - accuracy: 0.3340 - val_loss: 1.9974 - val_accuracy: 0.3155\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.9443 - accuracy: 0.3373 - val_loss: 1.9941 - val_accuracy: 0.3145\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.9294 - accuracy: 0.3429 - val_loss: 1.9928 - val_accuracy: 0.3168\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.9162 - accuracy: 0.3469 - val_loss: 1.9945 - val_accuracy: 0.3105\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.9093 - accuracy: 0.3527 - val_loss: 1.9973 - val_accuracy: 0.3134\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.8955 - accuracy: 0.3577 - val_loss: 2.0005 - val_accuracy: 0.3114\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.8834 - accuracy: 0.3623 - val_loss: 2.0054 - val_accuracy: 0.3100\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.8715 - accuracy: 0.3676 - val_loss: 2.0041 - val_accuracy: 0.3145\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.8557 - accuracy: 0.3715 - val_loss: 2.0125 - val_accuracy: 0.3125\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.8462 - accuracy: 0.3775 - val_loss: 2.0122 - val_accuracy: 0.3132\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.8360 - accuracy: 0.3811 - val_loss: 2.0083 - val_accuracy: 0.3197\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.8201 - accuracy: 0.3912 - val_loss: 2.0122 - val_accuracy: 0.3160\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.8088 - accuracy: 0.3955 - val_loss: 2.0214 - val_accuracy: 0.3117\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.7938 - accuracy: 0.4041 - val_loss: 2.0308 - val_accuracy: 0.3138\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.7811 - accuracy: 0.4087 - val_loss: 2.0329 - val_accuracy: 0.3140\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.7771 - accuracy: 0.4081 - val_loss: 2.0377 - val_accuracy: 0.3138\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 64s 251ms/step - loss: 1.7648 - accuracy: 0.4142 - val_loss: 2.0343 - val_accuracy: 0.3122\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.7548 - accuracy: 0.4181 - val_loss: 2.0460 - val_accuracy: 0.3100\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.7505 - accuracy: 0.4207 - val_loss: 2.0533 - val_accuracy: 0.3106\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 63s 246ms/step - loss: 1.7417 - accuracy: 0.4214 - val_loss: 2.0636 - val_accuracy: 0.3045\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.7348 - accuracy: 0.4237 - val_loss: 2.0657 - val_accuracy: 0.3063\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.7203 - accuracy: 0.4309 - val_loss: 2.0863 - val_accuracy: 0.3017\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 64s 249ms/step - loss: 1.7165 - accuracy: 0.4299 - val_loss: 2.1039 - val_accuracy: 0.2991\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.7104 - accuracy: 0.4335 - val_loss: 2.0979 - val_accuracy: 0.3071\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 64s 248ms/step - loss: 1.6927 - accuracy: 0.4425 - val_loss: 2.1041 - val_accuracy: 0.3035\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 65s 252ms/step - loss: 1.6837 - accuracy: 0.4448 - val_loss: 2.1134 - val_accuracy: 0.3057\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.6750 - accuracy: 0.4486 - val_loss: 2.1261 - val_accuracy: 0.3049\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 64s 250ms/step - loss: 1.6719 - accuracy: 0.4505 - val_loss: 2.1220 - val_accuracy: 0.3042\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 63s 247ms/step - loss: 1.6692 - accuracy: 0.4507 - val_loss: 2.1158 - val_accuracy: 0.3074\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 63s 248ms/step - loss: 1.6630 - accuracy: 0.4504 - val_loss: 2.1349 - val_accuracy: 0.3069\n",
            "80/80 [==============================] - 14s 180ms/step - loss: 2.1580 - accuracy: 0.3024\n",
            "[2.1580004692077637, 0.3023749887943268]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqyDJQt3v2mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f27722-2143-44c7-8e33-d6784de58fa7"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 77s 289ms/step - loss: 2.2381 - accuracy: 0.2219 - val_loss: 2.0626 - val_accuracy: 0.2782\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 73s 285ms/step - loss: 2.0592 - accuracy: 0.2833 - val_loss: 2.0271 - val_accuracy: 0.3028\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 74s 288ms/step - loss: 2.0186 - accuracy: 0.3017 - val_loss: 2.0124 - val_accuracy: 0.3100\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.9971 - accuracy: 0.3126 - val_loss: 2.0050 - val_accuracy: 0.3111\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 75s 291ms/step - loss: 1.9749 - accuracy: 0.3232 - val_loss: 2.0009 - val_accuracy: 0.3111\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 76s 295ms/step - loss: 1.9538 - accuracy: 0.3292 - val_loss: 1.9971 - val_accuracy: 0.3145\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 76s 296ms/step - loss: 1.9345 - accuracy: 0.3350 - val_loss: 1.9946 - val_accuracy: 0.3166\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.9139 - accuracy: 0.3440 - val_loss: 1.9980 - val_accuracy: 0.3137\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 75s 294ms/step - loss: 1.8972 - accuracy: 0.3505 - val_loss: 2.0033 - val_accuracy: 0.3146\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 75s 291ms/step - loss: 1.8747 - accuracy: 0.3555 - val_loss: 1.9994 - val_accuracy: 0.3180\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.8512 - accuracy: 0.3647 - val_loss: 2.0081 - val_accuracy: 0.3126\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 73s 285ms/step - loss: 1.8305 - accuracy: 0.3710 - val_loss: 2.0245 - val_accuracy: 0.3123\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 73s 283ms/step - loss: 1.8096 - accuracy: 0.3821 - val_loss: 2.0290 - val_accuracy: 0.3115\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.7927 - accuracy: 0.3887 - val_loss: 2.0365 - val_accuracy: 0.3109\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 73s 286ms/step - loss: 1.7691 - accuracy: 0.3988 - val_loss: 2.0677 - val_accuracy: 0.3023\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.7538 - accuracy: 0.4035 - val_loss: 2.0656 - val_accuracy: 0.3040\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.7259 - accuracy: 0.4150 - val_loss: 2.0827 - val_accuracy: 0.3026\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.7112 - accuracy: 0.4205 - val_loss: 2.0861 - val_accuracy: 0.3031\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 75s 293ms/step - loss: 1.6956 - accuracy: 0.4253 - val_loss: 2.1121 - val_accuracy: 0.2974\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 74s 288ms/step - loss: 1.6734 - accuracy: 0.4365 - val_loss: 2.1483 - val_accuracy: 0.2949\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.6620 - accuracy: 0.4374 - val_loss: 2.1541 - val_accuracy: 0.2948\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 75s 294ms/step - loss: 1.6381 - accuracy: 0.4459 - val_loss: 2.1627 - val_accuracy: 0.2978\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.6225 - accuracy: 0.4521 - val_loss: 2.1771 - val_accuracy: 0.2955\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.5999 - accuracy: 0.4603 - val_loss: 2.2096 - val_accuracy: 0.2914\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.5814 - accuracy: 0.4685 - val_loss: 2.2306 - val_accuracy: 0.2877\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 74s 288ms/step - loss: 1.5656 - accuracy: 0.4714 - val_loss: 2.2381 - val_accuracy: 0.2889\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 73s 285ms/step - loss: 1.5462 - accuracy: 0.4805 - val_loss: 2.3097 - val_accuracy: 0.2772\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.5280 - accuracy: 0.4886 - val_loss: 2.2907 - val_accuracy: 0.2892\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 74s 288ms/step - loss: 1.5031 - accuracy: 0.4958 - val_loss: 2.3241 - val_accuracy: 0.2760\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.4902 - accuracy: 0.4995 - val_loss: 2.3360 - val_accuracy: 0.2762\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.4792 - accuracy: 0.5069 - val_loss: 2.3751 - val_accuracy: 0.2746\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.4637 - accuracy: 0.5125 - val_loss: 2.3484 - val_accuracy: 0.2840\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 74s 288ms/step - loss: 1.4495 - accuracy: 0.5176 - val_loss: 2.3970 - val_accuracy: 0.2735\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 74s 287ms/step - loss: 1.4441 - accuracy: 0.5191 - val_loss: 2.4007 - val_accuracy: 0.2758\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.4207 - accuracy: 0.5288 - val_loss: 2.4247 - val_accuracy: 0.2732\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 74s 289ms/step - loss: 1.3963 - accuracy: 0.5366 - val_loss: 2.4614 - val_accuracy: 0.2765\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.3814 - accuracy: 0.5409 - val_loss: 2.4767 - val_accuracy: 0.2755\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 75s 291ms/step - loss: 1.3783 - accuracy: 0.5425 - val_loss: 2.4883 - val_accuracy: 0.2786\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 75s 292ms/step - loss: 1.3598 - accuracy: 0.5475 - val_loss: 2.5292 - val_accuracy: 0.2734\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 74s 290ms/step - loss: 1.3522 - accuracy: 0.5539 - val_loss: 2.5303 - val_accuracy: 0.2692\n",
            "80/80 [==============================] - 16s 199ms/step - loss: 2.5360 - accuracy: 0.2706\n",
            "[2.5360324382781982, 0.2706249952316284]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNRddGTHv2jR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7d9ab4-6ac2-4c54-da2b-a39398c7f907"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 2.2220 - accuracy: 0.2075 - val_loss: 2.1389 - val_accuracy: 0.2154\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.1422 - accuracy: 0.2147 - val_loss: 2.1344 - val_accuracy: 0.2154\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 58s 226ms/step - loss: 2.1422 - accuracy: 0.2167 - val_loss: 2.1358 - val_accuracy: 0.2148\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.1402 - accuracy: 0.2210 - val_loss: 2.1454 - val_accuracy: 0.2258\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.1319 - accuracy: 0.2287 - val_loss: 2.1481 - val_accuracy: 0.2218\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 2.1277 - accuracy: 0.2365 - val_loss: 2.1549 - val_accuracy: 0.2218\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 58s 226ms/step - loss: 2.1235 - accuracy: 0.2430 - val_loss: 2.1547 - val_accuracy: 0.2245\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.1181 - accuracy: 0.2488 - val_loss: 2.1495 - val_accuracy: 0.2234\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.1064 - accuracy: 0.2539 - val_loss: 2.1479 - val_accuracy: 0.2235\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0967 - accuracy: 0.2596 - val_loss: 2.1471 - val_accuracy: 0.2277\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0904 - accuracy: 0.2620 - val_loss: 2.1579 - val_accuracy: 0.2240\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0783 - accuracy: 0.2707 - val_loss: 2.1693 - val_accuracy: 0.2206\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.0679 - accuracy: 0.2769 - val_loss: 2.1690 - val_accuracy: 0.2155\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 2.0587 - accuracy: 0.2812 - val_loss: 2.1764 - val_accuracy: 0.2158\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 2.0510 - accuracy: 0.2837 - val_loss: 2.1646 - val_accuracy: 0.2178\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 57s 222ms/step - loss: 2.0489 - accuracy: 0.2829 - val_loss: 2.1702 - val_accuracy: 0.2195\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 58s 226ms/step - loss: 2.0295 - accuracy: 0.2973 - val_loss: 2.1867 - val_accuracy: 0.2178\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 58s 226ms/step - loss: 2.0205 - accuracy: 0.2975 - val_loss: 2.1867 - val_accuracy: 0.2211\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 58s 227ms/step - loss: 2.0150 - accuracy: 0.3029 - val_loss: 2.1886 - val_accuracy: 0.2218\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 58s 226ms/step - loss: 2.0201 - accuracy: 0.2954 - val_loss: 2.1829 - val_accuracy: 0.2266\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 2.0296 - accuracy: 0.2871 - val_loss: 2.1975 - val_accuracy: 0.2209\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 2.0258 - accuracy: 0.2930 - val_loss: 2.1997 - val_accuracy: 0.2229\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 2.0240 - accuracy: 0.2909 - val_loss: 2.2172 - val_accuracy: 0.2162\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 57s 222ms/step - loss: 2.0189 - accuracy: 0.2920 - val_loss: 2.2202 - val_accuracy: 0.2183\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 57s 221ms/step - loss: 2.0037 - accuracy: 0.3000 - val_loss: 2.2190 - val_accuracy: 0.2255\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 58s 226ms/step - loss: 1.9992 - accuracy: 0.3021 - val_loss: 2.2207 - val_accuracy: 0.2183\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 58s 224ms/step - loss: 1.9960 - accuracy: 0.3026 - val_loss: 2.2177 - val_accuracy: 0.2183\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 1.9931 - accuracy: 0.3053 - val_loss: 2.2230 - val_accuracy: 0.2211\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 1.9822 - accuracy: 0.3098 - val_loss: 2.2382 - val_accuracy: 0.2197\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 1.9873 - accuracy: 0.3056 - val_loss: 2.2420 - val_accuracy: 0.2188\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 1.9767 - accuracy: 0.3154 - val_loss: 2.2294 - val_accuracy: 0.2248\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 1.9770 - accuracy: 0.3154 - val_loss: 2.2362 - val_accuracy: 0.2243\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 1.9699 - accuracy: 0.3171 - val_loss: 2.2413 - val_accuracy: 0.2231\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 1.9572 - accuracy: 0.3223 - val_loss: 2.2518 - val_accuracy: 0.2237\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 57s 224ms/step - loss: 1.9587 - accuracy: 0.3246 - val_loss: 2.2579 - val_accuracy: 0.2143\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 57s 223ms/step - loss: 1.9521 - accuracy: 0.3286 - val_loss: 2.2791 - val_accuracy: 0.2185\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 1.9600 - accuracy: 0.3301 - val_loss: 2.2691 - val_accuracy: 0.2145\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 58s 227ms/step - loss: 1.9800 - accuracy: 0.3233 - val_loss: 2.2647 - val_accuracy: 0.2108\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 1.9663 - accuracy: 0.3309 - val_loss: 2.2861 - val_accuracy: 0.2097\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 58s 225ms/step - loss: 1.9580 - accuracy: 0.3325 - val_loss: 2.2881 - val_accuracy: 0.2071\n",
            "80/80 [==============================] - 14s 177ms/step - loss: 2.3037 - accuracy: 0.2021\n",
            "[2.3037478923797607, 0.20212499797344208]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDaMTRZ7v2g7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2beb5d56-9bad-44a9-f273-5afdadb140cb"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 15s 54ms/step - loss: 2.2439 - accuracy: 0.2289 - val_loss: 1.9783 - val_accuracy: 0.3105\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.9598 - accuracy: 0.3207 - val_loss: 1.9030 - val_accuracy: 0.3523\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.8909 - accuracy: 0.3536 - val_loss: 1.8785 - val_accuracy: 0.3578\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.8522 - accuracy: 0.3720 - val_loss: 1.8705 - val_accuracy: 0.3578\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.8247 - accuracy: 0.3789 - val_loss: 1.8666 - val_accuracy: 0.3608\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.8012 - accuracy: 0.3901 - val_loss: 1.8597 - val_accuracy: 0.3628\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.7701 - accuracy: 0.4017 - val_loss: 1.8677 - val_accuracy: 0.3632\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.7447 - accuracy: 0.4137 - val_loss: 1.8736 - val_accuracy: 0.3615\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.7211 - accuracy: 0.4207 - val_loss: 1.8837 - val_accuracy: 0.3608\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.6976 - accuracy: 0.4273 - val_loss: 1.8926 - val_accuracy: 0.3560\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.6738 - accuracy: 0.4371 - val_loss: 1.9111 - val_accuracy: 0.3532\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.6563 - accuracy: 0.4419 - val_loss: 1.9292 - val_accuracy: 0.3503\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.6347 - accuracy: 0.4479 - val_loss: 1.9426 - val_accuracy: 0.3486\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.6210 - accuracy: 0.4550 - val_loss: 1.9552 - val_accuracy: 0.3475\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.6143 - accuracy: 0.4552 - val_loss: 1.9576 - val_accuracy: 0.3500\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.5883 - accuracy: 0.4637 - val_loss: 1.9778 - val_accuracy: 0.3532\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.5712 - accuracy: 0.4682 - val_loss: 1.9779 - val_accuracy: 0.3540\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 14s 53ms/step - loss: 1.5540 - accuracy: 0.4770 - val_loss: 1.9960 - val_accuracy: 0.3454\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.5407 - accuracy: 0.4852 - val_loss: 2.0094 - val_accuracy: 0.3352\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.5251 - accuracy: 0.4913 - val_loss: 2.0185 - val_accuracy: 0.3411\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.5065 - accuracy: 0.4982 - val_loss: 2.0465 - val_accuracy: 0.3420\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 14s 53ms/step - loss: 1.4941 - accuracy: 0.5029 - val_loss: 2.0423 - val_accuracy: 0.3422\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4747 - accuracy: 0.5100 - val_loss: 2.0339 - val_accuracy: 0.3455\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4702 - accuracy: 0.5094 - val_loss: 2.0585 - val_accuracy: 0.3408\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4616 - accuracy: 0.5156 - val_loss: 2.0456 - val_accuracy: 0.3426\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4648 - accuracy: 0.5154 - val_loss: 2.0440 - val_accuracy: 0.3442\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4421 - accuracy: 0.5227 - val_loss: 2.0663 - val_accuracy: 0.3360\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4229 - accuracy: 0.5333 - val_loss: 2.0856 - val_accuracy: 0.3374\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.4139 - accuracy: 0.5318 - val_loss: 2.1184 - val_accuracy: 0.3309\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 14s 53ms/step - loss: 1.4005 - accuracy: 0.5377 - val_loss: 2.1005 - val_accuracy: 0.3355\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 14s 53ms/step - loss: 1.4016 - accuracy: 0.5372 - val_loss: 2.1277 - val_accuracy: 0.3308\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.3961 - accuracy: 0.5396 - val_loss: 2.1367 - val_accuracy: 0.3325\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 13s 53ms/step - loss: 1.3898 - accuracy: 0.5384 - val_loss: 2.1328 - val_accuracy: 0.3343\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 13s 53ms/step - loss: 1.3818 - accuracy: 0.5410 - val_loss: 2.1431 - val_accuracy: 0.3314\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.3715 - accuracy: 0.5460 - val_loss: 2.1693 - val_accuracy: 0.3331\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 14s 53ms/step - loss: 1.3542 - accuracy: 0.5502 - val_loss: 2.1779 - val_accuracy: 0.3329\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.3520 - accuracy: 0.5577 - val_loss: 2.2010 - val_accuracy: 0.3303\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.3404 - accuracy: 0.5585 - val_loss: 2.2103 - val_accuracy: 0.3265\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.3449 - accuracy: 0.5553 - val_loss: 2.1850 - val_accuracy: 0.3335\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 13s 52ms/step - loss: 1.3369 - accuracy: 0.5566 - val_loss: 2.2106 - val_accuracy: 0.3309\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 2.2459 - accuracy: 0.3156\n",
            "[2.245852470397949, 0.31562501192092896]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPBMUSdWvuvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f5f4a2-6fdf-48bf-de46-f1b11399621b"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 27s 92ms/step - loss: 2.1361 - accuracy: 0.2640 - val_loss: 1.8922 - val_accuracy: 0.3445\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.8820 - accuracy: 0.3490 - val_loss: 1.8474 - val_accuracy: 0.3634\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.8198 - accuracy: 0.3759 - val_loss: 1.8385 - val_accuracy: 0.3675\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.7823 - accuracy: 0.3878 - val_loss: 1.8358 - val_accuracy: 0.3698\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.7491 - accuracy: 0.4018 - val_loss: 1.8410 - val_accuracy: 0.3702\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.7126 - accuracy: 0.4138 - val_loss: 1.8604 - val_accuracy: 0.3688\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.6769 - accuracy: 0.4300 - val_loss: 1.8769 - val_accuracy: 0.3643\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.6352 - accuracy: 0.4451 - val_loss: 1.9042 - val_accuracy: 0.3589\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.5977 - accuracy: 0.4597 - val_loss: 1.9273 - val_accuracy: 0.3514\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.5553 - accuracy: 0.4788 - val_loss: 1.9490 - val_accuracy: 0.3471\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.5159 - accuracy: 0.4910 - val_loss: 1.9642 - val_accuracy: 0.3442\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.4777 - accuracy: 0.5061 - val_loss: 2.0029 - val_accuracy: 0.3374\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.4335 - accuracy: 0.5193 - val_loss: 2.0349 - val_accuracy: 0.3340\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.4031 - accuracy: 0.5308 - val_loss: 2.0589 - val_accuracy: 0.3292\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.3596 - accuracy: 0.5507 - val_loss: 2.0985 - val_accuracy: 0.3286\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.3161 - accuracy: 0.5647 - val_loss: 2.1132 - val_accuracy: 0.3198\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.2905 - accuracy: 0.5736 - val_loss: 2.1606 - val_accuracy: 0.3180\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.2611 - accuracy: 0.5866 - val_loss: 2.2058 - val_accuracy: 0.3174\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.2227 - accuracy: 0.6008 - val_loss: 2.2280 - val_accuracy: 0.3118\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.1920 - accuracy: 0.6139 - val_loss: 2.2649 - val_accuracy: 0.3105\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.1585 - accuracy: 0.6248 - val_loss: 2.3013 - val_accuracy: 0.3085\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.1353 - accuracy: 0.6305 - val_loss: 2.3496 - val_accuracy: 0.3114\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.1154 - accuracy: 0.6396 - val_loss: 2.3924 - val_accuracy: 0.3103\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.0854 - accuracy: 0.6481 - val_loss: 2.4361 - val_accuracy: 0.3108\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.0616 - accuracy: 0.6557 - val_loss: 2.4629 - val_accuracy: 0.3049\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.0338 - accuracy: 0.6662 - val_loss: 2.4860 - val_accuracy: 0.3068\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 1.0041 - accuracy: 0.6761 - val_loss: 2.5286 - val_accuracy: 0.3025\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.9793 - accuracy: 0.6838 - val_loss: 2.5620 - val_accuracy: 0.2997\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.9616 - accuracy: 0.6895 - val_loss: 2.6394 - val_accuracy: 0.2997\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.9505 - accuracy: 0.6907 - val_loss: 2.6750 - val_accuracy: 0.2995\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.9285 - accuracy: 0.7019 - val_loss: 2.7337 - val_accuracy: 0.2985\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.8873 - accuracy: 0.7202 - val_loss: 2.7714 - val_accuracy: 0.2948\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.8752 - accuracy: 0.7202 - val_loss: 2.8341 - val_accuracy: 0.2940\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.8574 - accuracy: 0.7288 - val_loss: 2.8735 - val_accuracy: 0.2875\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 23s 91ms/step - loss: 0.8249 - accuracy: 0.7421 - val_loss: 2.9108 - val_accuracy: 0.2860\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.7955 - accuracy: 0.7477 - val_loss: 2.9785 - val_accuracy: 0.2869\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.7787 - accuracy: 0.7569 - val_loss: 2.9805 - val_accuracy: 0.2815\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.7617 - accuracy: 0.7620 - val_loss: 3.0686 - val_accuracy: 0.2808\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.7448 - accuracy: 0.7684 - val_loss: 3.0897 - val_accuracy: 0.2822\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 23s 90ms/step - loss: 0.7145 - accuracy: 0.7790 - val_loss: 3.1762 - val_accuracy: 0.2829\n",
            "80/80 [==============================] - 3s 38ms/step - loss: 3.1873 - accuracy: 0.2788\n",
            "[3.1872739791870117, 0.2787500023841858]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_edXJqewY7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acba55bb-dc74-42e4-c825-602b7fba5228"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 7s 25ms/step - loss: 2.2905 - accuracy: 0.1942 - val_loss: 2.1396 - val_accuracy: 0.2155\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1426 - accuracy: 0.2161 - val_loss: 2.1365 - val_accuracy: 0.2154\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1423 - accuracy: 0.2168 - val_loss: 2.1373 - val_accuracy: 0.2158\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1395 - accuracy: 0.2250 - val_loss: 2.1392 - val_accuracy: 0.2100\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1178 - accuracy: 0.2424 - val_loss: 2.0590 - val_accuracy: 0.2605\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0443 - accuracy: 0.2639 - val_loss: 2.0383 - val_accuracy: 0.2694\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0096 - accuracy: 0.2901 - val_loss: 2.0216 - val_accuracy: 0.2803\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9961 - accuracy: 0.2996 - val_loss: 2.0258 - val_accuracy: 0.2852\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9897 - accuracy: 0.3030 - val_loss: 2.0330 - val_accuracy: 0.2874\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9731 - accuracy: 0.3176 - val_loss: 2.0161 - val_accuracy: 0.2849\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9483 - accuracy: 0.3286 - val_loss: 2.0160 - val_accuracy: 0.2937\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9380 - accuracy: 0.3332 - val_loss: 2.0371 - val_accuracy: 0.2920\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9449 - accuracy: 0.3338 - val_loss: 2.0326 - val_accuracy: 0.2918\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9474 - accuracy: 0.3278 - val_loss: 2.0465 - val_accuracy: 0.2943\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9397 - accuracy: 0.3369 - val_loss: 2.0251 - val_accuracy: 0.2895\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 1.9758 - accuracy: 0.3228 - val_loss: 2.1510 - val_accuracy: 0.2275\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0509 - accuracy: 0.2782 - val_loss: 2.1526 - val_accuracy: 0.2315\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1167 - accuracy: 0.2545 - val_loss: 2.1016 - val_accuracy: 0.2600\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0545 - accuracy: 0.2817 - val_loss: 2.0724 - val_accuracy: 0.2528\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.0339 - accuracy: 0.2880 - val_loss: 2.1403 - val_accuracy: 0.2363\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.0814 - accuracy: 0.2618 - val_loss: 2.0853 - val_accuracy: 0.2518\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.0757 - accuracy: 0.2555 - val_loss: 2.1364 - val_accuracy: 0.2389\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.1222 - accuracy: 0.2508 - val_loss: 2.1342 - val_accuracy: 0.2394\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1097 - accuracy: 0.2587 - val_loss: 2.1369 - val_accuracy: 0.2348\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0971 - accuracy: 0.2627 - val_loss: 2.1033 - val_accuracy: 0.2395\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0876 - accuracy: 0.2653 - val_loss: 2.1408 - val_accuracy: 0.2348\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.1105 - accuracy: 0.2614 - val_loss: 2.1270 - val_accuracy: 0.2400\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0854 - accuracy: 0.2746 - val_loss: 2.1107 - val_accuracy: 0.2509\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0808 - accuracy: 0.2764 - val_loss: 2.1144 - val_accuracy: 0.2349\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0768 - accuracy: 0.2677 - val_loss: 2.1013 - val_accuracy: 0.2395\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.0536 - accuracy: 0.2791 - val_loss: 2.1178 - val_accuracy: 0.2429\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0958 - accuracy: 0.2639 - val_loss: 2.1163 - val_accuracy: 0.2142\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0994 - accuracy: 0.2479 - val_loss: 2.1052 - val_accuracy: 0.2458\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0816 - accuracy: 0.2664 - val_loss: 2.0971 - val_accuracy: 0.2515\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0811 - accuracy: 0.2707 - val_loss: 2.1081 - val_accuracy: 0.2354\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0958 - accuracy: 0.2463 - val_loss: 2.1009 - val_accuracy: 0.2332\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 6s 24ms/step - loss: 2.0841 - accuracy: 0.2506 - val_loss: 2.1020 - val_accuracy: 0.2432\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.0814 - accuracy: 0.2599 - val_loss: 2.0984 - val_accuracy: 0.2400\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.0867 - accuracy: 0.2533 - val_loss: 2.1052 - val_accuracy: 0.2260\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 6s 25ms/step - loss: 2.1191 - accuracy: 0.2359 - val_loss: 2.1407 - val_accuracy: 0.2192\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 2.1507 - accuracy: 0.2180\n",
            "[2.1507132053375244, 0.21799999475479126]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CGeJPeZwY3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296d3661-8557-4ba1-a293-8185fd2b14b7"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "# print(history_sg_bi.history)\r\n",
        "# plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()\r\n",
        "# plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.31349998712539673, 0.27562499046325684, 0.21799999475479126]\n",
            "cbow [0.3023749887943268, 0.2706249952316284, 0.20212499797344208]\n",
            "glove [0.31562501192092896, 0.2787500023841858, 0.21799999475479126]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6YWN3P2wba7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}