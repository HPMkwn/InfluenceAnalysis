{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllCF_Sent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllCF_Sent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx-B1Nnp1Xm5",
        "outputId": "38e06b3b-27f2-4bf9-b8ad-dce30e8fc7ad"
      },
      "source": [
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7xUQbhdpm55",
        "outputId": "87708d53-d7b8-47ca-ea1f-244680d0e23c"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from gensim.models import Word2Vec\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import sentiwordnet as swn\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "print(device_name)\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/crowdFlower/crowdFlower_clean.csv')\r\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "v-3YRgz91vOB",
        "outputId": "f27a3186-6d0c-43d5-c1e0-fb47e271a0b3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35975</th>\n",
              "      <td>35975</td>\n",
              "      <td>6</td>\n",
              "      <td>katofawesome praying for love in a lap dance a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17590</th>\n",
              "      <td>17590</td>\n",
              "      <td>1</td>\n",
              "      <td>trying to fix my internet connectionguess my p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38071</th>\n",
              "      <td>38071</td>\n",
              "      <td>0</td>\n",
              "      <td>starting an account here on twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23304</th>\n",
              "      <td>23304</td>\n",
              "      <td>1</td>\n",
              "      <td>dipfico hmm wrong link ignore my tweet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26990</th>\n",
              "      <td>26990</td>\n",
              "      <td>0</td>\n",
              "      <td>mzunyque thanks before the major chop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  emotion                                             tweets\n",
              "35975       35975        6  katofawesome praying for love in a lap dance a...\n",
              "17590       17590        1  trying to fix my internet connectionguess my p...\n",
              "38071       38071        0                starting an account here on twitter\n",
              "23304       23304        1             dipfico hmm wrong link ignore my tweet\n",
              "26990       26990        0              mzunyque thanks before the major chop"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z18Mari1xuy",
        "outputId": "ee60b581-379f-4b7f-ebe1-9918c6cad647"
      },
      "source": [
        "df.emotion.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6,  1,  0,  2,  3,  4, 10,  5,  7,  8,  9, 12, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NsmUUgQp1il"
      },
      "source": [
        "category_dict = {0:[1,0,0,0,0,0,0,0,0,0,0,0,0],1:[0,1,0,0,0,0,0,0,0,0,0,0,0],2:[0,0,1,0,0,0,0,0,0,0,0,0,0],3:[0,0,0,1,0,0,0,0,0,0,0,0,0],4:[0,0,0,0,1,0,0,0,0,0,0,0,0],5:[0,0,0,0,0,1,0,0,0,0,0,0,0],6:[0,0,0,0,0,0,1,0,0,0,0,0,0],7:[0,0,0,0,0,0,0,1,0,0,0,0,0],8:[0,0,0,0,0,0,0,0,1,0,0,0,0],9:[0,0,0,0,0,0,0,0,0,1,0,0,0],10:[0,0,0,0,0,0,0,0,0,0,1,0,0],11:[0,0,0,0,0,0,0,0,0,0,0,1,0],12:[0,0,0,0,0,0,0,0,0,0,0,0,1]}\r\n",
        "data_tweet = [x.lower().split() for x in df['tweets']]\r\n",
        "data_cat = np.array([category_dict[x] for x in df['emotion']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMNiDUVVp5UK",
        "outputId": "010daa5b-d804-4447-9f6c-ecb8b63783b4"
      },
      "source": [
        "print(data_tweet[:5])\r\n",
        "print(data_cat[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['katofawesome', 'praying', 'for', 'love', 'in', 'a', 'lap', 'dance', 'and', 'paying', 'in', 'naivety', 'lt--one', 'of', 'my', 'fav', 'songs'], ['trying', 'to', 'fix', 'my', 'internet', 'connectionguess', 'my', 'prayers', 'have', 'been', 'answered', 'and', 'i', 'wont', 'have', 'any', 'study', 'distractions', 'ugh'], ['starting', 'an', 'account', 'here', 'on', 'twitter'], ['dipfico', 'hmm', 'wrong', 'link', 'ignore', 'my', 'tweet'], ['mzunyque', 'thanks', 'before', 'the', 'major', 'chop']]\n",
            "[[0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q37vZ3dKp5SB"
      },
      "source": [
        "# Parameters\r\n",
        "Min_count = 0\r\n",
        "Embedding_size = 200\r\n",
        "Window_size = 5\r\n",
        "Negative_sampling = 00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K-8Bji5p5OU"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\r\n",
        "                     window=Window_size,\r\n",
        "                     size=Embedding_size,\r\n",
        "                     negative=Negative_sampling,sg=1)\r\n",
        "w2v_sg.build_vocab(data_tweet)\r\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\r\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\r\n",
        "                     window=Window_size,\r\n",
        "                     size=Embedding_size,\r\n",
        "                     negative=Negative_sampling,sg=0)\r\n",
        "w2v_cbow.build_vocab(data_tweet)\r\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\r\n",
        "w2v_sg.wv.init_sims(True)\r\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs8MuQvjp5K6"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\r\n",
        "vocab_sg = [x for x in vocab_sg]\r\n",
        "vocab_cbow = w2v_cbow.wv.vocab\r\n",
        "vocab_cbow = [x for x in vocab_cbow] \r\n",
        "vocab_glove = {}\r\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\r\n",
        "  for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      vector = np.asarray(values[1:], \"float32\")\r\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaxTpKUQp1g0"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "data_tweet = [nltk.pos_tag(x) for x in data_tweet]\r\n",
        "def getsent(word,tag):\r\n",
        "  res=0\r\n",
        "  try:\r\n",
        "    x = swn.senti_synset(word+'.'+tag[0].lower()+'.01')\r\n",
        "    res =  (x.pos_score()-x.neg_score())\r\n",
        "  finally:\r\n",
        "    return res \r\n",
        "data_tweet = [[(i[0],i[1],getsent(i[0],i[1])) for i in x] for x in data_tweet]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo55tJBvp1eG"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][2]<0:\r\n",
        "            tweet[i]=((tweet[i][0]*(-1)),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][2]<0:\r\n",
        "            tweet[i]=((tweet[i][0]*(-1)),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtA07m2Xp1at"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N68btQXPp1YY"
      },
      "source": [
        "epochs = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvY_HaMgp1Vq",
        "outputId": "d811dfed-740f-4f80-c279-7cc0a5b9bd24"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\r\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\r\n",
        "model_sg.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[0]=history[1]\r\n",
        "print(\"SG_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 118s 453ms/step - loss: 2.2601 - accuracy: 0.2095 - val_loss: 2.1384 - val_accuracy: 0.2174\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 114s 444ms/step - loss: 2.1469 - accuracy: 0.2161 - val_loss: 2.1360 - val_accuracy: 0.2175\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 2.1479 - accuracy: 0.2187 - val_loss: 2.1367 - val_accuracy: 0.2180\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 2.1459 - accuracy: 0.2211 - val_loss: 2.1260 - val_accuracy: 0.2558\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 113s 439ms/step - loss: 2.1266 - accuracy: 0.2548 - val_loss: 2.0985 - val_accuracy: 0.2649\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 113s 443ms/step - loss: 2.0938 - accuracy: 0.2722 - val_loss: 2.0604 - val_accuracy: 0.2792\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 115s 451ms/step - loss: 2.0605 - accuracy: 0.2876 - val_loss: 2.0505 - val_accuracy: 0.2903\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 2.0377 - accuracy: 0.2999 - val_loss: 2.0389 - val_accuracy: 0.3009\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 113s 442ms/step - loss: 2.0216 - accuracy: 0.3095 - val_loss: 2.0305 - val_accuracy: 0.3028\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 115s 449ms/step - loss: 2.0047 - accuracy: 0.3170 - val_loss: 2.0205 - val_accuracy: 0.3042\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 1.9914 - accuracy: 0.3216 - val_loss: 2.0168 - val_accuracy: 0.3042\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 116s 453ms/step - loss: 1.9789 - accuracy: 0.3274 - val_loss: 2.0089 - val_accuracy: 0.3098\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 112s 437ms/step - loss: 1.9615 - accuracy: 0.3335 - val_loss: 2.0021 - val_accuracy: 0.3135\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 111s 431ms/step - loss: 1.9434 - accuracy: 0.3379 - val_loss: 2.0022 - val_accuracy: 0.3165\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 111s 435ms/step - loss: 1.9323 - accuracy: 0.3412 - val_loss: 2.0033 - val_accuracy: 0.3103\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 111s 432ms/step - loss: 1.9165 - accuracy: 0.3459 - val_loss: 2.0038 - val_accuracy: 0.3145\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 110s 431ms/step - loss: 1.9059 - accuracy: 0.3512 - val_loss: 2.0074 - val_accuracy: 0.3129\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 111s 434ms/step - loss: 1.8922 - accuracy: 0.3520 - val_loss: 2.0076 - val_accuracy: 0.3132\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 110s 430ms/step - loss: 1.8795 - accuracy: 0.3589 - val_loss: 2.0066 - val_accuracy: 0.3180\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 111s 432ms/step - loss: 1.8690 - accuracy: 0.3641 - val_loss: 2.0118 - val_accuracy: 0.3189\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 1.8586 - accuracy: 0.3673 - val_loss: 2.0093 - val_accuracy: 0.3186\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 112s 435ms/step - loss: 1.8513 - accuracy: 0.3692 - val_loss: 2.0165 - val_accuracy: 0.3149\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 111s 435ms/step - loss: 1.8357 - accuracy: 0.3767 - val_loss: 2.0320 - val_accuracy: 0.3128\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 111s 432ms/step - loss: 1.8234 - accuracy: 0.3814 - val_loss: 2.0476 - val_accuracy: 0.3148\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 110s 430ms/step - loss: 1.8152 - accuracy: 0.3869 - val_loss: 2.0431 - val_accuracy: 0.3162\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 116s 451ms/step - loss: 1.8086 - accuracy: 0.3907 - val_loss: 2.0740 - val_accuracy: 0.3098\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 118s 460ms/step - loss: 1.8038 - accuracy: 0.3927 - val_loss: 2.0792 - val_accuracy: 0.3111\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 118s 461ms/step - loss: 1.7906 - accuracy: 0.3950 - val_loss: 2.0930 - val_accuracy: 0.3126\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 120s 467ms/step - loss: 1.7898 - accuracy: 0.3965 - val_loss: 2.1098 - val_accuracy: 0.3086\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 116s 452ms/step - loss: 1.7785 - accuracy: 0.4004 - val_loss: 2.1302 - val_accuracy: 0.3065\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 115s 449ms/step - loss: 1.7691 - accuracy: 0.4065 - val_loss: 2.1320 - val_accuracy: 0.3109\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 117s 457ms/step - loss: 1.7795 - accuracy: 0.3983 - val_loss: 2.1318 - val_accuracy: 0.3102\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 116s 454ms/step - loss: 1.7651 - accuracy: 0.4034 - val_loss: 2.1453 - val_accuracy: 0.3071\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 116s 455ms/step - loss: 1.7535 - accuracy: 0.4126 - val_loss: 2.1219 - val_accuracy: 0.3146\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 115s 449ms/step - loss: 1.7447 - accuracy: 0.4158 - val_loss: 2.1386 - val_accuracy: 0.3097\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 113s 442ms/step - loss: 1.7494 - accuracy: 0.4124 - val_loss: 2.1321 - val_accuracy: 0.3151\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 113s 442ms/step - loss: 1.7240 - accuracy: 0.4203 - val_loss: 2.1423 - val_accuracy: 0.3117\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 114s 444ms/step - loss: 1.7120 - accuracy: 0.4307 - val_loss: 2.1426 - val_accuracy: 0.3182\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 116s 453ms/step - loss: 1.7089 - accuracy: 0.4274 - val_loss: 2.1542 - val_accuracy: 0.3122\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 118s 461ms/step - loss: 1.6905 - accuracy: 0.4365 - val_loss: 2.1443 - val_accuracy: 0.3123\n",
            "80/80 [==============================] - 27s 342ms/step - loss: 2.1839 - accuracy: 0.2944\n",
            "[2.1839029788970947, 0.2943750023841858]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESbfjsopp1TI",
        "outputId": "1a17271a-4940-49af-8a20-41a5acdee7c7"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 130s 493ms/step - loss: 2.2416 - accuracy: 0.2132 - val_loss: 2.0895 - val_accuracy: 0.2643\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 126s 493ms/step - loss: 2.0755 - accuracy: 0.2756 - val_loss: 2.0551 - val_accuracy: 0.2858\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 123s 479ms/step - loss: 2.0337 - accuracy: 0.3057 - val_loss: 2.0383 - val_accuracy: 0.3026\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 124s 482ms/step - loss: 2.0152 - accuracy: 0.3098 - val_loss: 2.0256 - val_accuracy: 0.3071\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 128s 499ms/step - loss: 1.9969 - accuracy: 0.3155 - val_loss: 2.0223 - val_accuracy: 0.3088\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 127s 494ms/step - loss: 1.9812 - accuracy: 0.3206 - val_loss: 2.0158 - val_accuracy: 0.3100\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 124s 483ms/step - loss: 1.9607 - accuracy: 0.3297 - val_loss: 2.0067 - val_accuracy: 0.3123\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 120s 470ms/step - loss: 1.9393 - accuracy: 0.3391 - val_loss: 2.0051 - val_accuracy: 0.3106\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 123s 479ms/step - loss: 1.9201 - accuracy: 0.3439 - val_loss: 2.0015 - val_accuracy: 0.3143\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 127s 496ms/step - loss: 1.8983 - accuracy: 0.3474 - val_loss: 1.9949 - val_accuracy: 0.3138\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 125s 490ms/step - loss: 1.8755 - accuracy: 0.3542 - val_loss: 1.9903 - val_accuracy: 0.3131\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 126s 493ms/step - loss: 1.8583 - accuracy: 0.3600 - val_loss: 1.9975 - val_accuracy: 0.3126\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 124s 484ms/step - loss: 1.8379 - accuracy: 0.3646 - val_loss: 2.0061 - val_accuracy: 0.3091\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 122s 477ms/step - loss: 1.8214 - accuracy: 0.3737 - val_loss: 2.0171 - val_accuracy: 0.3111\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 124s 482ms/step - loss: 1.8005 - accuracy: 0.3818 - val_loss: 2.0278 - val_accuracy: 0.3105\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 125s 488ms/step - loss: 1.7806 - accuracy: 0.3882 - val_loss: 2.0450 - val_accuracy: 0.3142\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 127s 494ms/step - loss: 1.7613 - accuracy: 0.3963 - val_loss: 2.0579 - val_accuracy: 0.3117\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 126s 491ms/step - loss: 1.7411 - accuracy: 0.4034 - val_loss: 2.0629 - val_accuracy: 0.3134\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 125s 486ms/step - loss: 1.7199 - accuracy: 0.4114 - val_loss: 2.0609 - val_accuracy: 0.3117\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 129s 503ms/step - loss: 1.7019 - accuracy: 0.4175 - val_loss: 2.0652 - val_accuracy: 0.3126\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 127s 496ms/step - loss: 1.6873 - accuracy: 0.4203 - val_loss: 2.0732 - val_accuracy: 0.3149\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 129s 502ms/step - loss: 1.6701 - accuracy: 0.4304 - val_loss: 2.0911 - val_accuracy: 0.3115\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 126s 492ms/step - loss: 1.6521 - accuracy: 0.4377 - val_loss: 2.1029 - val_accuracy: 0.3100\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 123s 479ms/step - loss: 1.6351 - accuracy: 0.4418 - val_loss: 2.1199 - val_accuracy: 0.3105\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 125s 486ms/step - loss: 1.6204 - accuracy: 0.4482 - val_loss: 2.1392 - val_accuracy: 0.3075\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 129s 505ms/step - loss: 1.6092 - accuracy: 0.4505 - val_loss: 2.1492 - val_accuracy: 0.3060\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 130s 507ms/step - loss: 1.5976 - accuracy: 0.4562 - val_loss: 2.1692 - val_accuracy: 0.3040\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 127s 497ms/step - loss: 1.5727 - accuracy: 0.4675 - val_loss: 2.1901 - val_accuracy: 0.3034\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 125s 487ms/step - loss: 1.5643 - accuracy: 0.4684 - val_loss: 2.2483 - val_accuracy: 0.2925\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 126s 492ms/step - loss: 1.5453 - accuracy: 0.4768 - val_loss: 2.2493 - val_accuracy: 0.2957\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 126s 493ms/step - loss: 1.5326 - accuracy: 0.4811 - val_loss: 2.2597 - val_accuracy: 0.2909\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 128s 498ms/step - loss: 1.5210 - accuracy: 0.4822 - val_loss: 2.2595 - val_accuracy: 0.2903\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 125s 489ms/step - loss: 1.5052 - accuracy: 0.4901 - val_loss: 2.2736 - val_accuracy: 0.2946\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 124s 484ms/step - loss: 1.4832 - accuracy: 0.4971 - val_loss: 2.2883 - val_accuracy: 0.2966\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 121s 472ms/step - loss: 1.4573 - accuracy: 0.5063 - val_loss: 2.3035 - val_accuracy: 0.2903\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 123s 479ms/step - loss: 1.4306 - accuracy: 0.5151 - val_loss: 2.3381 - val_accuracy: 0.2923\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 128s 499ms/step - loss: 1.4170 - accuracy: 0.5196 - val_loss: 2.3537 - val_accuracy: 0.2954\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 127s 495ms/step - loss: 1.4015 - accuracy: 0.5270 - val_loss: 2.3713 - val_accuracy: 0.2906\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 125s 489ms/step - loss: 1.3717 - accuracy: 0.5395 - val_loss: 2.3854 - val_accuracy: 0.2915\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 126s 490ms/step - loss: 1.3575 - accuracy: 0.5457 - val_loss: 2.4142 - val_accuracy: 0.2915\n",
            "80/80 [==============================] - 28s 357ms/step - loss: 2.4579 - accuracy: 0.2885\n",
            "[2.4579172134399414, 0.28850001096725464]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekiSU4z6p1Qi",
        "outputId": "fe4065ee-a117-483f-fff7-58f2132f4084"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 112s 432ms/step - loss: 2.2344 - accuracy: 0.2012 - val_loss: 2.1419 - val_accuracy: 0.2157\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 110s 430ms/step - loss: 2.1430 - accuracy: 0.2151 - val_loss: 2.1384 - val_accuracy: 0.2154\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 110s 428ms/step - loss: 2.1438 - accuracy: 0.2147 - val_loss: 2.1387 - val_accuracy: 0.2158\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 109s 424ms/step - loss: 2.1441 - accuracy: 0.2173 - val_loss: 2.1399 - val_accuracy: 0.2155\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 113s 443ms/step - loss: 2.1406 - accuracy: 0.2195 - val_loss: 2.1391 - val_accuracy: 0.2125\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 113s 440ms/step - loss: 2.1382 - accuracy: 0.2229 - val_loss: 2.1348 - val_accuracy: 0.2335\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 111s 435ms/step - loss: 2.1287 - accuracy: 0.2302 - val_loss: 2.1350 - val_accuracy: 0.2383\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 109s 424ms/step - loss: 2.1172 - accuracy: 0.2423 - val_loss: 2.1461 - val_accuracy: 0.2302\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 109s 427ms/step - loss: 2.1073 - accuracy: 0.2542 - val_loss: 2.1416 - val_accuracy: 0.2340\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 111s 432ms/step - loss: 2.0953 - accuracy: 0.2625 - val_loss: 2.1657 - val_accuracy: 0.2337\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 112s 436ms/step - loss: 2.0932 - accuracy: 0.2638 - val_loss: 2.1212 - val_accuracy: 0.2417\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 2.0816 - accuracy: 0.2738 - val_loss: 2.1664 - val_accuracy: 0.2329\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 113s 441ms/step - loss: 2.0745 - accuracy: 0.2704 - val_loss: 2.1295 - val_accuracy: 0.2423\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 112s 436ms/step - loss: 2.0675 - accuracy: 0.2769 - val_loss: 2.1552 - val_accuracy: 0.2368\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 110s 428ms/step - loss: 2.0633 - accuracy: 0.2779 - val_loss: 2.1426 - val_accuracy: 0.2342\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 109s 427ms/step - loss: 2.0512 - accuracy: 0.2840 - val_loss: 2.1464 - val_accuracy: 0.2395\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 109s 425ms/step - loss: 2.0440 - accuracy: 0.2930 - val_loss: 2.1433 - val_accuracy: 0.2389\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 108s 422ms/step - loss: 2.0379 - accuracy: 0.2918 - val_loss: 2.1511 - val_accuracy: 0.2425\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 106s 415ms/step - loss: 2.0369 - accuracy: 0.2905 - val_loss: 2.1562 - val_accuracy: 0.2403\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 108s 421ms/step - loss: 2.0334 - accuracy: 0.2949 - val_loss: 2.1547 - val_accuracy: 0.2409\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 109s 424ms/step - loss: 2.0396 - accuracy: 0.2896 - val_loss: 2.1535 - val_accuracy: 0.2294\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 109s 426ms/step - loss: 2.0340 - accuracy: 0.2950 - val_loss: 2.1457 - val_accuracy: 0.2377\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 110s 429ms/step - loss: 2.0335 - accuracy: 0.2965 - val_loss: 2.1464 - val_accuracy: 0.2368\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 110s 429ms/step - loss: 2.0282 - accuracy: 0.2964 - val_loss: 2.1545 - val_accuracy: 0.2346\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 108s 421ms/step - loss: 2.0191 - accuracy: 0.3002 - val_loss: 2.1566 - val_accuracy: 0.2343\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 109s 424ms/step - loss: 2.0181 - accuracy: 0.3036 - val_loss: 2.1656 - val_accuracy: 0.2335\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 108s 422ms/step - loss: 2.0080 - accuracy: 0.3062 - val_loss: 2.1702 - val_accuracy: 0.2326\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 108s 421ms/step - loss: 2.0014 - accuracy: 0.3102 - val_loss: 2.1712 - val_accuracy: 0.2303\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 108s 423ms/step - loss: 2.0042 - accuracy: 0.3104 - val_loss: 2.1743 - val_accuracy: 0.2349\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 106s 415ms/step - loss: 2.0145 - accuracy: 0.3002 - val_loss: 2.1762 - val_accuracy: 0.2342\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 105s 411ms/step - loss: 2.0170 - accuracy: 0.2999 - val_loss: 2.1892 - val_accuracy: 0.2295\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 107s 417ms/step - loss: 2.0167 - accuracy: 0.3007 - val_loss: 2.1930 - val_accuracy: 0.2332\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 108s 423ms/step - loss: 2.0110 - accuracy: 0.3042 - val_loss: 2.1978 - val_accuracy: 0.2300\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 112s 437ms/step - loss: 2.0062 - accuracy: 0.3086 - val_loss: 2.1948 - val_accuracy: 0.2337\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 110s 431ms/step - loss: 2.0025 - accuracy: 0.3105 - val_loss: 2.2024 - val_accuracy: 0.2288\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 110s 430ms/step - loss: 2.0012 - accuracy: 0.3111 - val_loss: 2.2141 - val_accuracy: 0.2278\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 108s 423ms/step - loss: 1.9948 - accuracy: 0.3135 - val_loss: 2.2095 - val_accuracy: 0.2288\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 108s 423ms/step - loss: 1.9922 - accuracy: 0.3184 - val_loss: 2.2182 - val_accuracy: 0.2249\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 109s 427ms/step - loss: 2.0026 - accuracy: 0.3143 - val_loss: 2.2163 - val_accuracy: 0.2275\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 111s 434ms/step - loss: 2.0092 - accuracy: 0.3087 - val_loss: 2.2301 - val_accuracy: 0.2312\n",
            "80/80 [==============================] - 28s 353ms/step - loss: 2.2538 - accuracy: 0.2264\n",
            "[2.2538485527038574, 0.22637499868869781]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL6K0JYhqRBo",
        "outputId": "5c3de94f-59f5-4404-cee0-bfc68b3ec47e"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 119s 458ms/step - loss: 2.2606 - accuracy: 0.2135 - val_loss: 2.1229 - val_accuracy: 0.2563\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 114s 444ms/step - loss: 2.1198 - accuracy: 0.2456 - val_loss: 2.0792 - val_accuracy: 0.2611\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 113s 443ms/step - loss: 2.0794 - accuracy: 0.2687 - val_loss: 2.0611 - val_accuracy: 0.2808\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 112s 438ms/step - loss: 2.0495 - accuracy: 0.2905 - val_loss: 2.0417 - val_accuracy: 0.2963\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 114s 446ms/step - loss: 2.0262 - accuracy: 0.3072 - val_loss: 2.0341 - val_accuracy: 0.2971\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 113s 442ms/step - loss: 2.0139 - accuracy: 0.3139 - val_loss: 2.0232 - val_accuracy: 0.3011\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 114s 444ms/step - loss: 1.9995 - accuracy: 0.3232 - val_loss: 2.0199 - val_accuracy: 0.3040\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 114s 447ms/step - loss: 1.9897 - accuracy: 0.3261 - val_loss: 2.0132 - val_accuracy: 0.3069\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 116s 452ms/step - loss: 1.9771 - accuracy: 0.3300 - val_loss: 2.0078 - val_accuracy: 0.3105\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 114s 447ms/step - loss: 1.9612 - accuracy: 0.3367 - val_loss: 1.9984 - val_accuracy: 0.3108\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 116s 454ms/step - loss: 1.9464 - accuracy: 0.3437 - val_loss: 1.9937 - val_accuracy: 0.3135\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 114s 446ms/step - loss: 1.9346 - accuracy: 0.3483 - val_loss: 1.9907 - val_accuracy: 0.3172\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 110s 431ms/step - loss: 1.9203 - accuracy: 0.3528 - val_loss: 1.9926 - val_accuracy: 0.3142\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 112s 436ms/step - loss: 1.9061 - accuracy: 0.3574 - val_loss: 1.9913 - val_accuracy: 0.3175\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 113s 440ms/step - loss: 1.8975 - accuracy: 0.3621 - val_loss: 1.9985 - val_accuracy: 0.3137\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 114s 444ms/step - loss: 1.8879 - accuracy: 0.3670 - val_loss: 1.9941 - val_accuracy: 0.3146\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 117s 456ms/step - loss: 1.8846 - accuracy: 0.3645 - val_loss: 2.0037 - val_accuracy: 0.3120\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 112s 439ms/step - loss: 1.8723 - accuracy: 0.3712 - val_loss: 2.0116 - val_accuracy: 0.3155\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 115s 449ms/step - loss: 1.8684 - accuracy: 0.3706 - val_loss: 2.0132 - val_accuracy: 0.3152\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 114s 446ms/step - loss: 1.8594 - accuracy: 0.3768 - val_loss: 2.0238 - val_accuracy: 0.3155\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 115s 447ms/step - loss: 1.8488 - accuracy: 0.3808 - val_loss: 2.0248 - val_accuracy: 0.3182\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 115s 448ms/step - loss: 1.8404 - accuracy: 0.3880 - val_loss: 2.0298 - val_accuracy: 0.3188\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 113s 440ms/step - loss: 1.8289 - accuracy: 0.3922 - val_loss: 2.0307 - val_accuracy: 0.3203\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 111s 434ms/step - loss: 1.8209 - accuracy: 0.3938 - val_loss: 2.0248 - val_accuracy: 0.3220\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 116s 453ms/step - loss: 1.8114 - accuracy: 0.3986 - val_loss: 2.0301 - val_accuracy: 0.3202\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 116s 455ms/step - loss: 1.8072 - accuracy: 0.4014 - val_loss: 2.0381 - val_accuracy: 0.3166\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 118s 461ms/step - loss: 1.8108 - accuracy: 0.3986 - val_loss: 2.0367 - val_accuracy: 0.3205\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 115s 451ms/step - loss: 1.8114 - accuracy: 0.3954 - val_loss: 2.0519 - val_accuracy: 0.3191\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 112s 436ms/step - loss: 1.8022 - accuracy: 0.4017 - val_loss: 2.0403 - val_accuracy: 0.3188\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 116s 452ms/step - loss: 1.7942 - accuracy: 0.4039 - val_loss: 2.0679 - val_accuracy: 0.3166\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 117s 458ms/step - loss: 1.7749 - accuracy: 0.4087 - val_loss: 2.0844 - val_accuracy: 0.3157\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 120s 467ms/step - loss: 1.7595 - accuracy: 0.4158 - val_loss: 2.0948 - val_accuracy: 0.3140\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 121s 471ms/step - loss: 1.7523 - accuracy: 0.4189 - val_loss: 2.1075 - val_accuracy: 0.3100\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 118s 462ms/step - loss: 1.7503 - accuracy: 0.4203 - val_loss: 2.0928 - val_accuracy: 0.3125\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 117s 455ms/step - loss: 1.7462 - accuracy: 0.4226 - val_loss: 2.1046 - val_accuracy: 0.3160\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 115s 450ms/step - loss: 1.7282 - accuracy: 0.4300 - val_loss: 2.1170 - val_accuracy: 0.3145\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 116s 452ms/step - loss: 1.7189 - accuracy: 0.4325 - val_loss: 2.1121 - val_accuracy: 0.3105\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 117s 455ms/step - loss: 1.7159 - accuracy: 0.4341 - val_loss: 2.1042 - val_accuracy: 0.3089\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 116s 454ms/step - loss: 1.7129 - accuracy: 0.4362 - val_loss: 2.1205 - val_accuracy: 0.3037\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 115s 449ms/step - loss: 1.7088 - accuracy: 0.4342 - val_loss: 2.1128 - val_accuracy: 0.3082\n",
            "80/80 [==============================] - 27s 340ms/step - loss: 2.1235 - accuracy: 0.3088\n",
            "[2.1234850883483887, 0.3087500035762787]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix4waUDRqQ-s",
        "outputId": "0708df06-7452-4002-d10d-664f99bbed3f"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 127s 481ms/step - loss: 2.2482 - accuracy: 0.2219 - val_loss: 2.0843 - val_accuracy: 0.2706\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 120s 468ms/step - loss: 2.0698 - accuracy: 0.2834 - val_loss: 2.0340 - val_accuracy: 0.2988\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 122s 477ms/step - loss: 2.0295 - accuracy: 0.3017 - val_loss: 2.0225 - val_accuracy: 0.3049\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 122s 475ms/step - loss: 2.0094 - accuracy: 0.3120 - val_loss: 2.0262 - val_accuracy: 0.3031\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 125s 487ms/step - loss: 1.9909 - accuracy: 0.3210 - val_loss: 2.0199 - val_accuracy: 0.3078\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 125s 487ms/step - loss: 1.9724 - accuracy: 0.3237 - val_loss: 2.0170 - val_accuracy: 0.3111\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 123s 480ms/step - loss: 1.9547 - accuracy: 0.3314 - val_loss: 2.0106 - val_accuracy: 0.3126\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 123s 479ms/step - loss: 1.9353 - accuracy: 0.3391 - val_loss: 2.0154 - val_accuracy: 0.3088\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 124s 485ms/step - loss: 1.9194 - accuracy: 0.3465 - val_loss: 2.0139 - val_accuracy: 0.3083\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 124s 485ms/step - loss: 1.9010 - accuracy: 0.3535 - val_loss: 2.0209 - val_accuracy: 0.3058\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 125s 487ms/step - loss: 1.8833 - accuracy: 0.3577 - val_loss: 2.0246 - val_accuracy: 0.3042\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 122s 476ms/step - loss: 1.8685 - accuracy: 0.3622 - val_loss: 2.0418 - val_accuracy: 0.3020\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 121s 472ms/step - loss: 1.8480 - accuracy: 0.3682 - val_loss: 2.0201 - val_accuracy: 0.3052\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 123s 481ms/step - loss: 1.8337 - accuracy: 0.3747 - val_loss: 2.0235 - val_accuracy: 0.3043\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 123s 479ms/step - loss: 1.8169 - accuracy: 0.3811 - val_loss: 2.0326 - val_accuracy: 0.3045\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 122s 475ms/step - loss: 1.7956 - accuracy: 0.3871 - val_loss: 2.0409 - val_accuracy: 0.3071\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 120s 466ms/step - loss: 1.7779 - accuracy: 0.3923 - val_loss: 2.0526 - val_accuracy: 0.3063\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 122s 474ms/step - loss: 1.7642 - accuracy: 0.3964 - val_loss: 2.0569 - val_accuracy: 0.3060\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 129s 502ms/step - loss: 1.7439 - accuracy: 0.4050 - val_loss: 2.0519 - val_accuracy: 0.3085\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 146s 568ms/step - loss: 1.7365 - accuracy: 0.4083 - val_loss: 2.0666 - val_accuracy: 0.3071\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 150s 584ms/step - loss: 1.7199 - accuracy: 0.4160 - val_loss: 2.0810 - val_accuracy: 0.3034\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 144s 560ms/step - loss: 1.6983 - accuracy: 0.4201 - val_loss: 2.1003 - val_accuracy: 0.3035\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 128s 498ms/step - loss: 1.6829 - accuracy: 0.4275 - val_loss: 2.1089 - val_accuracy: 0.2997\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 145s 567ms/step - loss: 1.6598 - accuracy: 0.4330 - val_loss: 2.1366 - val_accuracy: 0.2988\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 140s 547ms/step - loss: 1.6479 - accuracy: 0.4365 - val_loss: 2.1476 - val_accuracy: 0.2963\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 144s 561ms/step - loss: 1.6335 - accuracy: 0.4431 - val_loss: 2.1568 - val_accuracy: 0.2965\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 138s 538ms/step - loss: 1.6165 - accuracy: 0.4535 - val_loss: 2.1795 - val_accuracy: 0.2912\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 135s 528ms/step - loss: 1.6119 - accuracy: 0.4537 - val_loss: 2.1807 - val_accuracy: 0.2972\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 137s 534ms/step - loss: 1.5896 - accuracy: 0.4602 - val_loss: 2.2176 - val_accuracy: 0.2898\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 141s 552ms/step - loss: 1.5675 - accuracy: 0.4688 - val_loss: 2.2306 - val_accuracy: 0.2935\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 138s 538ms/step - loss: 1.5548 - accuracy: 0.4750 - val_loss: 2.2597 - val_accuracy: 0.2908\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 133s 518ms/step - loss: 1.5416 - accuracy: 0.4810 - val_loss: 2.2744 - val_accuracy: 0.2878\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 123s 478ms/step - loss: 1.5254 - accuracy: 0.4893 - val_loss: 2.2996 - val_accuracy: 0.2895\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 121s 471ms/step - loss: 1.5065 - accuracy: 0.4975 - val_loss: 2.3189 - val_accuracy: 0.2902\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 122s 476ms/step - loss: 1.4896 - accuracy: 0.4994 - val_loss: 2.3548 - val_accuracy: 0.2878\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 124s 485ms/step - loss: 1.4748 - accuracy: 0.5047 - val_loss: 2.3628 - val_accuracy: 0.2843\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 135s 527ms/step - loss: 1.4621 - accuracy: 0.5122 - val_loss: 2.3778 - val_accuracy: 0.2831\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 141s 549ms/step - loss: 1.4625 - accuracy: 0.5085 - val_loss: 2.3940 - val_accuracy: 0.2806\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 140s 546ms/step - loss: 1.4362 - accuracy: 0.5207 - val_loss: 2.4136 - val_accuracy: 0.2808\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 138s 537ms/step - loss: 1.4251 - accuracy: 0.5213 - val_loss: 2.4270 - val_accuracy: 0.2831\n",
            "80/80 [==============================] - 29s 372ms/step - loss: 2.4463 - accuracy: 0.2803\n",
            "[2.446263074874878, 0.28025001287460327]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-YHLgtPqQ8d",
        "outputId": "ae2f64d0-c320-4835-847e-36cf6e6c9040"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 85s 329ms/step - loss: 2.2495 - accuracy: 0.1966 - val_loss: 2.1409 - val_accuracy: 0.2188\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 78s 306ms/step - loss: 2.1476 - accuracy: 0.2136 - val_loss: 2.1381 - val_accuracy: 0.2182\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 82s 322ms/step - loss: 2.1487 - accuracy: 0.2182 - val_loss: 2.1385 - val_accuracy: 0.2185\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 81s 318ms/step - loss: 2.1471 - accuracy: 0.2169 - val_loss: 2.1389 - val_accuracy: 0.2180\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 80s 310ms/step - loss: 2.1481 - accuracy: 0.2149 - val_loss: 2.1404 - val_accuracy: 0.2208\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 81s 315ms/step - loss: 2.1478 - accuracy: 0.2221 - val_loss: 2.1348 - val_accuracy: 0.2342\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 82s 321ms/step - loss: 2.1404 - accuracy: 0.2221 - val_loss: 2.1349 - val_accuracy: 0.2269\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 82s 321ms/step - loss: 2.1360 - accuracy: 0.2262 - val_loss: 2.1445 - val_accuracy: 0.2108\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 87s 338ms/step - loss: 2.1287 - accuracy: 0.2293 - val_loss: 2.1377 - val_accuracy: 0.2300\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 81s 317ms/step - loss: 2.1269 - accuracy: 0.2394 - val_loss: 2.1247 - val_accuracy: 0.2378\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 79s 309ms/step - loss: 2.1199 - accuracy: 0.2418 - val_loss: 2.1188 - val_accuracy: 0.2395\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 76s 296ms/step - loss: 2.1128 - accuracy: 0.2437 - val_loss: 2.1168 - val_accuracy: 0.2412\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 76s 298ms/step - loss: 2.0995 - accuracy: 0.2525 - val_loss: 2.1090 - val_accuracy: 0.2374\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 77s 301ms/step - loss: 2.0915 - accuracy: 0.2569 - val_loss: 2.1082 - val_accuracy: 0.2437\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 80s 311ms/step - loss: 2.0849 - accuracy: 0.2597 - val_loss: 2.1041 - val_accuracy: 0.2386\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 77s 301ms/step - loss: 2.0710 - accuracy: 0.2699 - val_loss: 2.1195 - val_accuracy: 0.2398\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 78s 305ms/step - loss: 2.0642 - accuracy: 0.2730 - val_loss: 2.1378 - val_accuracy: 0.2202\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 79s 308ms/step - loss: 2.0570 - accuracy: 0.2750 - val_loss: 2.1341 - val_accuracy: 0.2246\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 83s 324ms/step - loss: 2.0475 - accuracy: 0.2794 - val_loss: 2.1232 - val_accuracy: 0.2368\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 79s 308ms/step - loss: 2.0444 - accuracy: 0.2846 - val_loss: 2.1119 - val_accuracy: 0.2491\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 79s 310ms/step - loss: 2.0382 - accuracy: 0.2893 - val_loss: 2.1174 - val_accuracy: 0.2463\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 81s 317ms/step - loss: 2.0326 - accuracy: 0.2935 - val_loss: 2.1171 - val_accuracy: 0.2478\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 83s 323ms/step - loss: 2.0263 - accuracy: 0.2980 - val_loss: 2.1204 - val_accuracy: 0.2483\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 80s 311ms/step - loss: 2.0129 - accuracy: 0.3038 - val_loss: 2.1302 - val_accuracy: 0.2505\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 81s 317ms/step - loss: 2.0038 - accuracy: 0.3080 - val_loss: 2.1500 - val_accuracy: 0.2434\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 81s 317ms/step - loss: 1.9978 - accuracy: 0.3103 - val_loss: 2.1553 - val_accuracy: 0.2465\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 89s 349ms/step - loss: 1.9979 - accuracy: 0.3116 - val_loss: 2.1551 - val_accuracy: 0.2454\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 81s 316ms/step - loss: 1.9842 - accuracy: 0.3173 - val_loss: 2.1559 - val_accuracy: 0.2468\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 79s 308ms/step - loss: 1.9799 - accuracy: 0.3250 - val_loss: 2.1614 - val_accuracy: 0.2414\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 78s 306ms/step - loss: 1.9759 - accuracy: 0.3280 - val_loss: 2.1689 - val_accuracy: 0.2458\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 82s 318ms/step - loss: 1.9769 - accuracy: 0.3272 - val_loss: 2.1589 - val_accuracy: 0.2469\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 80s 313ms/step - loss: 1.9668 - accuracy: 0.3273 - val_loss: 2.1921 - val_accuracy: 0.2377\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 80s 313ms/step - loss: 1.9604 - accuracy: 0.3266 - val_loss: 2.1663 - val_accuracy: 0.2457\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 79s 309ms/step - loss: 1.9624 - accuracy: 0.3279 - val_loss: 2.1648 - val_accuracy: 0.2478\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 81s 317ms/step - loss: 1.9614 - accuracy: 0.3264 - val_loss: 2.1714 - val_accuracy: 0.2471\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 78s 303ms/step - loss: 1.9475 - accuracy: 0.3330 - val_loss: 2.1811 - val_accuracy: 0.2435\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 79s 308ms/step - loss: 1.9398 - accuracy: 0.3372 - val_loss: 2.1903 - val_accuracy: 0.2420\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 82s 320ms/step - loss: 1.9404 - accuracy: 0.3355 - val_loss: 2.1983 - val_accuracy: 0.2422\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 83s 323ms/step - loss: 1.9346 - accuracy: 0.3371 - val_loss: 2.1911 - val_accuracy: 0.2489\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 77s 302ms/step - loss: 1.9287 - accuracy: 0.3353 - val_loss: 2.1960 - val_accuracy: 0.2442\n",
            "80/80 [==============================] - 18s 232ms/step - loss: 2.2256 - accuracy: 0.2435\n",
            "[2.2255759239196777, 0.2434999942779541]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJi4HR3eqQ4-",
        "outputId": "0c6f5d75-c169-4076-b516-82521f95aac5"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 19s 65ms/step - loss: 2.2313 - accuracy: 0.2329 - val_loss: 2.0119 - val_accuracy: 0.2937\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 16s 62ms/step - loss: 1.9883 - accuracy: 0.3079 - val_loss: 1.9293 - val_accuracy: 0.3334\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.9153 - accuracy: 0.3393 - val_loss: 1.9032 - val_accuracy: 0.3440\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.8743 - accuracy: 0.3565 - val_loss: 1.8859 - val_accuracy: 0.3494\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.8481 - accuracy: 0.3665 - val_loss: 1.8841 - val_accuracy: 0.3475\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.8313 - accuracy: 0.3714 - val_loss: 1.8804 - val_accuracy: 0.3495\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.8057 - accuracy: 0.3796 - val_loss: 1.8777 - val_accuracy: 0.3491\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 17s 65ms/step - loss: 1.7804 - accuracy: 0.3856 - val_loss: 1.8782 - val_accuracy: 0.3480\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.7585 - accuracy: 0.3924 - val_loss: 1.8741 - val_accuracy: 0.3535\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.7398 - accuracy: 0.4026 - val_loss: 1.8782 - val_accuracy: 0.3546\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 17s 65ms/step - loss: 1.7201 - accuracy: 0.4100 - val_loss: 1.8926 - val_accuracy: 0.3523\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.7057 - accuracy: 0.4142 - val_loss: 1.8908 - val_accuracy: 0.3528\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.6864 - accuracy: 0.4190 - val_loss: 1.9071 - val_accuracy: 0.3474\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.6669 - accuracy: 0.4306 - val_loss: 1.9195 - val_accuracy: 0.3449\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.6479 - accuracy: 0.4362 - val_loss: 1.9360 - val_accuracy: 0.3448\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.6259 - accuracy: 0.4433 - val_loss: 1.9498 - val_accuracy: 0.3483\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.6118 - accuracy: 0.4498 - val_loss: 1.9662 - val_accuracy: 0.3414\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.5954 - accuracy: 0.4564 - val_loss: 1.9762 - val_accuracy: 0.3375\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.5778 - accuracy: 0.4637 - val_loss: 2.0121 - val_accuracy: 0.3348\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.5632 - accuracy: 0.4681 - val_loss: 2.0207 - val_accuracy: 0.3351\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.5530 - accuracy: 0.4741 - val_loss: 2.0558 - val_accuracy: 0.3366\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.5474 - accuracy: 0.4750 - val_loss: 2.0718 - val_accuracy: 0.3323\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.5403 - accuracy: 0.4777 - val_loss: 2.0902 - val_accuracy: 0.3200\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.5297 - accuracy: 0.4801 - val_loss: 2.0787 - val_accuracy: 0.3282\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.5108 - accuracy: 0.4864 - val_loss: 2.0767 - val_accuracy: 0.3195\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.4938 - accuracy: 0.4943 - val_loss: 2.1086 - val_accuracy: 0.3202\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 17s 65ms/step - loss: 1.5126 - accuracy: 0.4882 - val_loss: 2.1112 - val_accuracy: 0.3200\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.4785 - accuracy: 0.5001 - val_loss: 2.1244 - val_accuracy: 0.3180\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.4718 - accuracy: 0.5034 - val_loss: 2.1500 - val_accuracy: 0.3157\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 17s 65ms/step - loss: 1.4594 - accuracy: 0.5075 - val_loss: 2.1725 - val_accuracy: 0.3123\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 17s 65ms/step - loss: 1.4722 - accuracy: 0.5021 - val_loss: 2.1541 - val_accuracy: 0.3206\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.4699 - accuracy: 0.5061 - val_loss: 2.1864 - val_accuracy: 0.3191\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.4438 - accuracy: 0.5176 - val_loss: 2.2159 - val_accuracy: 0.3128\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.4249 - accuracy: 0.5226 - val_loss: 2.2159 - val_accuracy: 0.3160\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 16s 64ms/step - loss: 1.4078 - accuracy: 0.5275 - val_loss: 2.2367 - val_accuracy: 0.3168\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 16s 63ms/step - loss: 1.4022 - accuracy: 0.5276 - val_loss: 2.2537 - val_accuracy: 0.3055\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 17s 64ms/step - loss: 1.3775 - accuracy: 0.5349 - val_loss: 2.3162 - val_accuracy: 0.2983\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 17s 64ms/step - loss: 1.3721 - accuracy: 0.5423 - val_loss: 2.2782 - val_accuracy: 0.3138\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 17s 64ms/step - loss: 1.3660 - accuracy: 0.5427 - val_loss: 2.3050 - val_accuracy: 0.3088\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 17s 65ms/step - loss: 1.3652 - accuracy: 0.5428 - val_loss: 2.2951 - val_accuracy: 0.3055\n",
            "80/80 [==============================] - 3s 31ms/step - loss: 2.3533 - accuracy: 0.3030\n",
            "[2.353306770324707, 0.30300000309944153]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rYX8zohqQ2h",
        "outputId": "46a219d5-f812-4128-e167-872e88a0852c"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "257/257 [==============================] - 31s 108ms/step - loss: 2.1427 - accuracy: 0.2607 - val_loss: 1.9208 - val_accuracy: 0.3405\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 27s 104ms/step - loss: 1.8999 - accuracy: 0.3496 - val_loss: 1.8711 - val_accuracy: 0.3562\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 27s 107ms/step - loss: 1.8427 - accuracy: 0.3700 - val_loss: 1.8541 - val_accuracy: 0.3620\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.8078 - accuracy: 0.3821 - val_loss: 1.8446 - val_accuracy: 0.3686\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.7763 - accuracy: 0.3935 - val_loss: 1.8457 - val_accuracy: 0.3683\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.7411 - accuracy: 0.4045 - val_loss: 1.8539 - val_accuracy: 0.3678\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.7049 - accuracy: 0.4131 - val_loss: 1.8662 - val_accuracy: 0.3623\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.6637 - accuracy: 0.4288 - val_loss: 1.8836 - val_accuracy: 0.3571\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.6251 - accuracy: 0.4434 - val_loss: 1.8970 - val_accuracy: 0.3545\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.5852 - accuracy: 0.4589 - val_loss: 1.9150 - val_accuracy: 0.3532\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.5504 - accuracy: 0.4736 - val_loss: 1.9415 - val_accuracy: 0.3465\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.5083 - accuracy: 0.4913 - val_loss: 1.9694 - val_accuracy: 0.3414\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 1.4676 - accuracy: 0.5061 - val_loss: 2.0015 - val_accuracy: 0.3394\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.4346 - accuracy: 0.5172 - val_loss: 2.0381 - val_accuracy: 0.3357\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 27s 107ms/step - loss: 1.4055 - accuracy: 0.5278 - val_loss: 2.0716 - val_accuracy: 0.3323\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.3665 - accuracy: 0.5373 - val_loss: 2.1016 - val_accuracy: 0.3308\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 27s 107ms/step - loss: 1.3357 - accuracy: 0.5491 - val_loss: 2.1300 - val_accuracy: 0.3255\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 28s 111ms/step - loss: 1.3003 - accuracy: 0.5644 - val_loss: 2.1502 - val_accuracy: 0.3300\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 28s 108ms/step - loss: 1.2657 - accuracy: 0.5764 - val_loss: 2.1888 - val_accuracy: 0.3229\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 28s 108ms/step - loss: 1.2432 - accuracy: 0.5845 - val_loss: 2.2368 - val_accuracy: 0.3223\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 28s 107ms/step - loss: 1.2212 - accuracy: 0.5893 - val_loss: 2.2683 - val_accuracy: 0.3174\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.1914 - accuracy: 0.6035 - val_loss: 2.3191 - val_accuracy: 0.3129\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.1490 - accuracy: 0.6182 - val_loss: 2.3688 - val_accuracy: 0.3083\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.1181 - accuracy: 0.6295 - val_loss: 2.4340 - val_accuracy: 0.3060\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 1.0886 - accuracy: 0.6423 - val_loss: 2.4752 - val_accuracy: 0.3017\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 28s 108ms/step - loss: 1.0646 - accuracy: 0.6528 - val_loss: 2.4939 - val_accuracy: 0.3000\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 27s 107ms/step - loss: 1.0251 - accuracy: 0.6664 - val_loss: 2.5644 - val_accuracy: 0.2940\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 28s 107ms/step - loss: 0.9949 - accuracy: 0.6782 - val_loss: 2.6315 - val_accuracy: 0.2940\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 28s 108ms/step - loss: 0.9716 - accuracy: 0.6861 - val_loss: 2.6850 - val_accuracy: 0.2920\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 28s 108ms/step - loss: 0.9486 - accuracy: 0.6946 - val_loss: 2.7301 - val_accuracy: 0.2931\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 28s 108ms/step - loss: 0.9341 - accuracy: 0.7020 - val_loss: 2.7903 - val_accuracy: 0.2943\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 27s 107ms/step - loss: 0.9016 - accuracy: 0.7127 - val_loss: 2.8260 - val_accuracy: 0.2882\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 27s 107ms/step - loss: 0.8892 - accuracy: 0.7164 - val_loss: 2.8879 - val_accuracy: 0.2895\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 0.8683 - accuracy: 0.7216 - val_loss: 2.8923 - val_accuracy: 0.2849\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 0.8381 - accuracy: 0.7319 - val_loss: 2.9930 - val_accuracy: 0.2858\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 0.8204 - accuracy: 0.7384 - val_loss: 3.0468 - val_accuracy: 0.2832\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 0.8152 - accuracy: 0.7395 - val_loss: 3.0944 - val_accuracy: 0.2822\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 0.8012 - accuracy: 0.7466 - val_loss: 3.0951 - val_accuracy: 0.2857\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 27s 105ms/step - loss: 0.7881 - accuracy: 0.7518 - val_loss: 3.1422 - val_accuracy: 0.2794\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 27s 106ms/step - loss: 0.8207 - accuracy: 0.7366 - val_loss: 3.0906 - val_accuracy: 0.2863\n",
            "80/80 [==============================] - 4s 47ms/step - loss: 3.0986 - accuracy: 0.2849\n",
            "[3.0986132621765137, 0.2848750054836273]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iFpUpYsJqQ0V",
        "outputId": "3ebeb3ef-7bc1-4f64-8c9f-93c8733ec186"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(13, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 9s 32ms/step - loss: 2.2394 - accuracy: 0.2032 - val_loss: 2.1386 - val_accuracy: 0.2128\n",
            "Epoch 2/40\n",
            "257/257 [==============================] - 7s 28ms/step - loss: 2.1347 - accuracy: 0.2432 - val_loss: 2.1070 - val_accuracy: 0.2729\n",
            "Epoch 3/40\n",
            "257/257 [==============================] - 7s 29ms/step - loss: 2.1065 - accuracy: 0.2617 - val_loss: 2.0735 - val_accuracy: 0.2538\n",
            "Epoch 4/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.0941 - accuracy: 0.2368 - val_loss: 2.1413 - val_accuracy: 0.2148\n",
            "Epoch 5/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1446 - accuracy: 0.2150 - val_loss: 2.1390 - val_accuracy: 0.2151\n",
            "Epoch 6/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1422 - accuracy: 0.2207 - val_loss: 2.1382 - val_accuracy: 0.2140\n",
            "Epoch 7/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1426 - accuracy: 0.2191 - val_loss: 2.1396 - val_accuracy: 0.2134\n",
            "Epoch 8/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1398 - accuracy: 0.2251 - val_loss: 2.1378 - val_accuracy: 0.2148\n",
            "Epoch 9/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1359 - accuracy: 0.2367 - val_loss: 2.1307 - val_accuracy: 0.2460\n",
            "Epoch 10/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1251 - accuracy: 0.2504 - val_loss: 2.1181 - val_accuracy: 0.2565\n",
            "Epoch 11/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.1070 - accuracy: 0.2581 - val_loss: 2.0861 - val_accuracy: 0.2622\n",
            "Epoch 12/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.0620 - accuracy: 0.2808 - val_loss: 2.0479 - val_accuracy: 0.2755\n",
            "Epoch 13/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 2.0195 - accuracy: 0.3000 - val_loss: 2.0520 - val_accuracy: 0.2802\n",
            "Epoch 14/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9943 - accuracy: 0.3139 - val_loss: 2.0319 - val_accuracy: 0.2832\n",
            "Epoch 15/40\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 1.9718 - accuracy: 0.3236 - val_loss: 2.0085 - val_accuracy: 0.3038\n",
            "Epoch 16/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9513 - accuracy: 0.3332 - val_loss: 2.0116 - val_accuracy: 0.3048\n",
            "Epoch 17/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9419 - accuracy: 0.3356 - val_loss: 2.0245 - val_accuracy: 0.2991\n",
            "Epoch 18/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9327 - accuracy: 0.3364 - val_loss: 2.0251 - val_accuracy: 0.3008\n",
            "Epoch 19/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9262 - accuracy: 0.3374 - val_loss: 2.0342 - val_accuracy: 0.2952\n",
            "Epoch 20/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9118 - accuracy: 0.3456 - val_loss: 2.0532 - val_accuracy: 0.2915\n",
            "Epoch 21/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.9069 - accuracy: 0.3510 - val_loss: 2.0531 - val_accuracy: 0.2946\n",
            "Epoch 22/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8999 - accuracy: 0.3515 - val_loss: 2.0599 - val_accuracy: 0.2912\n",
            "Epoch 23/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8912 - accuracy: 0.3585 - val_loss: 2.0857 - val_accuracy: 0.2912\n",
            "Epoch 24/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8872 - accuracy: 0.3586 - val_loss: 2.0794 - val_accuracy: 0.2914\n",
            "Epoch 25/40\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 1.8850 - accuracy: 0.3608 - val_loss: 2.0866 - val_accuracy: 0.2909\n",
            "Epoch 26/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8827 - accuracy: 0.3611 - val_loss: 2.0871 - val_accuracy: 0.2940\n",
            "Epoch 27/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8903 - accuracy: 0.3591 - val_loss: 2.0488 - val_accuracy: 0.3040\n",
            "Epoch 28/40\n",
            "257/257 [==============================] - 7s 27ms/step - loss: 1.8739 - accuracy: 0.3662 - val_loss: 2.0380 - val_accuracy: 0.3058\n",
            "Epoch 29/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8537 - accuracy: 0.3745 - val_loss: 2.0522 - val_accuracy: 0.3012\n",
            "Epoch 30/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8416 - accuracy: 0.3793 - val_loss: 2.0637 - val_accuracy: 0.2995\n",
            "Epoch 31/40\n",
            "257/257 [==============================] - 7s 29ms/step - loss: 1.8311 - accuracy: 0.3848 - val_loss: 2.0810 - val_accuracy: 0.2920\n",
            "Epoch 32/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8389 - accuracy: 0.3818 - val_loss: 2.1138 - val_accuracy: 0.2791\n",
            "Epoch 33/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8347 - accuracy: 0.3842 - val_loss: 2.1317 - val_accuracy: 0.2712\n",
            "Epoch 34/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8409 - accuracy: 0.3771 - val_loss: 2.1601 - val_accuracy: 0.2623\n",
            "Epoch 35/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8547 - accuracy: 0.3730 - val_loss: 2.1357 - val_accuracy: 0.2722\n",
            "Epoch 36/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8301 - accuracy: 0.3797 - val_loss: 2.1034 - val_accuracy: 0.2823\n",
            "Epoch 37/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.8082 - accuracy: 0.3948 - val_loss: 2.1028 - val_accuracy: 0.2838\n",
            "Epoch 38/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.7988 - accuracy: 0.3986 - val_loss: 2.1123 - val_accuracy: 0.2789\n",
            "Epoch 39/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.7871 - accuracy: 0.4009 - val_loss: 2.1160 - val_accuracy: 0.2862\n",
            "Epoch 40/40\n",
            "257/257 [==============================] - 7s 26ms/step - loss: 1.7960 - accuracy: 0.3993 - val_loss: 2.1261 - val_accuracy: 0.2883\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 2.1448 - accuracy: 0.2865\n",
            "[2.1448333263397217, 0.2865000069141388]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51aAWN2_qjv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8fada0a-be3d-4522-dce3-9636408b0d09"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "# print(history_sg_bi.history)\r\n",
        "# plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()\r\n",
        "# plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0, 0, 0]\n",
            "cbow [0, 0, 0.2434999942779541]\n",
            "glove [0.30300000309944153, 0.2848750054836273, 0.2865000069141388]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}