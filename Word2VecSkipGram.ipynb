{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2VecSkipGram.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmeW3Ow/y4RXTLkP+QrpTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/Word2VecSkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59gh-dKacc5I",
        "outputId": "fcf4a4ec-4fef-47a6-8f97-86ba9f6ab53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df_train = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/Data/Train/clean_all_train.csv')\n",
        "\n",
        "def generate_dictionary_data(text):\n",
        "  word_to_index= dict()\n",
        "  index_to_word = dict()\n",
        "  corpus = []\n",
        "  count = 0\n",
        "  vocab_size = 0    \n",
        "  for row in text:\n",
        "    for word in row.split():\n",
        "      corpus.append(word)\n",
        "      if word_to_index.get(word) == None:\n",
        "        word_to_index.update ( {word : count})\n",
        "        index_to_word.update ( {count : word })\n",
        "        count  += 1\n",
        "      vocab_size = len(word_to_index)\n",
        "      length_of_corpus = len(corpus)\n",
        "  return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus\n",
        "\n",
        "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index):\n",
        "  trgt_word_vector = np.zeros(vocab_size)  \n",
        "  index_of_word_dictionary = word_to_index.get(target_word) \n",
        "  trgt_word_vector[index_of_word_dictionary] = 1  \n",
        "  ctxt_word_vector = np.zeros(vocab_size)\n",
        "  for word in context_words:\n",
        "    index_of_word_dictionary = word_to_index.get(word) \n",
        "    ctxt_word_vector[index_of_word_dictionary] = 1    \n",
        "  return trgt_word_vector,ctxt_word_vector\n",
        "\n",
        "def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,batch_size):\n",
        "  while True:\n",
        "    for k in range(int(length_of_corpus/batch_size)):\n",
        "      training_data_target =  []\n",
        "      training_data_context =  []\n",
        "      for i,word in enumerate(corpus[batch_size*k:min(len(corpus),(batch_size*(k+1)))]):\n",
        "        cor = corpus[batch_size*k:min(len(corpus),(batch_size*(k+1)))]\n",
        "        length_of_cor = len(cor)\n",
        "        index_target_word = i\n",
        "        target_word = word\n",
        "        context_words = []\n",
        "        #when target word is the first word\n",
        "        if i == 0:  \n",
        "          # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
        "          context_words = [cor[x] for x in range(i + 1 , window_size + 1)] \n",
        "        #when target word is the last word\n",
        "        elif i == length_of_cor-1:\n",
        "          # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
        "          context_words = [cor[x] for x in range(length_of_cor - 2 ,length_of_cor -2 - window_size  , -1 )]\n",
        "        #When target word is the middle word\n",
        "        else:\n",
        "          #Before the middle target word\n",
        "          before_target_word_index = index_target_word - 1\n",
        "          for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
        "            if x >=0:\n",
        "              context_words.extend([cor[x]])\n",
        "          #After the middle target word\n",
        "          after_target_word_index = index_target_word + 1\n",
        "          for x in range(after_target_word_index, after_target_word_index + window_size):\n",
        "            if x < length_of_cor:\n",
        "              context_words.extend([cor[x]])\n",
        "        trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index)\n",
        "        training_data_target.append(trgt_word_vector)\n",
        "        training_data_context.append(ctxt_word_vector)\n",
        "      yield (np.array(training_data_target),np.array(training_data_context))\n",
        "      del training_data_target\n",
        "      del training_data_context\n",
        "\n",
        "\n",
        "def calculate_loss(actual,predicted):\n",
        "  sum_1 = tf.math.reduce_sum((tf.math.multiply(actual,predicted)))\n",
        "  sum_2 = tf.math.multiply(tf.math.reduce_sum(actual),tf.math.sqrt(tf.math.reduce_sum(tf.math.multiply(predicted,predicted))))\n",
        "  return tf.math.subtract(sum_2,sum_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy92onhScdjf",
        "outputId": "e1f8355e-ccd6-47a3-ca0f-192d18bf0bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#\n",
        "epochs_num = 5\n",
        "window_size = 3\n",
        "embedding_dimensions = 128\n",
        "batch_size = 16\n",
        "#\n",
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictionary_data(df_train['Tweets'])\n",
        "print(vocab_size)\n",
        "print(len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10220\n",
            "58800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOZKUZLOdHLr",
        "outputId": "57e8ca07-0dc8-4357-b76b-da24eb8255e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model = keras.Sequential([keras.layers.Dense(embedding_dimensions),\n",
        "                          keras.layers.Dense(vocab_size,activation = 'softmax')])\n",
        "model.compile(optimizer = \"adam\", loss = calculate_loss,metrics=[\"accuracy\"])\n",
        "model.fit(x = generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,batch_size),steps_per_epoch=int(length_of_corpus/batch_size),epochs = epochs_num)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3675/3675 [==============================] - 16s 4ms/step - loss: 3.2785 - accuracy: 0.1451\n",
            "Epoch 2/5\n",
            "3675/3675 [==============================] - 16s 4ms/step - loss: 3.2781 - accuracy: 0.2046\n",
            "Epoch 3/5\n",
            "3675/3675 [==============================] - 16s 4ms/step - loss: 3.2778 - accuracy: 0.2329\n",
            "Epoch 4/5\n",
            "3675/3675 [==============================] - 16s 4ms/step - loss: 3.2775 - accuracy: 0.2318\n",
            "Epoch 5/5\n",
            "3675/3675 [==============================] - 16s 4ms/step - loss: 3.2773 - accuracy: 0.2263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f83ae135470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxzYpxh2ZbCY"
      },
      "source": [
        "def make_dict(index_to_word,weight_matrix):\n",
        "  dict = {}\n",
        "  for index,i in enumerate(weight_matrix):\n",
        "    dict[index_to_word[index]] = i\n",
        "  return dict\n",
        "dict = make_dict(index_to_word,model.get_weights()[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdZXaP60SE_m"
      },
      "source": [
        "df = pd.DataFrame.from_dict(dict, orient=\"index\")\n",
        "df.to_csv(\"/content/drive/My Drive/InfluenceAnalysis/Word2VecSG/Dictionary.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU5mU-lObiCr"
      },
      "source": [
        "def find_similar_20(word,dict):\n",
        "  l = []\n",
        "  word_val = {}\n",
        "  if word in dict:\n",
        "    for i in dict:\n",
        "      word_val[i] = sum([x*y for x,y in zip(dict[i],dict[word])])\n",
        "    word_val = {k: v for k, v in sorted(word_val.items(), key=lambda item: item[1],reverse=True)}\n",
        "  c = 0\n",
        "  for i in word_val:\n",
        "    c += 1\n",
        "    if c==21:\n",
        "      break\n",
        "    l.append(i)\n",
        "  return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSqq9VcadZcC",
        "outputId": "5732ece5-e481-47fe-ffb6-bb0f11ad4b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "find_similar_20('sad',dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['andry',\n",
              " 'Frown',\n",
              " 'SL8',\n",
              " 'pouting',\n",
              " 'anxious',\n",
              " 'sad',\n",
              " 'geuly',\n",
              " 'nill',\n",
              " 'melbjs',\n",
              " 'realising',\n",
              " 'Wanna',\n",
              " 'Slash',\n",
              " 'Political',\n",
              " 'hsm2',\n",
              " 'talked',\n",
              " 'Following',\n",
              " 'stamped',\n",
              " 'smth',\n",
              " 'episodesFrown',\n",
              " 'Timeless']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}