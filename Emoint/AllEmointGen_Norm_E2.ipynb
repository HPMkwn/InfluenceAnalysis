{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointGen_Norm_E2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointGen_Norm_E2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d931f080-0815-4a66-8ca6-6660e1325ba1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Emoint/E2.csv')\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjQC2VxK0ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a060fe22-4802-4ac3-f208-5ef22042d517"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'tweets', 'emotion', 'intensity'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {0:[1,0,0,0],1:[0,1,0,0],2:[0,0,1,0],3:[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['tweets']]\n",
        "data_cat = np.array([category_dict[x] for x in df['emotion']])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f91a2a-2d24-4192-be26-56f5732afb6a"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['collect', 'all', 'the', 'time', 'when', 'minerva', 'would', 'chew', 'on', 'hi', 'cape', 'begin', 'to', 'shiver', 'run', 'a', 'hand', 'through', 'hi', 'hair', 'nnyou', 'can', 'surelynardentfli'], ['sanatana', 'lopez', 'is', 'definit', 'me', 'face_with_tears_of_joy'], ['2', 'biggest', 'fear', 'incur', 'std', 'and', 'pregnancyi', 'mean', 'theyr', 'basic', 'the', 'same', 'thing', 'anyway', 'forlif', 'annoy', 'weirdsmel'], ['i', 'feel', 'so', 'bless', 'to', 'work', 'with', 'the', 'famili', 'that', 'i', 'nanni', 'for', 'red_heart', 'noth', 'but', 'love', 'amp', 'appreci', 'make', 'me', 'smile'], ['rover', 'what', 'highlight', 'i', 'would', 'imagin', 'you', 'will', 'onli', 'have', 'second', 'of', 'highlight', 'for', 'the', 'whole', 'season', 'so', 'far', 'venkysout']]\n",
            "[[1 0 0 0]\n",
            " [0 0 1 0]\n",
            " [0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 200\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnPxfLf139kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b5877b-085e-4b2a-bacb-31a15f48d746"
      },
      "source": [
        "print(len(vocab_sg))\r\n",
        "print(len(vocab_cbow))\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13750\n",
            "13750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      temp = np.array([np.array([w2v.wv.get_vector(i) for i in x if i in vocab]) for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]])\r\n",
        "      temp = np.array([np.pad(x.flatten(),(0,Max_input_size*Embedding_size-len(x.flatten()))).reshape(Max_input_size,Embedding_size) for x in temp])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)\r\n",
        "\r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      temp = np.array([np.array([vocab[i] for i in x if i in vocab.keys()]) for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]])\r\n",
        "      temp = np.array([np.pad(x.flatten(),(0,Max_input_size*Embedding_size-len(x.flatten()))).reshape(Max_input_size,Embedding_size) for x in temp])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 60"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94w9X37VzLZZ"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2067f3-5d34-44ce-b350-8f96ed04c030"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(64))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy',f1,precision,recall])\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 37s 101ms/step - loss: 1.3813 - accuracy: 0.3123 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3734 - val_accuracy: 0.3183 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 1.3697 - accuracy: 0.3232 - f1: 0.0027 - precision: 0.0320 - recall: 0.0015 - val_loss: 1.3399 - val_accuracy: 0.3342 - val_f1: 0.0746 - val_precision: 0.6282 - val_recall: 0.0400\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.3400 - accuracy: 0.3515 - f1: 0.0740 - precision: 0.6012 - recall: 0.0409 - val_loss: 1.2928 - val_accuracy: 0.3942 - val_f1: 0.1519 - val_precision: 0.5981 - val_recall: 0.0875\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 1.2574 - accuracy: 0.4296 - f1: 0.2100 - precision: 0.6554 - recall: 0.1270 - val_loss: 1.2818 - val_accuracy: 0.4075 - val_f1: 0.2518 - val_precision: 0.5849 - val_recall: 0.1608\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.2051 - accuracy: 0.4506 - f1: 0.2704 - precision: 0.6902 - recall: 0.1702 - val_loss: 1.2670 - val_accuracy: 0.4158 - val_f1: 0.2590 - val_precision: 0.5759 - val_recall: 0.1675\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 1.1832 - accuracy: 0.4632 - f1: 0.2873 - precision: 0.7005 - recall: 0.1833 - val_loss: 1.2453 - val_accuracy: 0.4150 - val_f1: 0.2599 - val_precision: 0.6327 - val_recall: 0.1642\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.1642 - accuracy: 0.4815 - f1: 0.3038 - precision: 0.7313 - recall: 0.1939 - val_loss: 1.2381 - val_accuracy: 0.4392 - val_f1: 0.2931 - val_precision: 0.6114 - val_recall: 0.1933\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 1.1446 - accuracy: 0.4929 - f1: 0.3229 - precision: 0.7292 - recall: 0.2095 - val_loss: 1.2073 - val_accuracy: 0.4583 - val_f1: 0.3211 - val_precision: 0.6313 - val_recall: 0.2158\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 1.1201 - accuracy: 0.5131 - f1: 0.3600 - precision: 0.7391 - recall: 0.2404 - val_loss: 1.2062 - val_accuracy: 0.4558 - val_f1: 0.3063 - val_precision: 0.6358 - val_recall: 0.2025\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 1.0976 - accuracy: 0.5158 - f1: 0.3768 - precision: 0.7215 - recall: 0.2568 - val_loss: 1.2228 - val_accuracy: 0.4575 - val_f1: 0.3369 - val_precision: 0.6166 - val_recall: 0.2325\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 1.0657 - accuracy: 0.5361 - f1: 0.4098 - precision: 0.7357 - recall: 0.2860 - val_loss: 1.2168 - val_accuracy: 0.4683 - val_f1: 0.3629 - val_precision: 0.6179 - val_recall: 0.2575\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 1.0358 - accuracy: 0.5431 - f1: 0.4396 - precision: 0.7434 - recall: 0.3139 - val_loss: 1.2434 - val_accuracy: 0.4608 - val_f1: 0.3754 - val_precision: 0.5921 - val_recall: 0.2758\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 1.0080 - accuracy: 0.5519 - f1: 0.4635 - precision: 0.7492 - recall: 0.3379 - val_loss: 1.2398 - val_accuracy: 0.4675 - val_f1: 0.3736 - val_precision: 0.5930 - val_recall: 0.2733\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.9940 - accuracy: 0.5658 - f1: 0.4712 - precision: 0.7509 - recall: 0.3459 - val_loss: 1.2108 - val_accuracy: 0.4617 - val_f1: 0.3557 - val_precision: 0.5921 - val_recall: 0.2550\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.9862 - accuracy: 0.5733 - f1: 0.4693 - precision: 0.7634 - recall: 0.3414 - val_loss: 1.2105 - val_accuracy: 0.4758 - val_f1: 0.3878 - val_precision: 0.5903 - val_recall: 0.2892\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.9619 - accuracy: 0.5772 - f1: 0.4881 - precision: 0.7596 - recall: 0.3625 - val_loss: 1.2044 - val_accuracy: 0.4825 - val_f1: 0.3817 - val_precision: 0.6269 - val_recall: 0.2750\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.9385 - accuracy: 0.5837 - f1: 0.4973 - precision: 0.7543 - recall: 0.3740 - val_loss: 1.2195 - val_accuracy: 0.4642 - val_f1: 0.4082 - val_precision: 0.6042 - val_recall: 0.3092\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.9143 - accuracy: 0.6015 - f1: 0.5312 - precision: 0.7383 - recall: 0.4167 - val_loss: 1.2157 - val_accuracy: 0.4792 - val_f1: 0.4101 - val_precision: 0.6138 - val_recall: 0.3083\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.8870 - accuracy: 0.6107 - f1: 0.5487 - precision: 0.7554 - recall: 0.4328 - val_loss: 1.2516 - val_accuracy: 0.4825 - val_f1: 0.4312 - val_precision: 0.6177 - val_recall: 0.3317\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.8462 - accuracy: 0.6280 - f1: 0.5727 - precision: 0.7567 - recall: 0.4620 - val_loss: 1.3052 - val_accuracy: 0.4858 - val_f1: 0.4474 - val_precision: 0.5950 - val_recall: 0.3600\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.8034 - accuracy: 0.6502 - f1: 0.5964 - precision: 0.7604 - recall: 0.4918 - val_loss: 1.3503 - val_accuracy: 0.5042 - val_f1: 0.4638 - val_precision: 0.5769 - val_recall: 0.3883\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.7745 - accuracy: 0.6581 - f1: 0.6211 - precision: 0.7647 - recall: 0.5239 - val_loss: 1.3614 - val_accuracy: 0.5117 - val_f1: 0.4973 - val_precision: 0.5975 - val_recall: 0.4267\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.7466 - accuracy: 0.6780 - f1: 0.6439 - precision: 0.7759 - recall: 0.5517 - val_loss: 1.2927 - val_accuracy: 0.5100 - val_f1: 0.4830 - val_precision: 0.5838 - val_recall: 0.4125\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.7467 - accuracy: 0.6816 - f1: 0.6484 - precision: 0.7806 - recall: 0.5554 - val_loss: 1.3326 - val_accuracy: 0.5042 - val_f1: 0.4832 - val_precision: 0.5821 - val_recall: 0.4133\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.7228 - accuracy: 0.6991 - f1: 0.6536 - precision: 0.7775 - recall: 0.5657 - val_loss: 1.3657 - val_accuracy: 0.5392 - val_f1: 0.5173 - val_precision: 0.5915 - val_recall: 0.4600\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.6696 - accuracy: 0.7120 - f1: 0.6923 - precision: 0.7855 - recall: 0.6200 - val_loss: 1.4467 - val_accuracy: 0.5450 - val_f1: 0.5145 - val_precision: 0.5849 - val_recall: 0.4600\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6624 - accuracy: 0.7127 - f1: 0.7018 - precision: 0.7891 - recall: 0.6328 - val_loss: 1.5322 - val_accuracy: 0.5583 - val_f1: 0.5498 - val_precision: 0.6035 - val_recall: 0.5050\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6810 - accuracy: 0.7221 - f1: 0.7022 - precision: 0.7838 - recall: 0.6368 - val_loss: 1.5059 - val_accuracy: 0.5642 - val_f1: 0.5540 - val_precision: 0.6079 - val_recall: 0.5092\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6367 - accuracy: 0.7404 - f1: 0.7273 - precision: 0.8037 - recall: 0.6647 - val_loss: 1.3054 - val_accuracy: 0.5700 - val_f1: 0.5608 - val_precision: 0.6312 - val_recall: 0.5050\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6057 - accuracy: 0.7522 - f1: 0.7393 - precision: 0.8121 - recall: 0.6792 - val_loss: 1.3229 - val_accuracy: 0.5875 - val_f1: 0.5646 - val_precision: 0.6318 - val_recall: 0.5108\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.5632 - accuracy: 0.7750 - f1: 0.7662 - precision: 0.8322 - recall: 0.7105 - val_loss: 1.3044 - val_accuracy: 0.5967 - val_f1: 0.5803 - val_precision: 0.6356 - val_recall: 0.5342\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5275 - accuracy: 0.7877 - f1: 0.7755 - precision: 0.8406 - recall: 0.7202 - val_loss: 1.3121 - val_accuracy: 0.5800 - val_f1: 0.5702 - val_precision: 0.6278 - val_recall: 0.5225\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.5247 - accuracy: 0.7903 - f1: 0.7844 - precision: 0.8440 - recall: 0.7334 - val_loss: 1.2679 - val_accuracy: 0.5892 - val_f1: 0.5696 - val_precision: 0.6306 - val_recall: 0.5200\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.5269 - accuracy: 0.7917 - f1: 0.7851 - precision: 0.8452 - recall: 0.7334 - val_loss: 1.3991 - val_accuracy: 0.5883 - val_f1: 0.5702 - val_precision: 0.6258 - val_recall: 0.5242\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.5112 - accuracy: 0.8088 - f1: 0.7920 - precision: 0.8588 - recall: 0.7356 - val_loss: 1.4857 - val_accuracy: 0.5992 - val_f1: 0.5862 - val_precision: 0.6394 - val_recall: 0.5417\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.4430 - accuracy: 0.8261 - f1: 0.8223 - precision: 0.8685 - recall: 0.7809 - val_loss: 1.5236 - val_accuracy: 0.5975 - val_f1: 0.5796 - val_precision: 0.6306 - val_recall: 0.5367\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.4221 - accuracy: 0.8346 - f1: 0.8292 - precision: 0.8690 - recall: 0.7936 - val_loss: 1.3514 - val_accuracy: 0.6083 - val_f1: 0.5944 - val_precision: 0.6356 - val_recall: 0.5583\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3984 - accuracy: 0.8412 - f1: 0.8386 - precision: 0.8759 - recall: 0.8046 - val_loss: 1.4532 - val_accuracy: 0.6267 - val_f1: 0.6155 - val_precision: 0.6536 - val_recall: 0.5817\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3762 - accuracy: 0.8448 - f1: 0.8449 - precision: 0.8740 - recall: 0.8179 - val_loss: 1.4861 - val_accuracy: 0.6142 - val_f1: 0.6144 - val_precision: 0.6451 - val_recall: 0.5867\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3683 - accuracy: 0.8544 - f1: 0.8500 - precision: 0.8806 - recall: 0.8220 - val_loss: 1.5451 - val_accuracy: 0.6117 - val_f1: 0.6093 - val_precision: 0.6495 - val_recall: 0.5742\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3558 - accuracy: 0.8596 - f1: 0.8604 - precision: 0.8852 - recall: 0.8372 - val_loss: 1.6058 - val_accuracy: 0.6142 - val_f1: 0.6162 - val_precision: 0.6411 - val_recall: 0.5933\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3563 - accuracy: 0.8655 - f1: 0.8592 - precision: 0.8881 - recall: 0.8327 - val_loss: 1.6667 - val_accuracy: 0.5992 - val_f1: 0.5931 - val_precision: 0.6215 - val_recall: 0.5675\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3375 - accuracy: 0.8678 - f1: 0.8632 - precision: 0.8877 - recall: 0.8404 - val_loss: 1.6173 - val_accuracy: 0.5975 - val_f1: 0.5928 - val_precision: 0.6226 - val_recall: 0.5658\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3501 - accuracy: 0.8635 - f1: 0.8603 - precision: 0.8879 - recall: 0.8349 - val_loss: 1.6313 - val_accuracy: 0.6075 - val_f1: 0.6121 - val_precision: 0.6352 - val_recall: 0.5908\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3106 - accuracy: 0.8842 - f1: 0.8777 - precision: 0.8968 - recall: 0.8595 - val_loss: 1.7962 - val_accuracy: 0.5933 - val_f1: 0.5986 - val_precision: 0.6138 - val_recall: 0.5842\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.3192 - accuracy: 0.8757 - f1: 0.8751 - precision: 0.8948 - recall: 0.8565 - val_loss: 1.9563 - val_accuracy: 0.5992 - val_f1: 0.6040 - val_precision: 0.6196 - val_recall: 0.5892\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3214 - accuracy: 0.8761 - f1: 0.8746 - precision: 0.8905 - recall: 0.8594 - val_loss: 2.0060 - val_accuracy: 0.6108 - val_f1: 0.6113 - val_precision: 0.6286 - val_recall: 0.5950\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3378 - accuracy: 0.8660 - f1: 0.8659 - precision: 0.8835 - recall: 0.8492 - val_loss: 1.8582 - val_accuracy: 0.6092 - val_f1: 0.6094 - val_precision: 0.6276 - val_recall: 0.5925\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3428 - accuracy: 0.8725 - f1: 0.8730 - precision: 0.8938 - recall: 0.8533 - val_loss: 1.4973 - val_accuracy: 0.6175 - val_f1: 0.6154 - val_precision: 0.6486 - val_recall: 0.5858\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3142 - accuracy: 0.8807 - f1: 0.8767 - precision: 0.8974 - recall: 0.8571 - val_loss: 1.6588 - val_accuracy: 0.5833 - val_f1: 0.5814 - val_precision: 0.6077 - val_recall: 0.5575\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.2970 - accuracy: 0.8851 - f1: 0.8848 - precision: 0.9046 - recall: 0.8661 - val_loss: 1.7249 - val_accuracy: 0.5883 - val_f1: 0.5868 - val_precision: 0.6086 - val_recall: 0.5667\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3011 - accuracy: 0.8887 - f1: 0.8870 - precision: 0.9059 - recall: 0.8691 - val_loss: 1.6277 - val_accuracy: 0.6033 - val_f1: 0.6053 - val_precision: 0.6234 - val_recall: 0.5883\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2829 - accuracy: 0.8957 - f1: 0.8925 - precision: 0.9104 - recall: 0.8754 - val_loss: 1.7369 - val_accuracy: 0.6050 - val_f1: 0.6006 - val_precision: 0.6218 - val_recall: 0.5808\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 90ms/step - loss: 0.2592 - accuracy: 0.9042 - f1: 0.8981 - precision: 0.9141 - recall: 0.8827 - val_loss: 1.8333 - val_accuracy: 0.5808 - val_f1: 0.5774 - val_precision: 0.6028 - val_recall: 0.5542\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.3005 - accuracy: 0.8813 - f1: 0.8801 - precision: 0.8975 - recall: 0.8639 - val_loss: 2.0034 - val_accuracy: 0.5742 - val_f1: 0.5760 - val_precision: 0.5960 - val_recall: 0.5575\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.3502 - accuracy: 0.8610 - f1: 0.8579 - precision: 0.8825 - recall: 0.8349 - val_loss: 1.8094 - val_accuracy: 0.5917 - val_f1: 0.5922 - val_precision: 0.6154 - val_recall: 0.5708\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3628 - accuracy: 0.8593 - f1: 0.8542 - precision: 0.8800 - recall: 0.8301 - val_loss: 1.5949 - val_accuracy: 0.5858 - val_f1: 0.5799 - val_precision: 0.6161 - val_recall: 0.5483\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.4077 - accuracy: 0.8407 - f1: 0.8348 - precision: 0.8691 - recall: 0.8034 - val_loss: 1.6826 - val_accuracy: 0.6092 - val_f1: 0.6031 - val_precision: 0.6315 - val_recall: 0.5775\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3130 - accuracy: 0.8772 - f1: 0.8720 - precision: 0.8964 - recall: 0.8491 - val_loss: 1.9070 - val_accuracy: 0.6183 - val_f1: 0.6179 - val_precision: 0.6400 - val_recall: 0.5975\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.2650 - accuracy: 0.8929 - f1: 0.8939 - precision: 0.9121 - recall: 0.8765 - val_loss: 2.2198 - val_accuracy: 0.6033 - val_f1: 0.6053 - val_precision: 0.6224 - val_recall: 0.5892\n",
            "14/14 [==============================] - 1s 64ms/step - loss: 1.9514 - accuracy: 0.6121 - f1: 0.6089 - precision: 0.6318 - recall: 0.5879\n",
            "[1.951393485069275, 0.6121428608894348, 0.6089171171188354, 0.6318332552909851, 0.5878571271896362]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b82b60-6ac0-4a38-b0bd-48e58ae34aec"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 7s 100ms/step - loss: 1.3804 - accuracy: 0.2984 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3671 - val_accuracy: 0.3175 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.3556 - accuracy: 0.3263 - f1: 0.0049 - precision: 0.0541 - recall: 0.0026 - val_loss: 1.3209 - val_accuracy: 0.3700 - val_f1: 0.1204 - val_precision: 0.5554 - val_recall: 0.0683\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.2819 - accuracy: 0.4239 - f1: 0.1137 - precision: 0.6574 - recall: 0.0648 - val_loss: 1.2605 - val_accuracy: 0.4375 - val_f1: 0.2658 - val_precision: 0.5609 - val_recall: 0.1750\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1828 - accuracy: 0.4949 - f1: 0.3007 - precision: 0.6594 - recall: 0.1978 - val_loss: 1.1932 - val_accuracy: 0.4875 - val_f1: 0.3588 - val_precision: 0.5808 - val_recall: 0.2608\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.0996 - accuracy: 0.5630 - f1: 0.4263 - precision: 0.6848 - recall: 0.3110 - val_loss: 1.1814 - val_accuracy: 0.5017 - val_f1: 0.3974 - val_precision: 0.6246 - val_recall: 0.2925\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.0469 - accuracy: 0.5758 - f1: 0.4871 - precision: 0.7106 - recall: 0.3737 - val_loss: 1.1943 - val_accuracy: 0.5075 - val_f1: 0.4407 - val_precision: 0.5976 - val_recall: 0.3508\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.0091 - accuracy: 0.5927 - f1: 0.5267 - precision: 0.7056 - recall: 0.4220 - val_loss: 1.1956 - val_accuracy: 0.5058 - val_f1: 0.4575 - val_precision: 0.5942 - val_recall: 0.3725\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.9868 - accuracy: 0.6045 - f1: 0.5455 - precision: 0.7230 - recall: 0.4396 - val_loss: 1.2110 - val_accuracy: 0.5058 - val_f1: 0.4534 - val_precision: 0.5794 - val_recall: 0.3733\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.9630 - accuracy: 0.6183 - f1: 0.5514 - precision: 0.7348 - recall: 0.4427 - val_loss: 1.2356 - val_accuracy: 0.5075 - val_f1: 0.4711 - val_precision: 0.5702 - val_recall: 0.4017\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.9055 - accuracy: 0.6457 - f1: 0.5936 - precision: 0.7560 - recall: 0.4910 - val_loss: 1.2671 - val_accuracy: 0.5183 - val_f1: 0.4775 - val_precision: 0.5753 - val_recall: 0.4092\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.8804 - accuracy: 0.6585 - f1: 0.6117 - precision: 0.7692 - recall: 0.5097 - val_loss: 1.3744 - val_accuracy: 0.5000 - val_f1: 0.4659 - val_precision: 0.5565 - val_recall: 0.4017\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.8498 - accuracy: 0.6749 - f1: 0.6333 - precision: 0.7877 - recall: 0.5319 - val_loss: 1.3511 - val_accuracy: 0.4817 - val_f1: 0.4595 - val_precision: 0.5423 - val_recall: 0.3992\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.8117 - accuracy: 0.6830 - f1: 0.6413 - precision: 0.7826 - recall: 0.5452 - val_loss: 1.3593 - val_accuracy: 0.4900 - val_f1: 0.4692 - val_precision: 0.5489 - val_recall: 0.4108\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.7432 - accuracy: 0.7050 - f1: 0.6815 - precision: 0.7905 - recall: 0.6006 - val_loss: 1.3469 - val_accuracy: 0.4983 - val_f1: 0.4790 - val_precision: 0.5615 - val_recall: 0.4183\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.7142 - accuracy: 0.7212 - f1: 0.7061 - precision: 0.8041 - recall: 0.6300 - val_loss: 1.2617 - val_accuracy: 0.5092 - val_f1: 0.4950 - val_precision: 0.5851 - val_recall: 0.4300\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6864 - accuracy: 0.7366 - f1: 0.7166 - precision: 0.8091 - recall: 0.6435 - val_loss: 1.3465 - val_accuracy: 0.5133 - val_f1: 0.4807 - val_precision: 0.5754 - val_recall: 0.4133\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.6423 - accuracy: 0.7559 - f1: 0.7342 - precision: 0.8349 - recall: 0.6562 - val_loss: 1.3287 - val_accuracy: 0.5200 - val_f1: 0.4752 - val_precision: 0.5783 - val_recall: 0.4042\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.6332 - accuracy: 0.7537 - f1: 0.7322 - precision: 0.8293 - recall: 0.6569 - val_loss: 1.2768 - val_accuracy: 0.4942 - val_f1: 0.4674 - val_precision: 0.5727 - val_recall: 0.3958\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6336 - accuracy: 0.7517 - f1: 0.7308 - precision: 0.8330 - recall: 0.6526 - val_loss: 1.3483 - val_accuracy: 0.5142 - val_f1: 0.4959 - val_precision: 0.5639 - val_recall: 0.4433\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.6043 - accuracy: 0.7722 - f1: 0.7537 - precision: 0.8353 - recall: 0.6876 - val_loss: 1.3855 - val_accuracy: 0.5092 - val_f1: 0.4856 - val_precision: 0.5607 - val_recall: 0.4292\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5800 - accuracy: 0.7851 - f1: 0.7688 - precision: 0.8514 - recall: 0.7023 - val_loss: 1.3866 - val_accuracy: 0.5275 - val_f1: 0.5125 - val_precision: 0.5803 - val_recall: 0.4600\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5201 - accuracy: 0.8041 - f1: 0.7972 - precision: 0.8645 - recall: 0.7400 - val_loss: 1.4025 - val_accuracy: 0.5417 - val_f1: 0.5336 - val_precision: 0.5819 - val_recall: 0.4933\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.4678 - accuracy: 0.8253 - f1: 0.8236 - precision: 0.8749 - recall: 0.7785 - val_loss: 1.4843 - val_accuracy: 0.5508 - val_f1: 0.5468 - val_precision: 0.5947 - val_recall: 0.5067\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.4379 - accuracy: 0.8359 - f1: 0.8292 - precision: 0.8747 - recall: 0.7886 - val_loss: 1.5119 - val_accuracy: 0.5358 - val_f1: 0.5298 - val_precision: 0.5715 - val_recall: 0.4942\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.4428 - accuracy: 0.8353 - f1: 0.8345 - precision: 0.8760 - recall: 0.7973 - val_loss: 1.5665 - val_accuracy: 0.5558 - val_f1: 0.5512 - val_precision: 0.5844 - val_recall: 0.5217\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3876 - accuracy: 0.8642 - f1: 0.8542 - precision: 0.8968 - recall: 0.8158 - val_loss: 1.6774 - val_accuracy: 0.5442 - val_f1: 0.5456 - val_precision: 0.5741 - val_recall: 0.5200\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3769 - accuracy: 0.8589 - f1: 0.8601 - precision: 0.8959 - recall: 0.8272 - val_loss: 1.8839 - val_accuracy: 0.5442 - val_f1: 0.5438 - val_precision: 0.5704 - val_recall: 0.5200\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3666 - accuracy: 0.8680 - f1: 0.8661 - precision: 0.8945 - recall: 0.8398 - val_loss: 1.9358 - val_accuracy: 0.5600 - val_f1: 0.5585 - val_precision: 0.5787 - val_recall: 0.5400\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3118 - accuracy: 0.8851 - f1: 0.8868 - precision: 0.9154 - recall: 0.8601 - val_loss: 2.0741 - val_accuracy: 0.5517 - val_f1: 0.5505 - val_precision: 0.5624 - val_recall: 0.5392\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.3426 - accuracy: 0.8700 - f1: 0.8734 - precision: 0.8970 - recall: 0.8512 - val_loss: 1.8384 - val_accuracy: 0.5617 - val_f1: 0.5618 - val_precision: 0.5845 - val_recall: 0.5408\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3451 - accuracy: 0.8747 - f1: 0.8719 - precision: 0.8990 - recall: 0.8467 - val_loss: 1.6794 - val_accuracy: 0.5700 - val_f1: 0.5673 - val_precision: 0.5878 - val_recall: 0.5483\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3101 - accuracy: 0.8935 - f1: 0.8908 - precision: 0.9112 - recall: 0.8715 - val_loss: 1.7865 - val_accuracy: 0.5700 - val_f1: 0.5621 - val_precision: 0.5796 - val_recall: 0.5458\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.2689 - accuracy: 0.9056 - f1: 0.9075 - precision: 0.9235 - recall: 0.8921 - val_loss: 1.7333 - val_accuracy: 0.5717 - val_f1: 0.5691 - val_precision: 0.5907 - val_recall: 0.5492\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2390 - accuracy: 0.9110 - f1: 0.9102 - precision: 0.9241 - recall: 0.8969 - val_loss: 1.8307 - val_accuracy: 0.5817 - val_f1: 0.5785 - val_precision: 0.5956 - val_recall: 0.5625\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.2339 - accuracy: 0.9142 - f1: 0.9153 - precision: 0.9282 - recall: 0.9029 - val_loss: 1.9571 - val_accuracy: 0.5700 - val_f1: 0.5670 - val_precision: 0.5834 - val_recall: 0.5517\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2101 - accuracy: 0.9241 - f1: 0.9264 - precision: 0.9402 - recall: 0.9132 - val_loss: 2.1060 - val_accuracy: 0.5742 - val_f1: 0.5719 - val_precision: 0.5900 - val_recall: 0.5550\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2393 - accuracy: 0.9135 - f1: 0.9104 - precision: 0.9198 - recall: 0.9013 - val_loss: 1.9162 - val_accuracy: 0.5633 - val_f1: 0.5623 - val_precision: 0.5789 - val_recall: 0.5467\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1824 - accuracy: 0.9355 - f1: 0.9331 - precision: 0.9424 - recall: 0.9242 - val_loss: 1.9717 - val_accuracy: 0.5650 - val_f1: 0.5625 - val_precision: 0.5814 - val_recall: 0.5450\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1815 - accuracy: 0.9363 - f1: 0.9370 - precision: 0.9462 - recall: 0.9282 - val_loss: 2.1976 - val_accuracy: 0.5742 - val_f1: 0.5773 - val_precision: 0.5922 - val_recall: 0.5633\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1643 - accuracy: 0.9413 - f1: 0.9397 - precision: 0.9459 - recall: 0.9336 - val_loss: 2.1551 - val_accuracy: 0.5608 - val_f1: 0.5599 - val_precision: 0.5766 - val_recall: 0.5442\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1562 - accuracy: 0.9442 - f1: 0.9451 - precision: 0.9523 - recall: 0.9382 - val_loss: 2.3526 - val_accuracy: 0.5600 - val_f1: 0.5560 - val_precision: 0.5676 - val_recall: 0.5450\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1576 - accuracy: 0.9446 - f1: 0.9442 - precision: 0.9488 - recall: 0.9397 - val_loss: 2.2956 - val_accuracy: 0.5608 - val_f1: 0.5599 - val_precision: 0.5731 - val_recall: 0.5475\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1516 - accuracy: 0.9503 - f1: 0.9484 - precision: 0.9541 - recall: 0.9427 - val_loss: 2.4812 - val_accuracy: 0.5642 - val_f1: 0.5629 - val_precision: 0.5720 - val_recall: 0.5542\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1785 - accuracy: 0.9365 - f1: 0.9379 - precision: 0.9417 - recall: 0.9341 - val_loss: 2.2821 - val_accuracy: 0.5633 - val_f1: 0.5584 - val_precision: 0.5690 - val_recall: 0.5483\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1462 - accuracy: 0.9461 - f1: 0.9456 - precision: 0.9499 - recall: 0.9414 - val_loss: 2.2023 - val_accuracy: 0.5800 - val_f1: 0.5800 - val_precision: 0.5959 - val_recall: 0.5650\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.1661 - accuracy: 0.9396 - f1: 0.9383 - precision: 0.9436 - recall: 0.9330 - val_loss: 2.4872 - val_accuracy: 0.5550 - val_f1: 0.5554 - val_precision: 0.5671 - val_recall: 0.5442\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2129 - accuracy: 0.9257 - f1: 0.9264 - precision: 0.9349 - recall: 0.9180 - val_loss: 2.4107 - val_accuracy: 0.5717 - val_f1: 0.5651 - val_precision: 0.5747 - val_recall: 0.5558\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1540 - accuracy: 0.9441 - f1: 0.9457 - precision: 0.9534 - recall: 0.9383 - val_loss: 2.3928 - val_accuracy: 0.5625 - val_f1: 0.5591 - val_precision: 0.5713 - val_recall: 0.5475\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1591 - accuracy: 0.9405 - f1: 0.9411 - precision: 0.9474 - recall: 0.9349 - val_loss: 2.5238 - val_accuracy: 0.5708 - val_f1: 0.5717 - val_precision: 0.5804 - val_recall: 0.5633\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1440 - accuracy: 0.9477 - f1: 0.9471 - precision: 0.9510 - recall: 0.9434 - val_loss: 2.5537 - val_accuracy: 0.5792 - val_f1: 0.5768 - val_precision: 0.5848 - val_recall: 0.5692\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1380 - accuracy: 0.9469 - f1: 0.9469 - precision: 0.9511 - recall: 0.9427 - val_loss: 2.5266 - val_accuracy: 0.5800 - val_f1: 0.5758 - val_precision: 0.5873 - val_recall: 0.5650\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1462 - accuracy: 0.9447 - f1: 0.9458 - precision: 0.9497 - recall: 0.9420 - val_loss: 2.3456 - val_accuracy: 0.5775 - val_f1: 0.5735 - val_precision: 0.5825 - val_recall: 0.5650\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1481 - accuracy: 0.9486 - f1: 0.9489 - precision: 0.9525 - recall: 0.9454 - val_loss: 2.3814 - val_accuracy: 0.5792 - val_f1: 0.5802 - val_precision: 0.5891 - val_recall: 0.5717\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1355 - accuracy: 0.9543 - f1: 0.9536 - precision: 0.9560 - recall: 0.9512 - val_loss: 2.4926 - val_accuracy: 0.5675 - val_f1: 0.5649 - val_precision: 0.5735 - val_recall: 0.5567\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1417 - accuracy: 0.9499 - f1: 0.9496 - precision: 0.9548 - recall: 0.9446 - val_loss: 2.4416 - val_accuracy: 0.5875 - val_f1: 0.5849 - val_precision: 0.5934 - val_recall: 0.5767\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1269 - accuracy: 0.9565 - f1: 0.9568 - precision: 0.9601 - recall: 0.9535 - val_loss: 2.2775 - val_accuracy: 0.6050 - val_f1: 0.6004 - val_precision: 0.6121 - val_recall: 0.5892\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1197 - accuracy: 0.9584 - f1: 0.9585 - precision: 0.9627 - recall: 0.9544 - val_loss: 2.3448 - val_accuracy: 0.5958 - val_f1: 0.5918 - val_precision: 0.5997 - val_recall: 0.5842\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1026 - accuracy: 0.9633 - f1: 0.9639 - precision: 0.9662 - recall: 0.9617 - val_loss: 2.3460 - val_accuracy: 0.6067 - val_f1: 0.6048 - val_precision: 0.6133 - val_recall: 0.5967\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.0821 - accuracy: 0.9665 - f1: 0.9664 - precision: 0.9671 - recall: 0.9657 - val_loss: 2.3646 - val_accuracy: 0.6017 - val_f1: 0.6010 - val_precision: 0.6116 - val_recall: 0.5908\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.0801 - accuracy: 0.9682 - f1: 0.9675 - precision: 0.9698 - recall: 0.9652 - val_loss: 2.4062 - val_accuracy: 0.6000 - val_f1: 0.5993 - val_precision: 0.6028 - val_recall: 0.5958\n",
            "14/14 [==============================] - 1s 68ms/step - loss: 2.2749 - accuracy: 0.6257 - f1: 0.6226 - precision: 0.6275 - recall: 0.6179\n",
            "[2.2748939990997314, 0.6257143020629883, 0.6226033568382263, 0.6274819374084473, 0.6178570985794067]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc0d897-b550-4667-a1e1-3f0e981ab3f9"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 6s 106ms/step - loss: 1.3900 - accuracy: 0.2621 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3890 - val_accuracy: 0.3058 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.3532 - accuracy: 0.3347 - f1: 0.0031 - precision: 0.1076 - recall: 0.0016 - val_loss: 1.3714 - val_accuracy: 0.3367 - val_f1: 0.0363 - val_precision: 0.4424 - val_recall: 0.0192\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 98ms/step - loss: 1.3132 - accuracy: 0.3783 - f1: 0.0546 - precision: 0.6634 - recall: 0.0291 - val_loss: 1.3509 - val_accuracy: 0.3450 - val_f1: 0.0563 - val_precision: 0.5239 - val_recall: 0.0300\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 1.2611 - accuracy: 0.4197 - f1: 0.1318 - precision: 0.6783 - recall: 0.0756 - val_loss: 1.3458 - val_accuracy: 0.3358 - val_f1: 0.0960 - val_precision: 0.5019 - val_recall: 0.0533\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.2185 - accuracy: 0.4552 - f1: 0.2023 - precision: 0.6564 - recall: 0.1218 - val_loss: 1.3542 - val_accuracy: 0.3433 - val_f1: 0.1031 - val_precision: 0.4553 - val_recall: 0.0583\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 1.1789 - accuracy: 0.4848 - f1: 0.2768 - precision: 0.6622 - recall: 0.1765 - val_loss: 1.3678 - val_accuracy: 0.3300 - val_f1: 0.1438 - val_precision: 0.4413 - val_recall: 0.0867\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.1575 - accuracy: 0.5054 - f1: 0.3211 - precision: 0.6899 - recall: 0.2113 - val_loss: 1.3790 - val_accuracy: 0.3308 - val_f1: 0.2011 - val_precision: 0.4438 - val_recall: 0.1308\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.1206 - accuracy: 0.5221 - f1: 0.3656 - precision: 0.6594 - recall: 0.2543 - val_loss: 1.4271 - val_accuracy: 0.3300 - val_f1: 0.2066 - val_precision: 0.3772 - val_recall: 0.1433\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.0832 - accuracy: 0.5419 - f1: 0.4236 - precision: 0.6998 - recall: 0.3045 - val_loss: 1.4004 - val_accuracy: 0.3600 - val_f1: 0.2345 - val_precision: 0.4076 - val_recall: 0.1650\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 1.0630 - accuracy: 0.5428 - f1: 0.4432 - precision: 0.6811 - recall: 0.3302 - val_loss: 1.4474 - val_accuracy: 0.3500 - val_f1: 0.2424 - val_precision: 0.3729 - val_recall: 0.1800\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 102ms/step - loss: 1.0078 - accuracy: 0.5877 - f1: 0.4891 - precision: 0.7001 - recall: 0.3767 - val_loss: 1.5453 - val_accuracy: 0.3333 - val_f1: 0.2596 - val_precision: 0.3574 - val_recall: 0.2042\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.9926 - accuracy: 0.5899 - f1: 0.5208 - precision: 0.7023 - recall: 0.4152 - val_loss: 1.6065 - val_accuracy: 0.3233 - val_f1: 0.2665 - val_precision: 0.3514 - val_recall: 0.2150\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.9755 - accuracy: 0.5974 - f1: 0.5474 - precision: 0.7112 - recall: 0.4465 - val_loss: 1.5960 - val_accuracy: 0.3325 - val_f1: 0.2896 - val_precision: 0.3719 - val_recall: 0.2375\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.9322 - accuracy: 0.6208 - f1: 0.5633 - precision: 0.7138 - recall: 0.4660 - val_loss: 1.6251 - val_accuracy: 0.3367 - val_f1: 0.2814 - val_precision: 0.3572 - val_recall: 0.2325\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.9240 - accuracy: 0.6224 - f1: 0.5772 - precision: 0.7210 - recall: 0.4819 - val_loss: 1.6303 - val_accuracy: 0.3475 - val_f1: 0.3009 - val_precision: 0.3709 - val_recall: 0.2533\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 0.8623 - accuracy: 0.6501 - f1: 0.6058 - precision: 0.7422 - recall: 0.5124 - val_loss: 1.7401 - val_accuracy: 0.3583 - val_f1: 0.3269 - val_precision: 0.3764 - val_recall: 0.2892\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.8309 - accuracy: 0.6633 - f1: 0.6293 - precision: 0.7507 - recall: 0.5429 - val_loss: 1.7431 - val_accuracy: 0.3725 - val_f1: 0.3338 - val_precision: 0.3835 - val_recall: 0.2958\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.7904 - accuracy: 0.6830 - f1: 0.6547 - precision: 0.7631 - recall: 0.5738 - val_loss: 1.7927 - val_accuracy: 0.3725 - val_f1: 0.3413 - val_precision: 0.3879 - val_recall: 0.3050\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.7577 - accuracy: 0.7008 - f1: 0.6741 - precision: 0.7711 - recall: 0.5996 - val_loss: 1.8455 - val_accuracy: 0.3725 - val_f1: 0.3505 - val_precision: 0.3929 - val_recall: 0.3167\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.7389 - accuracy: 0.7097 - f1: 0.6925 - precision: 0.7834 - recall: 0.6213 - val_loss: 1.8490 - val_accuracy: 0.3783 - val_f1: 0.3556 - val_precision: 0.3955 - val_recall: 0.3233\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.7773 - accuracy: 0.6942 - f1: 0.6672 - precision: 0.7499 - recall: 0.6020 - val_loss: 1.9360 - val_accuracy: 0.3650 - val_f1: 0.3418 - val_precision: 0.3867 - val_recall: 0.3067\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.8143 - accuracy: 0.6666 - f1: 0.6472 - precision: 0.7342 - recall: 0.5794 - val_loss: 1.8723 - val_accuracy: 0.3608 - val_f1: 0.3422 - val_precision: 0.3903 - val_recall: 0.3050\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.8183 - accuracy: 0.6694 - f1: 0.6517 - precision: 0.7347 - recall: 0.5869 - val_loss: 1.9372 - val_accuracy: 0.3517 - val_f1: 0.3298 - val_precision: 0.3761 - val_recall: 0.2942\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.7372 - accuracy: 0.7119 - f1: 0.6942 - precision: 0.7751 - recall: 0.6290 - val_loss: 1.9566 - val_accuracy: 0.3683 - val_f1: 0.3492 - val_precision: 0.3881 - val_recall: 0.3175\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 98ms/step - loss: 0.7110 - accuracy: 0.7206 - f1: 0.7041 - precision: 0.7789 - recall: 0.6431 - val_loss: 2.0121 - val_accuracy: 0.3800 - val_f1: 0.3442 - val_precision: 0.3835 - val_recall: 0.3125\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.6732 - accuracy: 0.7395 - f1: 0.7199 - precision: 0.7901 - recall: 0.6618 - val_loss: 2.0250 - val_accuracy: 0.3525 - val_f1: 0.3382 - val_precision: 0.3741 - val_recall: 0.3092\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.7212 - accuracy: 0.7089 - f1: 0.6928 - precision: 0.7561 - recall: 0.6398 - val_loss: 2.0739 - val_accuracy: 0.3500 - val_f1: 0.3281 - val_precision: 0.3611 - val_recall: 0.3008\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.6608 - accuracy: 0.7450 - f1: 0.7324 - precision: 0.7987 - recall: 0.6767 - val_loss: 2.1548 - val_accuracy: 0.3625 - val_f1: 0.3524 - val_precision: 0.3840 - val_recall: 0.3258\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.6313 - accuracy: 0.7639 - f1: 0.7492 - precision: 0.8162 - recall: 0.6931 - val_loss: 2.1545 - val_accuracy: 0.3742 - val_f1: 0.3634 - val_precision: 0.3929 - val_recall: 0.3383\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5840 - accuracy: 0.7711 - f1: 0.7681 - precision: 0.8176 - recall: 0.7247 - val_loss: 2.1565 - val_accuracy: 0.3767 - val_f1: 0.3607 - val_precision: 0.3888 - val_recall: 0.3367\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.5523 - accuracy: 0.7882 - f1: 0.7859 - precision: 0.8297 - recall: 0.7469 - val_loss: 2.2321 - val_accuracy: 0.3675 - val_f1: 0.3606 - val_precision: 0.3850 - val_recall: 0.3392\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5496 - accuracy: 0.7969 - f1: 0.7851 - precision: 0.8302 - recall: 0.7451 - val_loss: 2.2564 - val_accuracy: 0.3800 - val_f1: 0.3747 - val_precision: 0.3991 - val_recall: 0.3533\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5447 - accuracy: 0.7852 - f1: 0.7842 - precision: 0.8271 - recall: 0.7458 - val_loss: 2.3502 - val_accuracy: 0.3633 - val_f1: 0.3532 - val_precision: 0.3770 - val_recall: 0.3325\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5471 - accuracy: 0.7898 - f1: 0.7858 - precision: 0.8296 - recall: 0.7468 - val_loss: 2.2888 - val_accuracy: 0.3883 - val_f1: 0.3739 - val_precision: 0.3972 - val_recall: 0.3533\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.4794 - accuracy: 0.8095 - f1: 0.8087 - precision: 0.8482 - recall: 0.7732 - val_loss: 2.3103 - val_accuracy: 0.3708 - val_f1: 0.3602 - val_precision: 0.3843 - val_recall: 0.3392\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4638 - accuracy: 0.8251 - f1: 0.8229 - precision: 0.8561 - recall: 0.7924 - val_loss: 2.3584 - val_accuracy: 0.3608 - val_f1: 0.3390 - val_precision: 0.3675 - val_recall: 0.3150\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4608 - accuracy: 0.8327 - f1: 0.8261 - precision: 0.8590 - recall: 0.7959 - val_loss: 2.4023 - val_accuracy: 0.3542 - val_f1: 0.3475 - val_precision: 0.3704 - val_recall: 0.3275\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4602 - accuracy: 0.8251 - f1: 0.8230 - precision: 0.8546 - recall: 0.7940 - val_loss: 2.4229 - val_accuracy: 0.3517 - val_f1: 0.3416 - val_precision: 0.3645 - val_recall: 0.3217\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.4318 - accuracy: 0.8379 - f1: 0.8326 - precision: 0.8612 - recall: 0.8062 - val_loss: 2.4842 - val_accuracy: 0.3458 - val_f1: 0.3340 - val_precision: 0.3536 - val_recall: 0.3167\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 102ms/step - loss: 0.4766 - accuracy: 0.8130 - f1: 0.8099 - precision: 0.8375 - recall: 0.7844 - val_loss: 2.4959 - val_accuracy: 0.3575 - val_f1: 0.3409 - val_precision: 0.3617 - val_recall: 0.3225\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4343 - accuracy: 0.8367 - f1: 0.8348 - precision: 0.8647 - recall: 0.8072 - val_loss: 2.4805 - val_accuracy: 0.3658 - val_f1: 0.3502 - val_precision: 0.3700 - val_recall: 0.3325\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4417 - accuracy: 0.8372 - f1: 0.8369 - precision: 0.8662 - recall: 0.8096 - val_loss: 2.5518 - val_accuracy: 0.3450 - val_f1: 0.3300 - val_precision: 0.3467 - val_recall: 0.3150\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.4545 - accuracy: 0.8234 - f1: 0.8233 - precision: 0.8508 - recall: 0.7977 - val_loss: 2.5624 - val_accuracy: 0.3433 - val_f1: 0.3411 - val_precision: 0.3593 - val_recall: 0.3250\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5186 - accuracy: 0.7965 - f1: 0.7964 - precision: 0.8263 - recall: 0.7688 - val_loss: 2.5128 - val_accuracy: 0.3575 - val_f1: 0.3465 - val_precision: 0.3690 - val_recall: 0.3267\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5035 - accuracy: 0.7934 - f1: 0.7939 - precision: 0.8266 - recall: 0.7639 - val_loss: 2.3775 - val_accuracy: 0.3725 - val_f1: 0.3608 - val_precision: 0.3824 - val_recall: 0.3417\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4738 - accuracy: 0.8172 - f1: 0.8111 - precision: 0.8489 - recall: 0.7769 - val_loss: 2.3270 - val_accuracy: 0.3700 - val_f1: 0.3659 - val_precision: 0.3896 - val_recall: 0.3450\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.4309 - accuracy: 0.8332 - f1: 0.8296 - precision: 0.8627 - recall: 0.7991 - val_loss: 2.3753 - val_accuracy: 0.3808 - val_f1: 0.3705 - val_precision: 0.3951 - val_recall: 0.3492\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4018 - accuracy: 0.8505 - f1: 0.8442 - precision: 0.8697 - recall: 0.8204 - val_loss: 2.4418 - val_accuracy: 0.3808 - val_f1: 0.3672 - val_precision: 0.3875 - val_recall: 0.3492\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.3727 - accuracy: 0.8557 - f1: 0.8566 - precision: 0.8824 - recall: 0.8327 - val_loss: 2.4841 - val_accuracy: 0.3667 - val_f1: 0.3629 - val_precision: 0.3817 - val_recall: 0.3458\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.3420 - accuracy: 0.8684 - f1: 0.8687 - precision: 0.8921 - recall: 0.8467 - val_loss: 2.5948 - val_accuracy: 0.3875 - val_f1: 0.3800 - val_precision: 0.3965 - val_recall: 0.3650\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.3349 - accuracy: 0.8759 - f1: 0.8736 - precision: 0.8961 - recall: 0.8527 - val_loss: 2.6449 - val_accuracy: 0.3908 - val_f1: 0.3786 - val_precision: 0.3965 - val_recall: 0.3625\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3223 - accuracy: 0.8820 - f1: 0.8789 - precision: 0.8983 - recall: 0.8604 - val_loss: 2.7560 - val_accuracy: 0.3742 - val_f1: 0.3733 - val_precision: 0.3888 - val_recall: 0.3592\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.3083 - accuracy: 0.8802 - f1: 0.8795 - precision: 0.8945 - recall: 0.8651 - val_loss: 2.7653 - val_accuracy: 0.3717 - val_f1: 0.3648 - val_precision: 0.3801 - val_recall: 0.3508\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.3090 - accuracy: 0.8858 - f1: 0.8853 - precision: 0.9006 - recall: 0.8707 - val_loss: 2.8218 - val_accuracy: 0.3658 - val_f1: 0.3612 - val_precision: 0.3763 - val_recall: 0.3475\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.2985 - accuracy: 0.8824 - f1: 0.8866 - precision: 0.9029 - recall: 0.8710 - val_loss: 2.9758 - val_accuracy: 0.3700 - val_f1: 0.3647 - val_precision: 0.3751 - val_recall: 0.3550\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.3113 - accuracy: 0.8830 - f1: 0.8815 - precision: 0.8982 - recall: 0.8657 - val_loss: 2.8309 - val_accuracy: 0.3825 - val_f1: 0.3743 - val_precision: 0.3880 - val_recall: 0.3617\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3168 - accuracy: 0.8812 - f1: 0.8815 - precision: 0.8956 - recall: 0.8679 - val_loss: 2.8541 - val_accuracy: 0.3858 - val_f1: 0.3795 - val_precision: 0.3943 - val_recall: 0.3658\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3344 - accuracy: 0.8736 - f1: 0.8709 - precision: 0.8852 - recall: 0.8572 - val_loss: 2.9505 - val_accuracy: 0.3750 - val_f1: 0.3738 - val_precision: 0.3859 - val_recall: 0.3625\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.3179 - accuracy: 0.8813 - f1: 0.8809 - precision: 0.8961 - recall: 0.8663 - val_loss: 2.9255 - val_accuracy: 0.3767 - val_f1: 0.3680 - val_precision: 0.3802 - val_recall: 0.3567\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 102ms/step - loss: 0.3201 - accuracy: 0.8802 - f1: 0.8821 - precision: 0.8993 - recall: 0.8656 - val_loss: 2.9547 - val_accuracy: 0.3733 - val_f1: 0.3687 - val_precision: 0.3827 - val_recall: 0.3558\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 2.9953 - accuracy: 0.3771 - f1: 0.3697 - precision: 0.3818 - recall: 0.3586\n",
            "[2.9953267574310303, 0.37714284658432007, 0.36974066495895386, 0.3817651569843292, 0.35857146978378296]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40462c2-5ab0-4661-d6b7-6aeece7b2ce2"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(64))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 6s 98ms/step - loss: 1.3817 - accuracy: 0.2946 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3729 - val_accuracy: 0.3183 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.3674 - accuracy: 0.3261 - f1: 0.0091 - precision: 0.0943 - recall: 0.0050 - val_loss: 1.3429 - val_accuracy: 0.3350 - val_f1: 0.0679 - val_precision: 0.7847 - val_recall: 0.0358\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 1.3356 - accuracy: 0.3628 - f1: 0.0872 - precision: 0.6988 - recall: 0.0493 - val_loss: 1.3194 - val_accuracy: 0.3892 - val_f1: 0.1088 - val_precision: 0.6685 - val_recall: 0.0600\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 1.3051 - accuracy: 0.3866 - f1: 0.1496 - precision: 0.6984 - recall: 0.0869 - val_loss: 1.3277 - val_accuracy: 0.3483 - val_f1: 0.0340 - val_precision: 0.7500 - val_recall: 0.0175\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 1.2858 - accuracy: 0.4027 - f1: 0.1529 - precision: 0.6244 - recall: 0.0932 - val_loss: 1.2862 - val_accuracy: 0.3883 - val_f1: 0.1243 - val_precision: 0.7037 - val_recall: 0.0692\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.2456 - accuracy: 0.4405 - f1: 0.2126 - precision: 0.6566 - recall: 0.1313 - val_loss: 1.2857 - val_accuracy: 0.3975 - val_f1: 0.2124 - val_precision: 0.5818 - val_recall: 0.1308\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.2159 - accuracy: 0.4532 - f1: 0.2661 - precision: 0.6616 - recall: 0.1682 - val_loss: 1.2769 - val_accuracy: 0.4200 - val_f1: 0.2437 - val_precision: 0.5552 - val_recall: 0.1567\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1977 - accuracy: 0.4552 - f1: 0.2928 - precision: 0.6607 - recall: 0.1903 - val_loss: 1.2513 - val_accuracy: 0.4333 - val_f1: 0.2836 - val_precision: 0.5639 - val_recall: 0.1900\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1781 - accuracy: 0.4775 - f1: 0.3010 - precision: 0.6589 - recall: 0.1963 - val_loss: 1.2446 - val_accuracy: 0.4333 - val_f1: 0.3006 - val_precision: 0.5828 - val_recall: 0.2033\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1620 - accuracy: 0.4935 - f1: 0.3195 - precision: 0.6908 - recall: 0.2092 - val_loss: 1.2679 - val_accuracy: 0.4142 - val_f1: 0.3128 - val_precision: 0.5617 - val_recall: 0.2175\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1417 - accuracy: 0.4993 - f1: 0.3423 - precision: 0.7115 - recall: 0.2277 - val_loss: 1.2408 - val_accuracy: 0.4267 - val_f1: 0.3153 - val_precision: 0.6030 - val_recall: 0.2142\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 1.1130 - accuracy: 0.5107 - f1: 0.3687 - precision: 0.7400 - recall: 0.2478 - val_loss: 1.2332 - val_accuracy: 0.4308 - val_f1: 0.3267 - val_precision: 0.6080 - val_recall: 0.2242\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 1.0814 - accuracy: 0.5205 - f1: 0.3808 - precision: 0.7639 - recall: 0.2562 - val_loss: 1.2380 - val_accuracy: 0.4242 - val_f1: 0.3317 - val_precision: 0.5873 - val_recall: 0.2317\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 1.0526 - accuracy: 0.5463 - f1: 0.4054 - precision: 0.7598 - recall: 0.2785 - val_loss: 1.2478 - val_accuracy: 0.4350 - val_f1: 0.3498 - val_precision: 0.5720 - val_recall: 0.2525\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.0127 - accuracy: 0.5567 - f1: 0.4387 - precision: 0.7889 - recall: 0.3067 - val_loss: 1.2007 - val_accuracy: 0.4708 - val_f1: 0.3531 - val_precision: 0.6113 - val_recall: 0.2492\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.9810 - accuracy: 0.5721 - f1: 0.4783 - precision: 0.7907 - recall: 0.3454 - val_loss: 1.1862 - val_accuracy: 0.4742 - val_f1: 0.3205 - val_precision: 0.6418 - val_recall: 0.2142\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.9886 - accuracy: 0.5857 - f1: 0.4755 - precision: 0.7679 - recall: 0.3486 - val_loss: 1.1702 - val_accuracy: 0.5158 - val_f1: 0.4472 - val_precision: 0.6273 - val_recall: 0.3483\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.8994 - accuracy: 0.6373 - f1: 0.5710 - precision: 0.7774 - recall: 0.4541 - val_loss: 1.1865 - val_accuracy: 0.5425 - val_f1: 0.4804 - val_precision: 0.6407 - val_recall: 0.3858\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.8440 - accuracy: 0.6570 - f1: 0.6173 - precision: 0.7892 - recall: 0.5081 - val_loss: 1.1743 - val_accuracy: 0.5467 - val_f1: 0.5014 - val_precision: 0.6446 - val_recall: 0.4117\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.8160 - accuracy: 0.6673 - f1: 0.6368 - precision: 0.7927 - recall: 0.5333 - val_loss: 1.1765 - val_accuracy: 0.5550 - val_f1: 0.5072 - val_precision: 0.6432 - val_recall: 0.4200\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.7905 - accuracy: 0.6785 - f1: 0.6522 - precision: 0.7917 - recall: 0.5560 - val_loss: 1.1544 - val_accuracy: 0.5650 - val_f1: 0.5332 - val_precision: 0.6652 - val_recall: 0.4475\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.7496 - accuracy: 0.7026 - f1: 0.6690 - precision: 0.8002 - recall: 0.5762 - val_loss: 1.1812 - val_accuracy: 0.5592 - val_f1: 0.5211 - val_precision: 0.6465 - val_recall: 0.4375\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.7503 - accuracy: 0.6996 - f1: 0.6753 - precision: 0.8088 - recall: 0.5813 - val_loss: 1.1485 - val_accuracy: 0.5817 - val_f1: 0.5358 - val_precision: 0.6650 - val_recall: 0.4508\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.7150 - accuracy: 0.7110 - f1: 0.7010 - precision: 0.8134 - recall: 0.6170 - val_loss: 1.2229 - val_accuracy: 0.5767 - val_f1: 0.5433 - val_precision: 0.6470 - val_recall: 0.4692\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.6640 - accuracy: 0.7292 - f1: 0.7150 - precision: 0.8236 - recall: 0.6334 - val_loss: 1.2512 - val_accuracy: 0.5908 - val_f1: 0.5614 - val_precision: 0.6562 - val_recall: 0.4917\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.6442 - accuracy: 0.7512 - f1: 0.7330 - precision: 0.8352 - recall: 0.6540 - val_loss: 1.2509 - val_accuracy: 0.5742 - val_f1: 0.5476 - val_precision: 0.6430 - val_recall: 0.4775\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.6422 - accuracy: 0.7539 - f1: 0.7368 - precision: 0.8435 - recall: 0.6554 - val_loss: 1.2994 - val_accuracy: 0.5767 - val_f1: 0.5597 - val_precision: 0.6431 - val_recall: 0.4958\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5736 - accuracy: 0.7787 - f1: 0.7669 - precision: 0.8536 - recall: 0.6978 - val_loss: 1.2942 - val_accuracy: 0.5825 - val_f1: 0.5599 - val_precision: 0.6320 - val_recall: 0.5033\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5139 - accuracy: 0.8022 - f1: 0.7962 - precision: 0.8727 - recall: 0.7333 - val_loss: 1.2705 - val_accuracy: 0.5967 - val_f1: 0.5753 - val_precision: 0.6471 - val_recall: 0.5183\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.4884 - accuracy: 0.8153 - f1: 0.8097 - precision: 0.8790 - recall: 0.7514 - val_loss: 1.3185 - val_accuracy: 0.5992 - val_f1: 0.5902 - val_precision: 0.6563 - val_recall: 0.5367\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.4627 - accuracy: 0.8221 - f1: 0.8210 - precision: 0.8835 - recall: 0.7674 - val_loss: 1.2833 - val_accuracy: 0.5817 - val_f1: 0.5768 - val_precision: 0.6383 - val_recall: 0.5267\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.4590 - accuracy: 0.8269 - f1: 0.8268 - precision: 0.8881 - recall: 0.7742 - val_loss: 1.2704 - val_accuracy: 0.5950 - val_f1: 0.5815 - val_precision: 0.6421 - val_recall: 0.5325\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.4435 - accuracy: 0.8326 - f1: 0.8317 - precision: 0.8883 - recall: 0.7826 - val_loss: 1.3228 - val_accuracy: 0.6092 - val_f1: 0.6038 - val_precision: 0.6648 - val_recall: 0.5533\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.4547 - accuracy: 0.8319 - f1: 0.8291 - precision: 0.8878 - recall: 0.7785 - val_loss: 1.3509 - val_accuracy: 0.6108 - val_f1: 0.6095 - val_precision: 0.6587 - val_recall: 0.5675\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.4641 - accuracy: 0.8265 - f1: 0.8210 - precision: 0.8788 - recall: 0.7709 - val_loss: 1.2874 - val_accuracy: 0.5900 - val_f1: 0.5733 - val_precision: 0.6398 - val_recall: 0.5200\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.5107 - accuracy: 0.8132 - f1: 0.8032 - precision: 0.8688 - recall: 0.7473 - val_loss: 1.3034 - val_accuracy: 0.6100 - val_f1: 0.5894 - val_precision: 0.6496 - val_recall: 0.5400\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.4825 - accuracy: 0.8236 - f1: 0.8196 - precision: 0.8814 - recall: 0.7667 - val_loss: 1.3496 - val_accuracy: 0.6175 - val_f1: 0.6085 - val_precision: 0.6563 - val_recall: 0.5675\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 0.3948 - accuracy: 0.8518 - f1: 0.8517 - precision: 0.8904 - recall: 0.8169 - val_loss: 1.3960 - val_accuracy: 0.6033 - val_f1: 0.5948 - val_precision: 0.6338 - val_recall: 0.5608\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3869 - accuracy: 0.8562 - f1: 0.8521 - precision: 0.8902 - recall: 0.8173 - val_loss: 1.4126 - val_accuracy: 0.6108 - val_f1: 0.6067 - val_precision: 0.6416 - val_recall: 0.5758\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3489 - accuracy: 0.8683 - f1: 0.8701 - precision: 0.9012 - recall: 0.8414 - val_loss: 1.6219 - val_accuracy: 0.6050 - val_f1: 0.6035 - val_precision: 0.6375 - val_recall: 0.5733\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.3467 - accuracy: 0.8762 - f1: 0.8713 - precision: 0.9003 - recall: 0.8446 - val_loss: 1.5331 - val_accuracy: 0.6067 - val_f1: 0.6101 - val_precision: 0.6366 - val_recall: 0.5858\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3325 - accuracy: 0.8779 - f1: 0.8791 - precision: 0.9060 - recall: 0.8539 - val_loss: 1.7245 - val_accuracy: 0.6067 - val_f1: 0.5996 - val_precision: 0.6269 - val_recall: 0.5750\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3206 - accuracy: 0.8855 - f1: 0.8860 - precision: 0.9106 - recall: 0.8630 - val_loss: 1.5696 - val_accuracy: 0.6092 - val_f1: 0.6085 - val_precision: 0.6313 - val_recall: 0.5875\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3139 - accuracy: 0.8843 - f1: 0.8799 - precision: 0.9041 - recall: 0.8572 - val_loss: 1.6093 - val_accuracy: 0.6208 - val_f1: 0.6226 - val_precision: 0.6491 - val_recall: 0.5983\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3296 - accuracy: 0.8883 - f1: 0.8807 - precision: 0.9114 - recall: 0.8523 - val_loss: 1.4368 - val_accuracy: 0.6150 - val_f1: 0.6174 - val_precision: 0.6489 - val_recall: 0.5892\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.3032 - accuracy: 0.8873 - f1: 0.8853 - precision: 0.9094 - recall: 0.8625 - val_loss: 1.6269 - val_accuracy: 0.6342 - val_f1: 0.6328 - val_precision: 0.6527 - val_recall: 0.6142\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.2538 - accuracy: 0.9061 - f1: 0.9078 - precision: 0.9285 - recall: 0.8881 - val_loss: 1.7221 - val_accuracy: 0.6417 - val_f1: 0.6423 - val_precision: 0.6676 - val_recall: 0.6192\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.2375 - accuracy: 0.9164 - f1: 0.9186 - precision: 0.9324 - recall: 0.9054 - val_loss: 1.7274 - val_accuracy: 0.6258 - val_f1: 0.6257 - val_precision: 0.6442 - val_recall: 0.6083\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2305 - accuracy: 0.9129 - f1: 0.9134 - precision: 0.9284 - recall: 0.8991 - val_loss: 1.7089 - val_accuracy: 0.6300 - val_f1: 0.6268 - val_precision: 0.6466 - val_recall: 0.6083\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.2209 - accuracy: 0.9203 - f1: 0.9177 - precision: 0.9349 - recall: 0.9013 - val_loss: 1.7704 - val_accuracy: 0.6208 - val_f1: 0.6204 - val_precision: 0.6386 - val_recall: 0.6033\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.2536 - accuracy: 0.9118 - f1: 0.9082 - precision: 0.9275 - recall: 0.8899 - val_loss: 1.7893 - val_accuracy: 0.6400 - val_f1: 0.6401 - val_precision: 0.6588 - val_recall: 0.6225\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2463 - accuracy: 0.9096 - f1: 0.9104 - precision: 0.9286 - recall: 0.8931 - val_loss: 1.7289 - val_accuracy: 0.6317 - val_f1: 0.6289 - val_precision: 0.6455 - val_recall: 0.6133\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2376 - accuracy: 0.9086 - f1: 0.9104 - precision: 0.9305 - recall: 0.8913 - val_loss: 1.6879 - val_accuracy: 0.6325 - val_f1: 0.6284 - val_precision: 0.6435 - val_recall: 0.6142\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.2372 - accuracy: 0.9195 - f1: 0.9187 - precision: 0.9379 - recall: 0.9004 - val_loss: 1.7738 - val_accuracy: 0.6358 - val_f1: 0.6366 - val_precision: 0.6553 - val_recall: 0.6192\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.2440 - accuracy: 0.9121 - f1: 0.9124 - precision: 0.9325 - recall: 0.8935 - val_loss: 1.7595 - val_accuracy: 0.6542 - val_f1: 0.6529 - val_precision: 0.6674 - val_recall: 0.6392\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.2145 - accuracy: 0.9198 - f1: 0.9222 - precision: 0.9364 - recall: 0.9087 - val_loss: 1.7466 - val_accuracy: 0.6575 - val_f1: 0.6598 - val_precision: 0.6803 - val_recall: 0.6408\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.2049 - accuracy: 0.9302 - f1: 0.9288 - precision: 0.9401 - recall: 0.9179 - val_loss: 1.8595 - val_accuracy: 0.6450 - val_f1: 0.6476 - val_precision: 0.6637 - val_recall: 0.6325\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1895 - accuracy: 0.9333 - f1: 0.9372 - precision: 0.9516 - recall: 0.9233 - val_loss: 1.9035 - val_accuracy: 0.6392 - val_f1: 0.6438 - val_precision: 0.6611 - val_recall: 0.6275\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2212 - accuracy: 0.9223 - f1: 0.9241 - precision: 0.9361 - recall: 0.9126 - val_loss: 1.8635 - val_accuracy: 0.6458 - val_f1: 0.6466 - val_precision: 0.6615 - val_recall: 0.6325\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2107 - accuracy: 0.9240 - f1: 0.9242 - precision: 0.9362 - recall: 0.9127 - val_loss: 1.9013 - val_accuracy: 0.6342 - val_f1: 0.6318 - val_precision: 0.6508 - val_recall: 0.6142\n",
            "14/14 [==============================] - 1s 71ms/step - loss: 2.0027 - accuracy: 0.6336 - f1: 0.6342 - precision: 0.6518 - recall: 0.6179\n",
            "[2.0027248859405518, 0.633571445941925, 0.634201169013977, 0.6517502665519714, 0.6178571581840515]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a3bc99-4d71-495a-9815-b1da83ded10d"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 7s 102ms/step - loss: 1.3791 - accuracy: 0.3028 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3662 - val_accuracy: 0.3175 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.3482 - accuracy: 0.3348 - f1: 0.0069 - precision: 0.1004 - recall: 0.0038 - val_loss: 1.3121 - val_accuracy: 0.3717 - val_f1: 0.0318 - val_precision: 0.5294 - val_recall: 0.0167\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.2628 - accuracy: 0.4240 - f1: 0.0974 - precision: 0.6787 - recall: 0.0547 - val_loss: 1.2374 - val_accuracy: 0.4500 - val_f1: 0.2551 - val_precision: 0.6286 - val_recall: 0.1617\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1748 - accuracy: 0.5023 - f1: 0.2957 - precision: 0.6732 - recall: 0.1919 - val_loss: 1.1895 - val_accuracy: 0.4883 - val_f1: 0.3507 - val_precision: 0.5935 - val_recall: 0.2500\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.1093 - accuracy: 0.5333 - f1: 0.4120 - precision: 0.6867 - recall: 0.2965 - val_loss: 1.1956 - val_accuracy: 0.4708 - val_f1: 0.3406 - val_precision: 0.6112 - val_recall: 0.2375\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.0623 - accuracy: 0.5644 - f1: 0.4465 - precision: 0.7081 - recall: 0.3292 - val_loss: 1.1868 - val_accuracy: 0.4792 - val_f1: 0.3630 - val_precision: 0.6130 - val_recall: 0.2592\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 1.0160 - accuracy: 0.5921 - f1: 0.4911 - precision: 0.7335 - recall: 0.3722 - val_loss: 1.1645 - val_accuracy: 0.4925 - val_f1: 0.3744 - val_precision: 0.6441 - val_recall: 0.2658\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.9967 - accuracy: 0.5999 - f1: 0.5088 - precision: 0.7363 - recall: 0.3921 - val_loss: 1.1682 - val_accuracy: 0.5025 - val_f1: 0.4333 - val_precision: 0.6029 - val_recall: 0.3392\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.9607 - accuracy: 0.6183 - f1: 0.5430 - precision: 0.7457 - recall: 0.4289 - val_loss: 1.1794 - val_accuracy: 0.5017 - val_f1: 0.4642 - val_precision: 0.5978 - val_recall: 0.3800\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.9193 - accuracy: 0.6349 - f1: 0.5796 - precision: 0.7462 - recall: 0.4757 - val_loss: 1.1851 - val_accuracy: 0.5017 - val_f1: 0.4567 - val_precision: 0.6027 - val_recall: 0.3683\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.8883 - accuracy: 0.6478 - f1: 0.6005 - precision: 0.7580 - recall: 0.4986 - val_loss: 1.1910 - val_accuracy: 0.5075 - val_f1: 0.4709 - val_precision: 0.6026 - val_recall: 0.3875\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.8581 - accuracy: 0.6550 - f1: 0.6210 - precision: 0.7713 - recall: 0.5209 - val_loss: 1.1778 - val_accuracy: 0.5183 - val_f1: 0.4912 - val_precision: 0.6010 - val_recall: 0.4158\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.7986 - accuracy: 0.6885 - f1: 0.6592 - precision: 0.7976 - recall: 0.5628 - val_loss: 1.2126 - val_accuracy: 0.5167 - val_f1: 0.4921 - val_precision: 0.6017 - val_recall: 0.4167\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.7664 - accuracy: 0.7004 - f1: 0.6782 - precision: 0.7987 - recall: 0.5899 - val_loss: 1.1996 - val_accuracy: 0.5225 - val_f1: 0.5075 - val_precision: 0.6132 - val_recall: 0.4333\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.7102 - accuracy: 0.7275 - f1: 0.7067 - precision: 0.8247 - recall: 0.6190 - val_loss: 1.2474 - val_accuracy: 0.5208 - val_f1: 0.5083 - val_precision: 0.5945 - val_recall: 0.4442\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.6954 - accuracy: 0.7353 - f1: 0.7178 - precision: 0.8268 - recall: 0.6350 - val_loss: 1.3097 - val_accuracy: 0.5192 - val_f1: 0.5010 - val_precision: 0.5839 - val_recall: 0.4392\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.6734 - accuracy: 0.7389 - f1: 0.7241 - precision: 0.8209 - recall: 0.6487 - val_loss: 1.2909 - val_accuracy: 0.5317 - val_f1: 0.5086 - val_precision: 0.5854 - val_recall: 0.4500\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.6514 - accuracy: 0.7511 - f1: 0.7339 - precision: 0.8256 - recall: 0.6621 - val_loss: 1.3003 - val_accuracy: 0.5275 - val_f1: 0.5038 - val_precision: 0.5780 - val_recall: 0.4467\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 0.5919 - accuracy: 0.7791 - f1: 0.7683 - precision: 0.8514 - recall: 0.7015 - val_loss: 1.3328 - val_accuracy: 0.5433 - val_f1: 0.5194 - val_precision: 0.5891 - val_recall: 0.4650\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5683 - accuracy: 0.7876 - f1: 0.7769 - precision: 0.8515 - recall: 0.7155 - val_loss: 1.4027 - val_accuracy: 0.5275 - val_f1: 0.5220 - val_precision: 0.5760 - val_recall: 0.4775\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.5157 - accuracy: 0.8030 - f1: 0.7972 - precision: 0.8636 - recall: 0.7410 - val_loss: 1.5069 - val_accuracy: 0.5408 - val_f1: 0.5194 - val_precision: 0.5756 - val_recall: 0.4733\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.4951 - accuracy: 0.8186 - f1: 0.8162 - precision: 0.8759 - recall: 0.7648 - val_loss: 1.4117 - val_accuracy: 0.5358 - val_f1: 0.5239 - val_precision: 0.5809 - val_recall: 0.4775\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.4925 - accuracy: 0.8175 - f1: 0.8170 - precision: 0.8734 - recall: 0.7681 - val_loss: 1.5225 - val_accuracy: 0.5358 - val_f1: 0.5226 - val_precision: 0.5633 - val_recall: 0.4875\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.4222 - accuracy: 0.8376 - f1: 0.8401 - precision: 0.8834 - recall: 0.8013 - val_loss: 1.5245 - val_accuracy: 0.5383 - val_f1: 0.5210 - val_precision: 0.5656 - val_recall: 0.4833\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.4165 - accuracy: 0.8532 - f1: 0.8453 - precision: 0.8922 - recall: 0.8035 - val_loss: 1.6531 - val_accuracy: 0.5433 - val_f1: 0.5311 - val_precision: 0.5646 - val_recall: 0.5017\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3926 - accuracy: 0.8622 - f1: 0.8532 - precision: 0.8936 - recall: 0.8170 - val_loss: 1.6659 - val_accuracy: 0.5433 - val_f1: 0.5263 - val_precision: 0.5624 - val_recall: 0.4950\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.4099 - accuracy: 0.8515 - f1: 0.8421 - precision: 0.8866 - recall: 0.8022 - val_loss: 1.6578 - val_accuracy: 0.5383 - val_f1: 0.5389 - val_precision: 0.5747 - val_recall: 0.5075\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3779 - accuracy: 0.8653 - f1: 0.8665 - precision: 0.9003 - recall: 0.8355 - val_loss: 1.6066 - val_accuracy: 0.5408 - val_f1: 0.5334 - val_precision: 0.5625 - val_recall: 0.5075\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3317 - accuracy: 0.8804 - f1: 0.8810 - precision: 0.9117 - recall: 0.8525 - val_loss: 1.7570 - val_accuracy: 0.5408 - val_f1: 0.5388 - val_precision: 0.5673 - val_recall: 0.5133\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.3054 - accuracy: 0.8850 - f1: 0.8899 - precision: 0.9232 - recall: 0.8595 - val_loss: 1.7807 - val_accuracy: 0.5500 - val_f1: 0.5485 - val_precision: 0.5733 - val_recall: 0.5258\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3506 - accuracy: 0.8772 - f1: 0.8720 - precision: 0.9054 - recall: 0.8414 - val_loss: 1.8747 - val_accuracy: 0.5433 - val_f1: 0.5377 - val_precision: 0.5587 - val_recall: 0.5183\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.3158 - accuracy: 0.8837 - f1: 0.8816 - precision: 0.9048 - recall: 0.8598 - val_loss: 2.0468 - val_accuracy: 0.5358 - val_f1: 0.5360 - val_precision: 0.5523 - val_recall: 0.5208\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.2751 - accuracy: 0.8994 - f1: 0.8987 - precision: 0.9231 - recall: 0.8761 - val_loss: 2.0268 - val_accuracy: 0.5492 - val_f1: 0.5455 - val_precision: 0.5651 - val_recall: 0.5275\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.2672 - accuracy: 0.9009 - f1: 0.9006 - precision: 0.9209 - recall: 0.8815 - val_loss: 1.9693 - val_accuracy: 0.5483 - val_f1: 0.5443 - val_precision: 0.5596 - val_recall: 0.5300\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2805 - accuracy: 0.8977 - f1: 0.8970 - precision: 0.9170 - recall: 0.8783 - val_loss: 2.0181 - val_accuracy: 0.5467 - val_f1: 0.5441 - val_precision: 0.5638 - val_recall: 0.5258\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.2602 - accuracy: 0.9043 - f1: 0.9030 - precision: 0.9215 - recall: 0.8854 - val_loss: 1.8087 - val_accuracy: 0.5525 - val_f1: 0.5490 - val_precision: 0.5677 - val_recall: 0.5317\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.2376 - accuracy: 0.9074 - f1: 0.9106 - precision: 0.9294 - recall: 0.8927 - val_loss: 1.8594 - val_accuracy: 0.5742 - val_f1: 0.5740 - val_precision: 0.5907 - val_recall: 0.5583\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 90ms/step - loss: 0.2285 - accuracy: 0.9173 - f1: 0.9180 - precision: 0.9324 - recall: 0.9043 - val_loss: 1.9662 - val_accuracy: 0.5683 - val_f1: 0.5559 - val_precision: 0.5758 - val_recall: 0.5375\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.2290 - accuracy: 0.9204 - f1: 0.9202 - precision: 0.9401 - recall: 0.9014 - val_loss: 1.9734 - val_accuracy: 0.5633 - val_f1: 0.5617 - val_precision: 0.5786 - val_recall: 0.5458\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 90ms/step - loss: 0.2090 - accuracy: 0.9233 - f1: 0.9219 - precision: 0.9370 - recall: 0.9074 - val_loss: 1.8979 - val_accuracy: 0.5675 - val_f1: 0.5605 - val_precision: 0.5800 - val_recall: 0.5425\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.2269 - accuracy: 0.9147 - f1: 0.9160 - precision: 0.9300 - recall: 0.9025 - val_loss: 2.1802 - val_accuracy: 0.5708 - val_f1: 0.5647 - val_precision: 0.5805 - val_recall: 0.5500\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1876 - accuracy: 0.9343 - f1: 0.9352 - precision: 0.9477 - recall: 0.9233 - val_loss: 2.0459 - val_accuracy: 0.5483 - val_f1: 0.5451 - val_precision: 0.5613 - val_recall: 0.5300\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.2122 - accuracy: 0.9263 - f1: 0.9235 - precision: 0.9377 - recall: 0.9100 - val_loss: 2.0660 - val_accuracy: 0.5525 - val_f1: 0.5522 - val_precision: 0.5688 - val_recall: 0.5367\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1757 - accuracy: 0.9370 - f1: 0.9353 - precision: 0.9468 - recall: 0.9242 - val_loss: 2.1714 - val_accuracy: 0.5500 - val_f1: 0.5478 - val_precision: 0.5659 - val_recall: 0.5308\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1874 - accuracy: 0.9302 - f1: 0.9303 - precision: 0.9428 - recall: 0.9183 - val_loss: 2.2892 - val_accuracy: 0.5692 - val_f1: 0.5696 - val_precision: 0.5823 - val_recall: 0.5575\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.1841 - accuracy: 0.9304 - f1: 0.9308 - precision: 0.9400 - recall: 0.9220 - val_loss: 2.1699 - val_accuracy: 0.5725 - val_f1: 0.5696 - val_precision: 0.5841 - val_recall: 0.5558\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1702 - accuracy: 0.9442 - f1: 0.9416 - precision: 0.9522 - recall: 0.9313 - val_loss: 2.3661 - val_accuracy: 0.5667 - val_f1: 0.5643 - val_precision: 0.5723 - val_recall: 0.5567\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1552 - accuracy: 0.9424 - f1: 0.9433 - precision: 0.9486 - recall: 0.9381 - val_loss: 2.3016 - val_accuracy: 0.5692 - val_f1: 0.5678 - val_precision: 0.5787 - val_recall: 0.5575\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1413 - accuracy: 0.9507 - f1: 0.9512 - precision: 0.9577 - recall: 0.9448 - val_loss: 2.3191 - val_accuracy: 0.5717 - val_f1: 0.5674 - val_precision: 0.5779 - val_recall: 0.5575\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1463 - accuracy: 0.9514 - f1: 0.9504 - precision: 0.9573 - recall: 0.9437 - val_loss: 2.4447 - val_accuracy: 0.5750 - val_f1: 0.5751 - val_precision: 0.5838 - val_recall: 0.5667\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 88ms/step - loss: 0.1536 - accuracy: 0.9451 - f1: 0.9458 - precision: 0.9525 - recall: 0.9393 - val_loss: 2.5120 - val_accuracy: 0.5775 - val_f1: 0.5736 - val_precision: 0.5835 - val_recall: 0.5642\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1545 - accuracy: 0.9432 - f1: 0.9438 - precision: 0.9486 - recall: 0.9390 - val_loss: 2.8221 - val_accuracy: 0.5842 - val_f1: 0.5825 - val_precision: 0.5903 - val_recall: 0.5750\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1526 - accuracy: 0.9493 - f1: 0.9473 - precision: 0.9540 - recall: 0.9407 - val_loss: 2.3798 - val_accuracy: 0.5875 - val_f1: 0.5874 - val_precision: 0.5961 - val_recall: 0.5792\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.1408 - accuracy: 0.9558 - f1: 0.9559 - precision: 0.9617 - recall: 0.9503 - val_loss: 2.6092 - val_accuracy: 0.5708 - val_f1: 0.5694 - val_precision: 0.5765 - val_recall: 0.5625\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1362 - accuracy: 0.9515 - f1: 0.9511 - precision: 0.9563 - recall: 0.9459 - val_loss: 2.3555 - val_accuracy: 0.6025 - val_f1: 0.6014 - val_precision: 0.6071 - val_recall: 0.5958\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1153 - accuracy: 0.9556 - f1: 0.9553 - precision: 0.9605 - recall: 0.9501 - val_loss: 2.3824 - val_accuracy: 0.5975 - val_f1: 0.5999 - val_precision: 0.6094 - val_recall: 0.5908\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1133 - accuracy: 0.9549 - f1: 0.9560 - precision: 0.9607 - recall: 0.9514 - val_loss: 2.4279 - val_accuracy: 0.5942 - val_f1: 0.5959 - val_precision: 0.6019 - val_recall: 0.5900\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1007 - accuracy: 0.9622 - f1: 0.9611 - precision: 0.9623 - recall: 0.9599 - val_loss: 2.4078 - val_accuracy: 0.5883 - val_f1: 0.5879 - val_precision: 0.5953 - val_recall: 0.5808\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.1176 - accuracy: 0.9594 - f1: 0.9591 - precision: 0.9622 - recall: 0.9561 - val_loss: 2.5754 - val_accuracy: 0.5867 - val_f1: 0.5836 - val_precision: 0.5899 - val_recall: 0.5775\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.0918 - accuracy: 0.9644 - f1: 0.9644 - precision: 0.9652 - recall: 0.9636 - val_loss: 2.5045 - val_accuracy: 0.5842 - val_f1: 0.5823 - val_precision: 0.5898 - val_recall: 0.5750\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 2.2767 - accuracy: 0.6179 - f1: 0.6188 - precision: 0.6280 - recall: 0.6100\n",
            "[2.2766897678375244, 0.6178571581840515, 0.6187962889671326, 0.6279531121253967, 0.6100000143051147]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3311a9-2c71-4981-c8e5-2f19eeae5c80"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 6s 106ms/step - loss: 1.3862 - accuracy: 0.2943 - f1: 3.5420e-04 - precision: 0.0179 - recall: 1.7887e-04 - val_loss: 1.3717 - val_accuracy: 0.3200 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 98ms/step - loss: 1.3521 - accuracy: 0.3432 - f1: 7.7131e-05 - precision: 0.0020 - recall: 3.9337e-05 - val_loss: 1.3625 - val_accuracy: 0.3450 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 1.3360 - accuracy: 0.3443 - f1: 0.0048 - precision: 0.1601 - recall: 0.0025 - val_loss: 1.3605 - val_accuracy: 0.3400 - val_f1: 0.0099 - val_precision: 0.4167 - val_recall: 0.0050\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 1.3052 - accuracy: 0.3949 - f1: 0.0392 - precision: 0.5299 - recall: 0.0206 - val_loss: 1.3602 - val_accuracy: 0.3517 - val_f1: 0.0353 - val_precision: 0.5667 - val_recall: 0.0183\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.2550 - accuracy: 0.4233 - f1: 0.1114 - precision: 0.6414 - recall: 0.0616 - val_loss: 1.3556 - val_accuracy: 0.3617 - val_f1: 0.0896 - val_precision: 0.4662 - val_recall: 0.0500\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 1.1991 - accuracy: 0.4674 - f1: 0.2203 - precision: 0.6533 - recall: 0.1346 - val_loss: 1.3650 - val_accuracy: 0.3650 - val_f1: 0.1857 - val_precision: 0.4633 - val_recall: 0.1167\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.1551 - accuracy: 0.4882 - f1: 0.3283 - precision: 0.6376 - recall: 0.2219 - val_loss: 1.4141 - val_accuracy: 0.3642 - val_f1: 0.2311 - val_precision: 0.4554 - val_recall: 0.1558\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 1.1130 - accuracy: 0.5236 - f1: 0.4011 - precision: 0.6589 - recall: 0.2892 - val_loss: 1.4085 - val_accuracy: 0.3633 - val_f1: 0.2553 - val_precision: 0.4295 - val_recall: 0.1825\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 98ms/step - loss: 1.0637 - accuracy: 0.5486 - f1: 0.4330 - precision: 0.6703 - recall: 0.3217 - val_loss: 1.4520 - val_accuracy: 0.3792 - val_f1: 0.2711 - val_precision: 0.4164 - val_recall: 0.2017\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 1.0319 - accuracy: 0.5590 - f1: 0.4783 - precision: 0.6770 - recall: 0.3707 - val_loss: 1.4681 - val_accuracy: 0.3617 - val_f1: 0.2822 - val_precision: 0.4219 - val_recall: 0.2125\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9909 - accuracy: 0.6036 - f1: 0.5203 - precision: 0.7029 - recall: 0.4146 - val_loss: 1.4895 - val_accuracy: 0.3675 - val_f1: 0.3210 - val_precision: 0.4340 - val_recall: 0.2550\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.9357 - accuracy: 0.6236 - f1: 0.5679 - precision: 0.7157 - recall: 0.4721 - val_loss: 1.5296 - val_accuracy: 0.3800 - val_f1: 0.3089 - val_precision: 0.4116 - val_recall: 0.2475\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.9213 - accuracy: 0.6328 - f1: 0.5827 - precision: 0.7276 - recall: 0.4875 - val_loss: 1.5792 - val_accuracy: 0.3658 - val_f1: 0.3002 - val_precision: 0.3912 - val_recall: 0.2442\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.9157 - accuracy: 0.6303 - f1: 0.5789 - precision: 0.7189 - recall: 0.4858 - val_loss: 1.5867 - val_accuracy: 0.3700 - val_f1: 0.3197 - val_precision: 0.4120 - val_recall: 0.2617\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 98ms/step - loss: 0.9184 - accuracy: 0.6405 - f1: 0.5930 - precision: 0.7087 - recall: 0.5103 - val_loss: 1.6288 - val_accuracy: 0.3900 - val_f1: 0.3397 - val_precision: 0.4076 - val_recall: 0.2917\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.8597 - accuracy: 0.6593 - f1: 0.6230 - precision: 0.7376 - recall: 0.5401 - val_loss: 1.6654 - val_accuracy: 0.3800 - val_f1: 0.3332 - val_precision: 0.3920 - val_recall: 0.2900\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.8111 - accuracy: 0.6730 - f1: 0.6409 - precision: 0.7438 - recall: 0.5637 - val_loss: 1.6787 - val_accuracy: 0.3642 - val_f1: 0.3271 - val_precision: 0.3874 - val_recall: 0.2833\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.8253 - accuracy: 0.6654 - f1: 0.6368 - precision: 0.7346 - recall: 0.5630 - val_loss: 1.6255 - val_accuracy: 0.3875 - val_f1: 0.3375 - val_precision: 0.4014 - val_recall: 0.2917\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.7825 - accuracy: 0.6913 - f1: 0.6614 - precision: 0.7631 - recall: 0.5847 - val_loss: 1.6640 - val_accuracy: 0.3942 - val_f1: 0.3556 - val_precision: 0.4130 - val_recall: 0.3125\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.7825 - accuracy: 0.6946 - f1: 0.6723 - precision: 0.7625 - recall: 0.6021 - val_loss: 1.6336 - val_accuracy: 0.3975 - val_f1: 0.3613 - val_precision: 0.4240 - val_recall: 0.3150\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.7104 - accuracy: 0.7343 - f1: 0.7150 - precision: 0.7984 - recall: 0.6479 - val_loss: 1.7059 - val_accuracy: 0.4000 - val_f1: 0.3778 - val_precision: 0.4255 - val_recall: 0.3400\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.6857 - accuracy: 0.7387 - f1: 0.7177 - precision: 0.7942 - recall: 0.6553 - val_loss: 1.7768 - val_accuracy: 0.3850 - val_f1: 0.3530 - val_precision: 0.4063 - val_recall: 0.3125\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.6960 - accuracy: 0.7271 - f1: 0.7128 - precision: 0.7784 - recall: 0.6580 - val_loss: 1.8150 - val_accuracy: 0.3875 - val_f1: 0.3639 - val_precision: 0.4036 - val_recall: 0.3317\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.7109 - accuracy: 0.7194 - f1: 0.7000 - precision: 0.7657 - recall: 0.6450 - val_loss: 1.8316 - val_accuracy: 0.3875 - val_f1: 0.3801 - val_precision: 0.4199 - val_recall: 0.3475\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.7149 - accuracy: 0.7215 - f1: 0.7039 - precision: 0.7700 - recall: 0.6489 - val_loss: 1.8325 - val_accuracy: 0.3967 - val_f1: 0.3875 - val_precision: 0.4297 - val_recall: 0.3533\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.6727 - accuracy: 0.7419 - f1: 0.7279 - precision: 0.7890 - recall: 0.6762 - val_loss: 1.8781 - val_accuracy: 0.3950 - val_f1: 0.3722 - val_precision: 0.4066 - val_recall: 0.3433\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.6838 - accuracy: 0.7307 - f1: 0.7212 - precision: 0.7787 - recall: 0.6719 - val_loss: 1.8952 - val_accuracy: 0.3917 - val_f1: 0.3677 - val_precision: 0.4017 - val_recall: 0.3392\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.6891 - accuracy: 0.7300 - f1: 0.7213 - precision: 0.7747 - recall: 0.6756 - val_loss: 1.8115 - val_accuracy: 0.4167 - val_f1: 0.4039 - val_precision: 0.4394 - val_recall: 0.3742\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.6586 - accuracy: 0.7396 - f1: 0.7336 - precision: 0.7953 - recall: 0.6816 - val_loss: 1.8901 - val_accuracy: 0.3933 - val_f1: 0.3606 - val_precision: 0.3981 - val_recall: 0.3300\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.6333 - accuracy: 0.7578 - f1: 0.7397 - precision: 0.7937 - recall: 0.6931 - val_loss: 1.9564 - val_accuracy: 0.3933 - val_f1: 0.3665 - val_precision: 0.4027 - val_recall: 0.3367\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.6202 - accuracy: 0.7496 - f1: 0.7460 - precision: 0.8053 - recall: 0.6954 - val_loss: 1.9006 - val_accuracy: 0.4150 - val_f1: 0.3917 - val_precision: 0.4252 - val_recall: 0.3633\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.6193 - accuracy: 0.7575 - f1: 0.7538 - precision: 0.8071 - recall: 0.7077 - val_loss: 1.9181 - val_accuracy: 0.4075 - val_f1: 0.3984 - val_precision: 0.4294 - val_recall: 0.3717\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5996 - accuracy: 0.7632 - f1: 0.7475 - precision: 0.8022 - recall: 0.7005 - val_loss: 1.9824 - val_accuracy: 0.4233 - val_f1: 0.3965 - val_precision: 0.4249 - val_recall: 0.3717\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.5419 - accuracy: 0.7946 - f1: 0.7853 - precision: 0.8239 - recall: 0.7504 - val_loss: 2.0114 - val_accuracy: 0.4075 - val_f1: 0.3900 - val_precision: 0.4157 - val_recall: 0.3675\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.5430 - accuracy: 0.7959 - f1: 0.7881 - precision: 0.8270 - recall: 0.7530 - val_loss: 2.0841 - val_accuracy: 0.4267 - val_f1: 0.4197 - val_precision: 0.4437 - val_recall: 0.3983\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.5131 - accuracy: 0.8027 - f1: 0.8010 - precision: 0.8383 - recall: 0.7671 - val_loss: 2.1392 - val_accuracy: 0.4292 - val_f1: 0.4193 - val_precision: 0.4441 - val_recall: 0.3975\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.4803 - accuracy: 0.8269 - f1: 0.8231 - precision: 0.8554 - recall: 0.7935 - val_loss: 2.1465 - val_accuracy: 0.4100 - val_f1: 0.4034 - val_precision: 0.4291 - val_recall: 0.3808\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4657 - accuracy: 0.8217 - f1: 0.8153 - precision: 0.8476 - recall: 0.7857 - val_loss: 2.1776 - val_accuracy: 0.4075 - val_f1: 0.3827 - val_precision: 0.4097 - val_recall: 0.3592\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.4641 - accuracy: 0.8206 - f1: 0.8173 - precision: 0.8463 - recall: 0.7905 - val_loss: 2.1476 - val_accuracy: 0.4175 - val_f1: 0.4082 - val_precision: 0.4344 - val_recall: 0.3850\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.4888 - accuracy: 0.8075 - f1: 0.8086 - precision: 0.8363 - recall: 0.7828 - val_loss: 2.1286 - val_accuracy: 0.3983 - val_f1: 0.3937 - val_precision: 0.4209 - val_recall: 0.3700\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4572 - accuracy: 0.8228 - f1: 0.8205 - precision: 0.8522 - recall: 0.7915 - val_loss: 2.2620 - val_accuracy: 0.3892 - val_f1: 0.3824 - val_precision: 0.4070 - val_recall: 0.3608\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4631 - accuracy: 0.8279 - f1: 0.8246 - precision: 0.8536 - recall: 0.7979 - val_loss: 2.2403 - val_accuracy: 0.3967 - val_f1: 0.3789 - val_precision: 0.3994 - val_recall: 0.3608\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4722 - accuracy: 0.8100 - f1: 0.8040 - precision: 0.8339 - recall: 0.7764 - val_loss: 2.2448 - val_accuracy: 0.3933 - val_f1: 0.3788 - val_precision: 0.4039 - val_recall: 0.3567\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.4620 - accuracy: 0.8191 - f1: 0.8183 - precision: 0.8474 - recall: 0.7915 - val_loss: 2.3699 - val_accuracy: 0.3867 - val_f1: 0.3793 - val_precision: 0.3990 - val_recall: 0.3617\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.4506 - accuracy: 0.8341 - f1: 0.8304 - precision: 0.8544 - recall: 0.8080 - val_loss: 2.4164 - val_accuracy: 0.3800 - val_f1: 0.3718 - val_precision: 0.3905 - val_recall: 0.3550\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.4575 - accuracy: 0.8201 - f1: 0.8144 - precision: 0.8395 - recall: 0.7911 - val_loss: 2.4015 - val_accuracy: 0.3975 - val_f1: 0.3885 - val_precision: 0.4070 - val_recall: 0.3717\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.4326 - accuracy: 0.8290 - f1: 0.8263 - precision: 0.8509 - recall: 0.8033 - val_loss: 2.4106 - val_accuracy: 0.4008 - val_f1: 0.3915 - val_precision: 0.4087 - val_recall: 0.3758\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.4392 - accuracy: 0.8344 - f1: 0.8318 - precision: 0.8571 - recall: 0.8081 - val_loss: 2.4208 - val_accuracy: 0.4100 - val_f1: 0.4070 - val_precision: 0.4228 - val_recall: 0.3925\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 102ms/step - loss: 0.3763 - accuracy: 0.8581 - f1: 0.8528 - precision: 0.8745 - recall: 0.8323 - val_loss: 2.5360 - val_accuracy: 0.4008 - val_f1: 0.4007 - val_precision: 0.4141 - val_recall: 0.3883\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 102ms/step - loss: 0.3393 - accuracy: 0.8729 - f1: 0.8679 - precision: 0.8868 - recall: 0.8499 - val_loss: 2.5335 - val_accuracy: 0.4058 - val_f1: 0.4033 - val_precision: 0.4185 - val_recall: 0.3892\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3434 - accuracy: 0.8732 - f1: 0.8721 - precision: 0.8876 - recall: 0.8573 - val_loss: 2.5491 - val_accuracy: 0.4067 - val_f1: 0.3967 - val_precision: 0.4173 - val_recall: 0.3783\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3228 - accuracy: 0.8785 - f1: 0.8789 - precision: 0.8929 - recall: 0.8655 - val_loss: 2.6235 - val_accuracy: 0.4042 - val_f1: 0.3935 - val_precision: 0.4091 - val_recall: 0.3792\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.3138 - accuracy: 0.8846 - f1: 0.8815 - precision: 0.8966 - recall: 0.8669 - val_loss: 2.6701 - val_accuracy: 0.4117 - val_f1: 0.4100 - val_precision: 0.4235 - val_recall: 0.3975\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.2927 - accuracy: 0.8906 - f1: 0.8914 - precision: 0.9038 - recall: 0.8793 - val_loss: 2.6939 - val_accuracy: 0.4025 - val_f1: 0.3918 - val_precision: 0.4074 - val_recall: 0.3775\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3002 - accuracy: 0.8853 - f1: 0.8843 - precision: 0.8972 - recall: 0.8718 - val_loss: 2.7154 - val_accuracy: 0.4100 - val_f1: 0.4041 - val_precision: 0.4204 - val_recall: 0.3892\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.2763 - accuracy: 0.8999 - f1: 0.8994 - precision: 0.9103 - recall: 0.8889 - val_loss: 2.8049 - val_accuracy: 0.4100 - val_f1: 0.4057 - val_precision: 0.4180 - val_recall: 0.3942\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.2661 - accuracy: 0.8973 - f1: 0.8980 - precision: 0.9092 - recall: 0.8872 - val_loss: 2.7527 - val_accuracy: 0.4267 - val_f1: 0.4234 - val_precision: 0.4378 - val_recall: 0.4100\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.2618 - accuracy: 0.8965 - f1: 0.8944 - precision: 0.9048 - recall: 0.8844 - val_loss: 2.8998 - val_accuracy: 0.4233 - val_f1: 0.4169 - val_precision: 0.4296 - val_recall: 0.4050\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.2822 - accuracy: 0.8904 - f1: 0.8907 - precision: 0.8998 - recall: 0.8820 - val_loss: 2.8421 - val_accuracy: 0.4392 - val_f1: 0.4304 - val_precision: 0.4406 - val_recall: 0.4208\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 101ms/step - loss: 0.2805 - accuracy: 0.8898 - f1: 0.8894 - precision: 0.8975 - recall: 0.8816 - val_loss: 2.8969 - val_accuracy: 0.4233 - val_f1: 0.4147 - val_precision: 0.4278 - val_recall: 0.4025\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 3.1251 - accuracy: 0.3721 - f1: 0.3720 - precision: 0.3841 - recall: 0.3607\n",
            "[3.1250722408294678, 0.37214285135269165, 0.3719688355922699, 0.38408711552619934, 0.36071428656578064]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924566c6-fa14-47b5-c181-19532b196557"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(64))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 3s 24ms/step - loss: 1.3804 - accuracy: 0.2985 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3608 - val_accuracy: 0.3258 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.3181 - accuracy: 0.3728 - f1: 0.0938 - precision: 0.4410 - recall: 0.0564 - val_loss: 1.1525 - val_accuracy: 0.4942 - val_f1: 0.3320 - val_precision: 0.6953 - val_recall: 0.2192\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 1.0805 - accuracy: 0.5322 - f1: 0.4456 - precision: 0.7036 - recall: 0.3309 - val_loss: 1.0801 - val_accuracy: 0.5258 - val_f1: 0.4485 - val_precision: 0.7165 - val_recall: 0.3275\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.9701 - accuracy: 0.5880 - f1: 0.5215 - precision: 0.7373 - recall: 0.4071 - val_loss: 1.0430 - val_accuracy: 0.5475 - val_f1: 0.4877 - val_precision: 0.6961 - val_recall: 0.3767\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.8789 - accuracy: 0.6226 - f1: 0.5770 - precision: 0.7649 - recall: 0.4645 - val_loss: 1.0220 - val_accuracy: 0.5750 - val_f1: 0.5242 - val_precision: 0.7014 - val_recall: 0.4192\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.8033 - accuracy: 0.6823 - f1: 0.6421 - precision: 0.7794 - recall: 0.5475 - val_loss: 0.9636 - val_accuracy: 0.6183 - val_f1: 0.6024 - val_precision: 0.6914 - val_recall: 0.5342\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.6852 - accuracy: 0.7409 - f1: 0.7261 - precision: 0.8164 - recall: 0.6545 - val_loss: 0.9585 - val_accuracy: 0.6500 - val_f1: 0.6363 - val_precision: 0.7223 - val_recall: 0.5692\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.6003 - accuracy: 0.7853 - f1: 0.7736 - precision: 0.8515 - recall: 0.7094 - val_loss: 0.9559 - val_accuracy: 0.6642 - val_f1: 0.6515 - val_precision: 0.7255 - val_recall: 0.5917\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.5880 - accuracy: 0.7886 - f1: 0.7792 - precision: 0.8462 - recall: 0.7225 - val_loss: 0.9071 - val_accuracy: 0.6617 - val_f1: 0.6546 - val_precision: 0.7358 - val_recall: 0.5900\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.5367 - accuracy: 0.8147 - f1: 0.8024 - precision: 0.8749 - recall: 0.7418 - val_loss: 0.9351 - val_accuracy: 0.6725 - val_f1: 0.6605 - val_precision: 0.7526 - val_recall: 0.5892\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.5208 - accuracy: 0.8167 - f1: 0.8119 - precision: 0.8764 - recall: 0.7571 - val_loss: 0.9163 - val_accuracy: 0.6917 - val_f1: 0.6912 - val_precision: 0.7440 - val_recall: 0.6458\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4759 - accuracy: 0.8319 - f1: 0.8326 - precision: 0.8822 - recall: 0.7890 - val_loss: 0.9489 - val_accuracy: 0.6975 - val_f1: 0.6962 - val_precision: 0.7382 - val_recall: 0.6592\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.4100 - accuracy: 0.8620 - f1: 0.8604 - precision: 0.8977 - recall: 0.8263 - val_loss: 1.0112 - val_accuracy: 0.6900 - val_f1: 0.6916 - val_precision: 0.7320 - val_recall: 0.6558\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3693 - accuracy: 0.8754 - f1: 0.8793 - precision: 0.9086 - recall: 0.8522 - val_loss: 0.9524 - val_accuracy: 0.6908 - val_f1: 0.6893 - val_precision: 0.7396 - val_recall: 0.6458\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3459 - accuracy: 0.8862 - f1: 0.8894 - precision: 0.9180 - recall: 0.8628 - val_loss: 0.9975 - val_accuracy: 0.6817 - val_f1: 0.6880 - val_precision: 0.7278 - val_recall: 0.6525\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3600 - accuracy: 0.8835 - f1: 0.8837 - precision: 0.9114 - recall: 0.8580 - val_loss: 0.9449 - val_accuracy: 0.7008 - val_f1: 0.6867 - val_precision: 0.7425 - val_recall: 0.6392\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3203 - accuracy: 0.8994 - f1: 0.9022 - precision: 0.9298 - recall: 0.8766 - val_loss: 0.9574 - val_accuracy: 0.7083 - val_f1: 0.7061 - val_precision: 0.7414 - val_recall: 0.6742\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3025 - accuracy: 0.9068 - f1: 0.9064 - precision: 0.9251 - recall: 0.8886 - val_loss: 0.9587 - val_accuracy: 0.6892 - val_f1: 0.6890 - val_precision: 0.7288 - val_recall: 0.6542\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3083 - accuracy: 0.9017 - f1: 0.9011 - precision: 0.9246 - recall: 0.8789 - val_loss: 0.9352 - val_accuracy: 0.6975 - val_f1: 0.6953 - val_precision: 0.7371 - val_recall: 0.6583\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.3341 - accuracy: 0.8918 - f1: 0.8935 - precision: 0.9195 - recall: 0.8693 - val_loss: 0.9622 - val_accuracy: 0.6850 - val_f1: 0.6762 - val_precision: 0.7175 - val_recall: 0.6400\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3472 - accuracy: 0.8871 - f1: 0.8828 - precision: 0.9109 - recall: 0.8567 - val_loss: 1.0140 - val_accuracy: 0.6858 - val_f1: 0.6826 - val_precision: 0.7110 - val_recall: 0.6567\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2839 - accuracy: 0.9127 - f1: 0.9135 - precision: 0.9299 - recall: 0.8978 - val_loss: 1.0173 - val_accuracy: 0.7108 - val_f1: 0.7112 - val_precision: 0.7381 - val_recall: 0.6867\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2637 - accuracy: 0.9191 - f1: 0.9151 - precision: 0.9335 - recall: 0.8974 - val_loss: 1.0180 - val_accuracy: 0.7042 - val_f1: 0.7022 - val_precision: 0.7322 - val_recall: 0.6750\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2764 - accuracy: 0.9126 - f1: 0.9127 - precision: 0.9330 - recall: 0.8937 - val_loss: 1.0523 - val_accuracy: 0.6958 - val_f1: 0.6964 - val_precision: 0.7185 - val_recall: 0.6758\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.2553 - accuracy: 0.9163 - f1: 0.9189 - precision: 0.9348 - recall: 0.9037 - val_loss: 1.0760 - val_accuracy: 0.7108 - val_f1: 0.7156 - val_precision: 0.7340 - val_recall: 0.6983\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.2237 - accuracy: 0.9311 - f1: 0.9305 - precision: 0.9401 - recall: 0.9212 - val_loss: 1.1056 - val_accuracy: 0.6975 - val_f1: 0.7031 - val_precision: 0.7187 - val_recall: 0.6883\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1888 - accuracy: 0.9437 - f1: 0.9452 - precision: 0.9522 - recall: 0.9383 - val_loss: 1.1408 - val_accuracy: 0.7033 - val_f1: 0.7072 - val_precision: 0.7190 - val_recall: 0.6958\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1661 - accuracy: 0.9471 - f1: 0.9486 - precision: 0.9540 - recall: 0.9433 - val_loss: 1.2392 - val_accuracy: 0.7008 - val_f1: 0.7031 - val_precision: 0.7123 - val_recall: 0.6942\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1687 - accuracy: 0.9476 - f1: 0.9472 - precision: 0.9533 - recall: 0.9413 - val_loss: 1.3472 - val_accuracy: 0.6958 - val_f1: 0.6980 - val_precision: 0.7054 - val_recall: 0.6908\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1778 - accuracy: 0.9458 - f1: 0.9456 - precision: 0.9506 - recall: 0.9408 - val_loss: 1.3314 - val_accuracy: 0.6925 - val_f1: 0.6902 - val_precision: 0.7008 - val_recall: 0.6800\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1586 - accuracy: 0.9460 - f1: 0.9481 - precision: 0.9532 - recall: 0.9430 - val_loss: 1.4182 - val_accuracy: 0.6850 - val_f1: 0.6875 - val_precision: 0.6970 - val_recall: 0.6783\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1569 - accuracy: 0.9454 - f1: 0.9474 - precision: 0.9532 - recall: 0.9418 - val_loss: 1.3812 - val_accuracy: 0.6875 - val_f1: 0.6870 - val_precision: 0.6942 - val_recall: 0.6800\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1605 - accuracy: 0.9463 - f1: 0.9464 - precision: 0.9513 - recall: 0.9416 - val_loss: 1.4136 - val_accuracy: 0.7033 - val_f1: 0.7052 - val_precision: 0.7142 - val_recall: 0.6967\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1396 - accuracy: 0.9540 - f1: 0.9539 - precision: 0.9584 - recall: 0.9496 - val_loss: 1.4715 - val_accuracy: 0.6817 - val_f1: 0.6872 - val_precision: 0.6956 - val_recall: 0.6792\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1555 - accuracy: 0.9490 - f1: 0.9487 - precision: 0.9537 - recall: 0.9438 - val_loss: 1.3581 - val_accuracy: 0.6992 - val_f1: 0.6996 - val_precision: 0.7087 - val_recall: 0.6908\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1584 - accuracy: 0.9475 - f1: 0.9457 - precision: 0.9511 - recall: 0.9405 - val_loss: 1.3990 - val_accuracy: 0.6942 - val_f1: 0.6950 - val_precision: 0.7037 - val_recall: 0.6867\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1578 - accuracy: 0.9459 - f1: 0.9464 - precision: 0.9509 - recall: 0.9420 - val_loss: 1.4315 - val_accuracy: 0.6708 - val_f1: 0.6741 - val_precision: 0.6826 - val_recall: 0.6658\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1797 - accuracy: 0.9421 - f1: 0.9421 - precision: 0.9469 - recall: 0.9375 - val_loss: 1.3698 - val_accuracy: 0.6925 - val_f1: 0.6926 - val_precision: 0.7023 - val_recall: 0.6833\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1857 - accuracy: 0.9378 - f1: 0.9381 - precision: 0.9408 - recall: 0.9355 - val_loss: 1.3139 - val_accuracy: 0.7008 - val_f1: 0.7013 - val_precision: 0.7095 - val_recall: 0.6933\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1869 - accuracy: 0.9380 - f1: 0.9366 - precision: 0.9450 - recall: 0.9284 - val_loss: 1.4020 - val_accuracy: 0.6917 - val_f1: 0.6921 - val_precision: 0.7013 - val_recall: 0.6833\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1366 - accuracy: 0.9508 - f1: 0.9512 - precision: 0.9560 - recall: 0.9465 - val_loss: 1.4751 - val_accuracy: 0.6892 - val_f1: 0.6886 - val_precision: 0.6957 - val_recall: 0.6817\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1315 - accuracy: 0.9528 - f1: 0.9536 - precision: 0.9564 - recall: 0.9508 - val_loss: 1.4220 - val_accuracy: 0.7100 - val_f1: 0.7135 - val_precision: 0.7241 - val_recall: 0.7033\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1168 - accuracy: 0.9566 - f1: 0.9570 - precision: 0.9593 - recall: 0.9546 - val_loss: 1.4995 - val_accuracy: 0.6975 - val_f1: 0.6973 - val_precision: 0.7075 - val_recall: 0.6875\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1065 - accuracy: 0.9553 - f1: 0.9569 - precision: 0.9596 - recall: 0.9543 - val_loss: 1.3409 - val_accuracy: 0.7033 - val_f1: 0.7044 - val_precision: 0.7159 - val_recall: 0.6933\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1175 - accuracy: 0.9530 - f1: 0.9531 - precision: 0.9571 - recall: 0.9492 - val_loss: 1.5679 - val_accuracy: 0.6958 - val_f1: 0.6966 - val_precision: 0.7017 - val_recall: 0.6917\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1022 - accuracy: 0.9583 - f1: 0.9593 - precision: 0.9624 - recall: 0.9561 - val_loss: 1.4952 - val_accuracy: 0.7175 - val_f1: 0.7195 - val_precision: 0.7293 - val_recall: 0.7100\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1185 - accuracy: 0.9582 - f1: 0.9573 - precision: 0.9601 - recall: 0.9546 - val_loss: 1.7593 - val_accuracy: 0.7042 - val_f1: 0.7045 - val_precision: 0.7099 - val_recall: 0.6992\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1456 - accuracy: 0.9461 - f1: 0.9458 - precision: 0.9489 - recall: 0.9428 - val_loss: 1.6393 - val_accuracy: 0.6958 - val_f1: 0.6931 - val_precision: 0.6998 - val_recall: 0.6867\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1096 - accuracy: 0.9555 - f1: 0.9554 - precision: 0.9583 - recall: 0.9526 - val_loss: 1.5564 - val_accuracy: 0.7083 - val_f1: 0.7066 - val_precision: 0.7118 - val_recall: 0.7017\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0892 - accuracy: 0.9672 - f1: 0.9682 - precision: 0.9707 - recall: 0.9656 - val_loss: 1.5994 - val_accuracy: 0.7083 - val_f1: 0.7094 - val_precision: 0.7156 - val_recall: 0.7033\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0864 - accuracy: 0.9627 - f1: 0.9618 - precision: 0.9653 - recall: 0.9583 - val_loss: 1.6855 - val_accuracy: 0.6992 - val_f1: 0.6963 - val_precision: 0.7027 - val_recall: 0.6900\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0936 - accuracy: 0.9609 - f1: 0.9607 - precision: 0.9618 - recall: 0.9596 - val_loss: 1.6556 - val_accuracy: 0.7058 - val_f1: 0.7053 - val_precision: 0.7116 - val_recall: 0.6992\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1112 - accuracy: 0.9565 - f1: 0.9560 - precision: 0.9607 - recall: 0.9514 - val_loss: 1.6314 - val_accuracy: 0.6950 - val_f1: 0.6970 - val_precision: 0.7034 - val_recall: 0.6908\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1168 - accuracy: 0.9561 - f1: 0.9570 - precision: 0.9596 - recall: 0.9545 - val_loss: 1.5783 - val_accuracy: 0.7125 - val_f1: 0.7114 - val_precision: 0.7163 - val_recall: 0.7067\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0812 - accuracy: 0.9644 - f1: 0.9652 - precision: 0.9677 - recall: 0.9628 - val_loss: 1.6994 - val_accuracy: 0.7083 - val_f1: 0.7110 - val_precision: 0.7171 - val_recall: 0.7050\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.1135 - accuracy: 0.9552 - f1: 0.9552 - precision: 0.9572 - recall: 0.9533 - val_loss: 1.6010 - val_accuracy: 0.6925 - val_f1: 0.6960 - val_precision: 0.7048 - val_recall: 0.6875\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0839 - accuracy: 0.9663 - f1: 0.9666 - precision: 0.9672 - recall: 0.9659 - val_loss: 1.6301 - val_accuracy: 0.7125 - val_f1: 0.7120 - val_precision: 0.7166 - val_recall: 0.7075\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0667 - accuracy: 0.9737 - f1: 0.9748 - precision: 0.9763 - recall: 0.9733 - val_loss: 1.8039 - val_accuracy: 0.7042 - val_f1: 0.7015 - val_precision: 0.7074 - val_recall: 0.6958\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0747 - accuracy: 0.9710 - f1: 0.9712 - precision: 0.9737 - recall: 0.9688 - val_loss: 1.6360 - val_accuracy: 0.7092 - val_f1: 0.7079 - val_precision: 0.7135 - val_recall: 0.7025\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0605 - accuracy: 0.9710 - f1: 0.9716 - precision: 0.9738 - recall: 0.9695 - val_loss: 1.7053 - val_accuracy: 0.7058 - val_f1: 0.7069 - val_precision: 0.7123 - val_recall: 0.7017\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6213 - accuracy: 0.7214 - f1: 0.7224 - precision: 0.7264 - recall: 0.7186\n",
            "[1.6212527751922607, 0.7214285731315613, 0.7224202752113342, 0.7263627648353577, 0.7185713052749634]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd17fa5-2626-40b0-b926-05b950fe1184"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 4s 31ms/step - loss: 1.3826 - accuracy: 0.2940 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3343 - val_accuracy: 0.3408 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 1.2724 - accuracy: 0.4111 - f1: 0.0853 - precision: 0.4238 - recall: 0.0529 - val_loss: 1.1175 - val_accuracy: 0.5350 - val_f1: 0.3937 - val_precision: 0.6978 - val_recall: 0.2758\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 1.0489 - accuracy: 0.5584 - f1: 0.4793 - precision: 0.7090 - recall: 0.3651 - val_loss: 0.9810 - val_accuracy: 0.5933 - val_f1: 0.5347 - val_precision: 0.7167 - val_recall: 0.4275\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.8754 - accuracy: 0.6629 - f1: 0.6042 - precision: 0.7587 - recall: 0.5042 - val_loss: 0.9106 - val_accuracy: 0.6433 - val_f1: 0.6059 - val_precision: 0.7475 - val_recall: 0.5108\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.7736 - accuracy: 0.6965 - f1: 0.6676 - precision: 0.7939 - recall: 0.5776 - val_loss: 0.9055 - val_accuracy: 0.6483 - val_f1: 0.6195 - val_precision: 0.7463 - val_recall: 0.5300\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.7149 - accuracy: 0.7304 - f1: 0.7094 - precision: 0.8193 - recall: 0.6268 - val_loss: 0.8983 - val_accuracy: 0.6625 - val_f1: 0.6488 - val_precision: 0.7367 - val_recall: 0.5800\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.6407 - accuracy: 0.7640 - f1: 0.7430 - precision: 0.8303 - recall: 0.6729 - val_loss: 0.8976 - val_accuracy: 0.6717 - val_f1: 0.6691 - val_precision: 0.7421 - val_recall: 0.6100\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.5587 - accuracy: 0.8015 - f1: 0.7909 - precision: 0.8628 - recall: 0.7307 - val_loss: 0.9562 - val_accuracy: 0.6517 - val_f1: 0.6495 - val_precision: 0.7072 - val_recall: 0.6008\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.5141 - accuracy: 0.8072 - f1: 0.8046 - precision: 0.8621 - recall: 0.7547 - val_loss: 0.9591 - val_accuracy: 0.6583 - val_f1: 0.6585 - val_precision: 0.7003 - val_recall: 0.6217\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.4903 - accuracy: 0.8264 - f1: 0.8238 - precision: 0.8740 - recall: 0.7800 - val_loss: 0.9376 - val_accuracy: 0.6808 - val_f1: 0.6737 - val_precision: 0.7144 - val_recall: 0.6375\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.4514 - accuracy: 0.8369 - f1: 0.8327 - precision: 0.8753 - recall: 0.7945 - val_loss: 0.8894 - val_accuracy: 0.7017 - val_f1: 0.6997 - val_precision: 0.7343 - val_recall: 0.6683\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3913 - accuracy: 0.8620 - f1: 0.8592 - precision: 0.8907 - recall: 0.8301 - val_loss: 0.8754 - val_accuracy: 0.7017 - val_f1: 0.7049 - val_precision: 0.7404 - val_recall: 0.6733\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.3481 - accuracy: 0.8742 - f1: 0.8773 - precision: 0.9038 - recall: 0.8526 - val_loss: 0.9098 - val_accuracy: 0.7017 - val_f1: 0.7057 - val_precision: 0.7397 - val_recall: 0.6750\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.2934 - accuracy: 0.8918 - f1: 0.8883 - precision: 0.9104 - recall: 0.8675 - val_loss: 0.9697 - val_accuracy: 0.7042 - val_f1: 0.7008 - val_precision: 0.7259 - val_recall: 0.6775\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.2549 - accuracy: 0.9072 - f1: 0.9065 - precision: 0.9238 - recall: 0.8899 - val_loss: 0.9736 - val_accuracy: 0.7133 - val_f1: 0.7100 - val_precision: 0.7268 - val_recall: 0.6942\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1897 - accuracy: 0.9317 - f1: 0.9300 - precision: 0.9411 - recall: 0.9192 - val_loss: 1.0527 - val_accuracy: 0.7092 - val_f1: 0.7082 - val_precision: 0.7256 - val_recall: 0.6917\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1655 - accuracy: 0.9349 - f1: 0.9338 - precision: 0.9427 - recall: 0.9251 - val_loss: 1.1531 - val_accuracy: 0.7050 - val_f1: 0.7036 - val_precision: 0.7171 - val_recall: 0.6908\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1862 - accuracy: 0.9288 - f1: 0.9296 - precision: 0.9368 - recall: 0.9226 - val_loss: 1.1632 - val_accuracy: 0.7033 - val_f1: 0.7058 - val_precision: 0.7151 - val_recall: 0.6967\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1549 - accuracy: 0.9427 - f1: 0.9437 - precision: 0.9495 - recall: 0.9381 - val_loss: 1.1721 - val_accuracy: 0.7133 - val_f1: 0.7107 - val_precision: 0.7201 - val_recall: 0.7017\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1337 - accuracy: 0.9480 - f1: 0.9483 - precision: 0.9530 - recall: 0.9438 - val_loss: 1.1782 - val_accuracy: 0.7050 - val_f1: 0.7092 - val_precision: 0.7205 - val_recall: 0.6983\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1236 - accuracy: 0.9495 - f1: 0.9510 - precision: 0.9555 - recall: 0.9466 - val_loss: 1.3244 - val_accuracy: 0.6933 - val_f1: 0.6901 - val_precision: 0.6988 - val_recall: 0.6817\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1324 - accuracy: 0.9491 - f1: 0.9498 - precision: 0.9539 - recall: 0.9459 - val_loss: 1.3929 - val_accuracy: 0.6933 - val_f1: 0.6957 - val_precision: 0.7050 - val_recall: 0.6867\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1259 - accuracy: 0.9538 - f1: 0.9536 - precision: 0.9561 - recall: 0.9510 - val_loss: 1.4298 - val_accuracy: 0.6875 - val_f1: 0.6887 - val_precision: 0.6985 - val_recall: 0.6792\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1141 - accuracy: 0.9569 - f1: 0.9561 - precision: 0.9589 - recall: 0.9532 - val_loss: 1.4951 - val_accuracy: 0.6925 - val_f1: 0.6950 - val_precision: 0.7035 - val_recall: 0.6867\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.1323 - accuracy: 0.9465 - f1: 0.9458 - precision: 0.9489 - recall: 0.9428 - val_loss: 1.4927 - val_accuracy: 0.7058 - val_f1: 0.7041 - val_precision: 0.7108 - val_recall: 0.6975\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1224 - accuracy: 0.9544 - f1: 0.9553 - precision: 0.9572 - recall: 0.9535 - val_loss: 1.4532 - val_accuracy: 0.6883 - val_f1: 0.6892 - val_precision: 0.6979 - val_recall: 0.6808\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1337 - accuracy: 0.9453 - f1: 0.9461 - precision: 0.9501 - recall: 0.9422 - val_loss: 1.3555 - val_accuracy: 0.6933 - val_f1: 0.6917 - val_precision: 0.7013 - val_recall: 0.6825\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0912 - accuracy: 0.9580 - f1: 0.9585 - precision: 0.9603 - recall: 0.9567 - val_loss: 1.4347 - val_accuracy: 0.6967 - val_f1: 0.6983 - val_precision: 0.7077 - val_recall: 0.6892\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0890 - accuracy: 0.9641 - f1: 0.9644 - precision: 0.9657 - recall: 0.9632 - val_loss: 1.4981 - val_accuracy: 0.6967 - val_f1: 0.6989 - val_precision: 0.7062 - val_recall: 0.6917\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0823 - accuracy: 0.9651 - f1: 0.9650 - precision: 0.9661 - recall: 0.9639 - val_loss: 1.5094 - val_accuracy: 0.7067 - val_f1: 0.7081 - val_precision: 0.7130 - val_recall: 0.7033\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0737 - accuracy: 0.9648 - f1: 0.9651 - precision: 0.9660 - recall: 0.9642 - val_loss: 1.5238 - val_accuracy: 0.7150 - val_f1: 0.7178 - val_precision: 0.7250 - val_recall: 0.7108\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0677 - accuracy: 0.9673 - f1: 0.9676 - precision: 0.9686 - recall: 0.9666 - val_loss: 1.5622 - val_accuracy: 0.7008 - val_f1: 0.7044 - val_precision: 0.7123 - val_recall: 0.6967\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0703 - accuracy: 0.9661 - f1: 0.9662 - precision: 0.9670 - recall: 0.9653 - val_loss: 1.5708 - val_accuracy: 0.7150 - val_f1: 0.7153 - val_precision: 0.7190 - val_recall: 0.7117\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0679 - accuracy: 0.9683 - f1: 0.9683 - precision: 0.9690 - recall: 0.9676 - val_loss: 1.5996 - val_accuracy: 0.7125 - val_f1: 0.7114 - val_precision: 0.7153 - val_recall: 0.7075\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0737 - accuracy: 0.9664 - f1: 0.9666 - precision: 0.9678 - recall: 0.9654 - val_loss: 1.5404 - val_accuracy: 0.7167 - val_f1: 0.7158 - val_precision: 0.7225 - val_recall: 0.7092\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0648 - accuracy: 0.9694 - f1: 0.9697 - precision: 0.9707 - recall: 0.9686 - val_loss: 1.5967 - val_accuracy: 0.7217 - val_f1: 0.7246 - val_precision: 0.7292 - val_recall: 0.7200\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0604 - accuracy: 0.9669 - f1: 0.9670 - precision: 0.9674 - recall: 0.9666 - val_loss: 1.6385 - val_accuracy: 0.7167 - val_f1: 0.7157 - val_precision: 0.7181 - val_recall: 0.7133\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0527 - accuracy: 0.9735 - f1: 0.9738 - precision: 0.9749 - recall: 0.9727 - val_loss: 1.6735 - val_accuracy: 0.7200 - val_f1: 0.7212 - val_precision: 0.7268 - val_recall: 0.7158\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0504 - accuracy: 0.9732 - f1: 0.9733 - precision: 0.9735 - recall: 0.9732 - val_loss: 1.7251 - val_accuracy: 0.7150 - val_f1: 0.7150 - val_precision: 0.7193 - val_recall: 0.7108\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0507 - accuracy: 0.9752 - f1: 0.9746 - precision: 0.9757 - recall: 0.9735 - val_loss: 1.7733 - val_accuracy: 0.7108 - val_f1: 0.7141 - val_precision: 0.7182 - val_recall: 0.7100\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0528 - accuracy: 0.9771 - f1: 0.9768 - precision: 0.9775 - recall: 0.9761 - val_loss: 1.7879 - val_accuracy: 0.7150 - val_f1: 0.7161 - val_precision: 0.7198 - val_recall: 0.7125\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 0.0460 - accuracy: 0.9768 - f1: 0.9775 - precision: 0.9784 - recall: 0.9766 - val_loss: 1.7672 - val_accuracy: 0.7125 - val_f1: 0.7118 - val_precision: 0.7145 - val_recall: 0.7092\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0539 - accuracy: 0.9718 - f1: 0.9716 - precision: 0.9724 - recall: 0.9709 - val_loss: 1.7916 - val_accuracy: 0.7050 - val_f1: 0.7043 - val_precision: 0.7069 - val_recall: 0.7017\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0516 - accuracy: 0.9749 - f1: 0.9739 - precision: 0.9751 - recall: 0.9728 - val_loss: 1.8019 - val_accuracy: 0.7108 - val_f1: 0.7111 - val_precision: 0.7147 - val_recall: 0.7075\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0511 - accuracy: 0.9727 - f1: 0.9727 - precision: 0.9732 - recall: 0.9722 - val_loss: 1.8659 - val_accuracy: 0.7108 - val_f1: 0.7101 - val_precision: 0.7128 - val_recall: 0.7075\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0525 - accuracy: 0.9711 - f1: 0.9713 - precision: 0.9717 - recall: 0.9710 - val_loss: 1.8580 - val_accuracy: 0.7117 - val_f1: 0.7119 - val_precision: 0.7155 - val_recall: 0.7083\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0462 - accuracy: 0.9762 - f1: 0.9763 - precision: 0.9773 - recall: 0.9753 - val_loss: 1.9449 - val_accuracy: 0.7033 - val_f1: 0.7029 - val_precision: 0.7049 - val_recall: 0.7008\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0458 - accuracy: 0.9747 - f1: 0.9747 - precision: 0.9753 - recall: 0.9740 - val_loss: 1.8171 - val_accuracy: 0.7125 - val_f1: 0.7109 - val_precision: 0.7127 - val_recall: 0.7092\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0461 - accuracy: 0.9751 - f1: 0.9751 - precision: 0.9752 - recall: 0.9750 - val_loss: 1.8959 - val_accuracy: 0.7117 - val_f1: 0.7118 - val_precision: 0.7136 - val_recall: 0.7100\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0521 - accuracy: 0.9724 - f1: 0.9727 - precision: 0.9735 - recall: 0.9720 - val_loss: 1.9781 - val_accuracy: 0.7108 - val_f1: 0.7109 - val_precision: 0.7127 - val_recall: 0.7092\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0536 - accuracy: 0.9716 - f1: 0.9717 - precision: 0.9719 - recall: 0.9715 - val_loss: 1.8103 - val_accuracy: 0.6958 - val_f1: 0.6970 - val_precision: 0.7041 - val_recall: 0.6900\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1141 - accuracy: 0.9575 - f1: 0.9577 - precision: 0.9587 - recall: 0.9567 - val_loss: 1.6381 - val_accuracy: 0.6875 - val_f1: 0.6865 - val_precision: 0.6923 - val_recall: 0.6808\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.1285 - accuracy: 0.9442 - f1: 0.9449 - precision: 0.9486 - recall: 0.9413 - val_loss: 1.5770 - val_accuracy: 0.7058 - val_f1: 0.7073 - val_precision: 0.7131 - val_recall: 0.7017\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0746 - accuracy: 0.9666 - f1: 0.9657 - precision: 0.9668 - recall: 0.9647 - val_loss: 1.6333 - val_accuracy: 0.7100 - val_f1: 0.7126 - val_precision: 0.7153 - val_recall: 0.7100\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0559 - accuracy: 0.9749 - f1: 0.9751 - precision: 0.9756 - recall: 0.9746 - val_loss: 1.6974 - val_accuracy: 0.7217 - val_f1: 0.7214 - val_precision: 0.7253 - val_recall: 0.7175\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0437 - accuracy: 0.9770 - f1: 0.9764 - precision: 0.9772 - recall: 0.9756 - val_loss: 1.8188 - val_accuracy: 0.7225 - val_f1: 0.7223 - val_precision: 0.7272 - val_recall: 0.7175\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0460 - accuracy: 0.9757 - f1: 0.9762 - precision: 0.9773 - recall: 0.9752 - val_loss: 1.9763 - val_accuracy: 0.7092 - val_f1: 0.7104 - val_precision: 0.7126 - val_recall: 0.7083\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0471 - accuracy: 0.9725 - f1: 0.9726 - precision: 0.9735 - recall: 0.9717 - val_loss: 1.9996 - val_accuracy: 0.7033 - val_f1: 0.7046 - val_precision: 0.7075 - val_recall: 0.7017\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0555 - accuracy: 0.9720 - f1: 0.9721 - precision: 0.9728 - recall: 0.9715 - val_loss: 1.8096 - val_accuracy: 0.7200 - val_f1: 0.7208 - val_precision: 0.7242 - val_recall: 0.7175\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 0.0484 - accuracy: 0.9748 - f1: 0.9747 - precision: 0.9751 - recall: 0.9743 - val_loss: 1.7430 - val_accuracy: 0.7167 - val_f1: 0.7170 - val_precision: 0.7216 - val_recall: 0.7125\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.8300 - accuracy: 0.7179 - f1: 0.7190 - precision: 0.7224 - recall: 0.7157\n",
            "[1.8300162553787231, 0.7178571224212646, 0.7190290689468384, 0.7224035859107971, 0.7157143354415894]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f700ed46-93ab-40ce-c6bf-54b27ef7df67"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 3s 36ms/step - loss: 1.3852 - accuracy: 0.2794 - f1: 2.9915e-04 - precision: 0.0142 - recall: 1.5125e-04 - val_loss: 1.3408 - val_accuracy: 0.3475 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.3107 - accuracy: 0.3739 - f1: 0.0486 - precision: 0.5276 - recall: 0.0267 - val_loss: 1.2514 - val_accuracy: 0.4308 - val_f1: 0.2471 - val_precision: 0.5265 - val_recall: 0.1617\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.2298 - accuracy: 0.4526 - f1: 0.2344 - precision: 0.5762 - recall: 0.1501 - val_loss: 1.2615 - val_accuracy: 0.3967 - val_f1: 0.2385 - val_precision: 0.5293 - val_recall: 0.1550\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.1591 - accuracy: 0.5027 - f1: 0.3254 - precision: 0.6254 - recall: 0.2233 - val_loss: 1.2140 - val_accuracy: 0.4700 - val_f1: 0.4246 - val_precision: 0.5731 - val_recall: 0.3375\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 1.0479 - accuracy: 0.5782 - f1: 0.4992 - precision: 0.6848 - recall: 0.3949 - val_loss: 1.2273 - val_accuracy: 0.4975 - val_f1: 0.4759 - val_precision: 0.5514 - val_recall: 0.4192\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.9702 - accuracy: 0.6163 - f1: 0.5688 - precision: 0.7041 - recall: 0.4803 - val_loss: 1.2196 - val_accuracy: 0.5183 - val_f1: 0.4905 - val_precision: 0.5716 - val_recall: 0.4300\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.9552 - accuracy: 0.6274 - f1: 0.5842 - precision: 0.7136 - recall: 0.4981 - val_loss: 1.2448 - val_accuracy: 0.5300 - val_f1: 0.5123 - val_precision: 0.5839 - val_recall: 0.4567\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.9133 - accuracy: 0.6516 - f1: 0.6166 - precision: 0.7352 - recall: 0.5343 - val_loss: 1.2765 - val_accuracy: 0.4958 - val_f1: 0.4708 - val_precision: 0.5432 - val_recall: 0.4158\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.8850 - accuracy: 0.6633 - f1: 0.6342 - precision: 0.7411 - recall: 0.5575 - val_loss: 1.2791 - val_accuracy: 0.5167 - val_f1: 0.4873 - val_precision: 0.5644 - val_recall: 0.4292\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.8351 - accuracy: 0.6756 - f1: 0.6472 - precision: 0.7567 - recall: 0.5667 - val_loss: 1.4568 - val_accuracy: 0.4983 - val_f1: 0.4886 - val_precision: 0.5347 - val_recall: 0.4500\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.8240 - accuracy: 0.6895 - f1: 0.6621 - precision: 0.7533 - recall: 0.5921 - val_loss: 1.4028 - val_accuracy: 0.4975 - val_f1: 0.4814 - val_precision: 0.5394 - val_recall: 0.4350\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.8121 - accuracy: 0.7038 - f1: 0.6745 - precision: 0.7647 - recall: 0.6047 - val_loss: 1.4004 - val_accuracy: 0.4675 - val_f1: 0.4467 - val_precision: 0.5093 - val_recall: 0.3983\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.7266 - accuracy: 0.7263 - f1: 0.7131 - precision: 0.7945 - recall: 0.6475 - val_loss: 1.4381 - val_accuracy: 0.4767 - val_f1: 0.4725 - val_precision: 0.5367 - val_recall: 0.4225\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.6728 - accuracy: 0.7456 - f1: 0.7349 - precision: 0.8107 - recall: 0.6731 - val_loss: 1.4567 - val_accuracy: 0.5175 - val_f1: 0.4976 - val_precision: 0.5470 - val_recall: 0.4567\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.6751 - accuracy: 0.7514 - f1: 0.7354 - precision: 0.8031 - recall: 0.6790 - val_loss: 1.4258 - val_accuracy: 0.5133 - val_f1: 0.5027 - val_precision: 0.5569 - val_recall: 0.4583\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.6253 - accuracy: 0.7566 - f1: 0.7534 - precision: 0.8176 - recall: 0.6997 - val_loss: 1.5096 - val_accuracy: 0.5083 - val_f1: 0.4956 - val_precision: 0.5344 - val_recall: 0.4625\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.5514 - accuracy: 0.8050 - f1: 0.7916 - precision: 0.8441 - recall: 0.7457 - val_loss: 1.5849 - val_accuracy: 0.5200 - val_f1: 0.5014 - val_precision: 0.5373 - val_recall: 0.4700\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.5406 - accuracy: 0.8037 - f1: 0.7979 - precision: 0.8386 - recall: 0.7615 - val_loss: 1.6814 - val_accuracy: 0.4867 - val_f1: 0.4774 - val_precision: 0.5128 - val_recall: 0.4467\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.5221 - accuracy: 0.8029 - f1: 0.8001 - precision: 0.8468 - recall: 0.7590 - val_loss: 1.6822 - val_accuracy: 0.4925 - val_f1: 0.4886 - val_precision: 0.5214 - val_recall: 0.4600\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.4641 - accuracy: 0.8317 - f1: 0.8286 - precision: 0.8658 - recall: 0.7950 - val_loss: 1.7490 - val_accuracy: 0.4900 - val_f1: 0.4862 - val_precision: 0.5139 - val_recall: 0.4617\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4399 - accuracy: 0.8428 - f1: 0.8387 - precision: 0.8736 - recall: 0.8069 - val_loss: 1.8397 - val_accuracy: 0.4525 - val_f1: 0.4461 - val_precision: 0.4771 - val_recall: 0.4192\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4903 - accuracy: 0.8246 - f1: 0.8224 - precision: 0.8542 - recall: 0.7933 - val_loss: 1.8646 - val_accuracy: 0.4792 - val_f1: 0.4753 - val_precision: 0.5060 - val_recall: 0.4483\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4804 - accuracy: 0.8247 - f1: 0.8185 - precision: 0.8552 - recall: 0.7853 - val_loss: 2.0402 - val_accuracy: 0.4733 - val_f1: 0.4666 - val_precision: 0.4905 - val_recall: 0.4450\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4377 - accuracy: 0.8371 - f1: 0.8363 - precision: 0.8694 - recall: 0.8059 - val_loss: 2.0863 - val_accuracy: 0.4442 - val_f1: 0.4365 - val_precision: 0.4605 - val_recall: 0.4150\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.5054 - accuracy: 0.8177 - f1: 0.8158 - precision: 0.8499 - recall: 0.7848 - val_loss: 2.1126 - val_accuracy: 0.4075 - val_f1: 0.3999 - val_precision: 0.4273 - val_recall: 0.3758\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.5247 - accuracy: 0.8089 - f1: 0.8051 - precision: 0.8377 - recall: 0.7753 - val_loss: 2.0793 - val_accuracy: 0.4275 - val_f1: 0.4217 - val_precision: 0.4461 - val_recall: 0.4000\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.5624 - accuracy: 0.7857 - f1: 0.7827 - precision: 0.8160 - recall: 0.7523 - val_loss: 1.9547 - val_accuracy: 0.4567 - val_f1: 0.4554 - val_precision: 0.4800 - val_recall: 0.4333\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4563 - accuracy: 0.8313 - f1: 0.8297 - precision: 0.8579 - recall: 0.8037 - val_loss: 1.9395 - val_accuracy: 0.4842 - val_f1: 0.4884 - val_precision: 0.5095 - val_recall: 0.4692\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3657 - accuracy: 0.8629 - f1: 0.8613 - precision: 0.8860 - recall: 0.8383 - val_loss: 1.9838 - val_accuracy: 0.5008 - val_f1: 0.4876 - val_precision: 0.5087 - val_recall: 0.4683\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.3439 - accuracy: 0.8773 - f1: 0.8738 - precision: 0.9011 - recall: 0.8484 - val_loss: 2.1113 - val_accuracy: 0.4650 - val_f1: 0.4664 - val_precision: 0.4862 - val_recall: 0.4483\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3087 - accuracy: 0.8891 - f1: 0.8889 - precision: 0.9116 - recall: 0.8677 - val_loss: 2.2198 - val_accuracy: 0.4792 - val_f1: 0.4708 - val_precision: 0.4898 - val_recall: 0.4533\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2777 - accuracy: 0.8988 - f1: 0.9000 - precision: 0.9194 - recall: 0.8816 - val_loss: 2.3017 - val_accuracy: 0.4833 - val_f1: 0.4815 - val_precision: 0.4965 - val_recall: 0.4675\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.3082 - accuracy: 0.8926 - f1: 0.8873 - precision: 0.9062 - recall: 0.8694 - val_loss: 2.3659 - val_accuracy: 0.4592 - val_f1: 0.4541 - val_precision: 0.4711 - val_recall: 0.4383\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3607 - accuracy: 0.8714 - f1: 0.8706 - precision: 0.8871 - recall: 0.8551 - val_loss: 2.3980 - val_accuracy: 0.4675 - val_f1: 0.4643 - val_precision: 0.4825 - val_recall: 0.4475\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3402 - accuracy: 0.8738 - f1: 0.8751 - precision: 0.8951 - recall: 0.8563 - val_loss: 2.3749 - val_accuracy: 0.4650 - val_f1: 0.4612 - val_precision: 0.4797 - val_recall: 0.4442\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3343 - accuracy: 0.8840 - f1: 0.8857 - precision: 0.9048 - recall: 0.8677 - val_loss: 2.4950 - val_accuracy: 0.4683 - val_f1: 0.4661 - val_precision: 0.4807 - val_recall: 0.4525\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3427 - accuracy: 0.8809 - f1: 0.8781 - precision: 0.8935 - recall: 0.8634 - val_loss: 2.4533 - val_accuracy: 0.4592 - val_f1: 0.4526 - val_precision: 0.4680 - val_recall: 0.4383\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3779 - accuracy: 0.8695 - f1: 0.8672 - precision: 0.8840 - recall: 0.8512 - val_loss: 2.3513 - val_accuracy: 0.4608 - val_f1: 0.4613 - val_precision: 0.4750 - val_recall: 0.4483\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3688 - accuracy: 0.8698 - f1: 0.8717 - precision: 0.8885 - recall: 0.8556 - val_loss: 2.3572 - val_accuracy: 0.4708 - val_f1: 0.4641 - val_precision: 0.4793 - val_recall: 0.4500\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4071 - accuracy: 0.8511 - f1: 0.8531 - precision: 0.8729 - recall: 0.8344 - val_loss: 2.3705 - val_accuracy: 0.4442 - val_f1: 0.4469 - val_precision: 0.4623 - val_recall: 0.4325\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4149 - accuracy: 0.8562 - f1: 0.8536 - precision: 0.8741 - recall: 0.8341 - val_loss: 2.2932 - val_accuracy: 0.4817 - val_f1: 0.4818 - val_precision: 0.4934 - val_recall: 0.4708\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3098 - accuracy: 0.8872 - f1: 0.8869 - precision: 0.9018 - recall: 0.8727 - val_loss: 2.2739 - val_accuracy: 0.4817 - val_f1: 0.4735 - val_precision: 0.4899 - val_recall: 0.4583\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3324 - accuracy: 0.8783 - f1: 0.8771 - precision: 0.8937 - recall: 0.8613 - val_loss: 2.1798 - val_accuracy: 0.4825 - val_f1: 0.4787 - val_precision: 0.4972 - val_recall: 0.4617\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.3014 - accuracy: 0.8883 - f1: 0.8888 - precision: 0.9026 - recall: 0.8756 - val_loss: 2.2226 - val_accuracy: 0.5067 - val_f1: 0.5016 - val_precision: 0.5168 - val_recall: 0.4875\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.2404 - accuracy: 0.9140 - f1: 0.9115 - precision: 0.9221 - recall: 0.9013 - val_loss: 2.3366 - val_accuracy: 0.4942 - val_f1: 0.4950 - val_precision: 0.5093 - val_recall: 0.4817\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2103 - accuracy: 0.9261 - f1: 0.9269 - precision: 0.9355 - recall: 0.9185 - val_loss: 2.4192 - val_accuracy: 0.4900 - val_f1: 0.4895 - val_precision: 0.5004 - val_recall: 0.4792\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1912 - accuracy: 0.9335 - f1: 0.9329 - precision: 0.9403 - recall: 0.9257 - val_loss: 2.4277 - val_accuracy: 0.4942 - val_f1: 0.4943 - val_precision: 0.5068 - val_recall: 0.4825\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1766 - accuracy: 0.9397 - f1: 0.9387 - precision: 0.9452 - recall: 0.9324 - val_loss: 2.4636 - val_accuracy: 0.4933 - val_f1: 0.4946 - val_precision: 0.5038 - val_recall: 0.4858\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.1759 - accuracy: 0.9401 - f1: 0.9400 - precision: 0.9469 - recall: 0.9333 - val_loss: 2.6281 - val_accuracy: 0.5150 - val_f1: 0.5161 - val_precision: 0.5241 - val_recall: 0.5083\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1862 - accuracy: 0.9368 - f1: 0.9365 - precision: 0.9421 - recall: 0.9310 - val_loss: 2.6787 - val_accuracy: 0.5283 - val_f1: 0.5248 - val_precision: 0.5324 - val_recall: 0.5175\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.1872 - accuracy: 0.9317 - f1: 0.9315 - precision: 0.9363 - recall: 0.9269 - val_loss: 2.5789 - val_accuracy: 0.5233 - val_f1: 0.5244 - val_precision: 0.5315 - val_recall: 0.5175\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1829 - accuracy: 0.9379 - f1: 0.9375 - precision: 0.9446 - recall: 0.9305 - val_loss: 2.6844 - val_accuracy: 0.5183 - val_f1: 0.5190 - val_precision: 0.5257 - val_recall: 0.5125\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1825 - accuracy: 0.9438 - f1: 0.9432 - precision: 0.9490 - recall: 0.9375 - val_loss: 2.6736 - val_accuracy: 0.4908 - val_f1: 0.4902 - val_precision: 0.4982 - val_recall: 0.4825\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1993 - accuracy: 0.9285 - f1: 0.9290 - precision: 0.9346 - recall: 0.9234 - val_loss: 2.6666 - val_accuracy: 0.5067 - val_f1: 0.5053 - val_precision: 0.5142 - val_recall: 0.4967\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1649 - accuracy: 0.9418 - f1: 0.9408 - precision: 0.9470 - recall: 0.9348 - val_loss: 2.8001 - val_accuracy: 0.5242 - val_f1: 0.5202 - val_precision: 0.5318 - val_recall: 0.5092\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1893 - accuracy: 0.9360 - f1: 0.9358 - precision: 0.9414 - recall: 0.9302 - val_loss: 2.8899 - val_accuracy: 0.5192 - val_f1: 0.5132 - val_precision: 0.5190 - val_recall: 0.5075\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1781 - accuracy: 0.9366 - f1: 0.9373 - precision: 0.9422 - recall: 0.9325 - val_loss: 2.8614 - val_accuracy: 0.5075 - val_f1: 0.5054 - val_precision: 0.5118 - val_recall: 0.4992\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1922 - accuracy: 0.9354 - f1: 0.9355 - precision: 0.9406 - recall: 0.9305 - val_loss: 2.8645 - val_accuracy: 0.4983 - val_f1: 0.4973 - val_precision: 0.5049 - val_recall: 0.4900\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.1728 - accuracy: 0.9427 - f1: 0.9420 - precision: 0.9482 - recall: 0.9359 - val_loss: 3.0227 - val_accuracy: 0.5142 - val_f1: 0.5137 - val_precision: 0.5210 - val_recall: 0.5067\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.1611 - accuracy: 0.9493 - f1: 0.9489 - precision: 0.9521 - recall: 0.9458 - val_loss: 2.8317 - val_accuracy: 0.5100 - val_f1: 0.5066 - val_precision: 0.5135 - val_recall: 0.5000\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 2.9080 - accuracy: 0.5179 - f1: 0.5151 - precision: 0.5234 - recall: 0.5071\n",
            "[2.9079625606536865, 0.5178571343421936, 0.5151183009147644, 0.5234401822090149, 0.5071429014205933]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "c2870d51-276f-4552-cdcd-0d89ee16725e"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "print(history_sg_bi.history)\r\n",
        "plt.title(\"Skip-Gram + Bi-LSTM accuracy, val accuracy vs epochs Graph\")\r\n",
        "plt.plot(history_sg_bi.history['val_accuracy'],c='b',label='val_accuracy')\r\n",
        "plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()\r\n",
        "plt.title(\"GloVe + Bi-LSTM accuracy, val accuracy vs epochs Graph\")\r\n",
        "plt.plot(history_glove_bi.history['val_accuracy'],c='b',label='val_accuracy')\r\n",
        "plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.6121428608894348, 0.6257143020629883, 0.37714284658432007]\n",
            "cbow [0.633571445941925, 0.6178571581840515, 0.37214285135269165]\n",
            "glove [0.7214285731315613, 0.7178571224212646, 0.5178571343421936]\n",
            "{'loss': [1.375142216682434, 1.3396177291870117, 1.2521896362304688, 1.1581766605377197, 1.0858362913131714, 1.0320643186569214, 0.9951009750366211, 0.9591707587242126, 0.9261817336082458, 0.8738539218902588, 0.8384911417961121, 0.798815131187439, 0.7759623527526855, 0.7301204800605774, 0.7116013169288635, 0.6791383028030396, 0.6477429270744324, 0.6495519280433655, 0.6303984522819519, 0.5908117890357971, 0.5566393733024597, 0.5117102265357971, 0.46300312876701355, 0.4244723916053772, 0.42704054713249207, 0.37878942489624023, 0.36232924461364746, 0.3547220230102539, 0.3171303868293762, 0.34451305866241455, 0.3447644114494324, 0.31023168563842773, 0.27504628896713257, 0.25325676798820496, 0.24344445765018463, 0.21954932808876038, 0.2319929599761963, 0.19346991181373596, 0.18479502201080322, 0.16640283167362213, 0.17841273546218872, 0.16301007568836212, 0.1601337045431137, 0.17400112748146057, 0.15304604172706604, 0.18225382268428802, 0.20591937005519867, 0.14874136447906494, 0.1536727398633957, 0.15339133143424988, 0.13700808584690094, 0.15030428767204285, 0.14815086126327515, 0.15105105936527252, 0.13139483332633972, 0.13408967852592468, 0.1209208220243454, 0.09382717311382294, 0.0839681625366211, 0.08403096348047256], 'accuracy': [0.3134782612323761, 0.34282609820365906, 0.448913037776947, 0.511956512928009, 0.5602173805236816, 0.5834782719612122, 0.5963043570518494, 0.6134782433509827, 0.6352173686027527, 0.6569564938545227, 0.6717391014099121, 0.6893478035926819, 0.7017391324043274, 0.7132608890533447, 0.7260869741439819, 0.7371739149093628, 0.747826099395752, 0.7486956715583801, 0.7539130449295044, 0.7730434536933899, 0.7926086783409119, 0.8106521964073181, 0.8332608938217163, 0.8406521677970886, 0.842391312122345, 0.8678261041641235, 0.8665217161178589, 0.8717391490936279, 0.8832608461380005, 0.8715217113494873, 0.872826099395752, 0.8893478512763977, 0.9021739363670349, 0.9073913097381592, 0.9104347825050354, 0.9182608723640442, 0.9143478274345398, 0.9313043355941772, 0.9339130520820618, 0.9399999976158142, 0.9360869526863098, 0.9410869479179382, 0.947826087474823, 0.9402173757553101, 0.9458695650100708, 0.9367391467094421, 0.9291304349899292, 0.9463043212890625, 0.9428260922431946, 0.9423912763595581, 0.9480434656143188, 0.9452173709869385, 0.9491304159164429, 0.9454348087310791, 0.9558695554733276, 0.9543478488922119, 0.9569565057754517, 0.9652174115180969, 0.967391312122345, 0.9671739339828491], 'f1': [0.0, 0.022202320396900177, 0.16323867440223694, 0.32603874802589417, 0.4360347390174866, 0.4925174415111542, 0.5307578444480896, 0.5562093257904053, 0.5738110542297363, 0.6079455614089966, 0.6302989721298218, 0.65915846824646, 0.6658639907836914, 0.6884515285491943, 0.7105028033256531, 0.719692587852478, 0.7343766689300537, 0.7306938767433167, 0.7378624677658081, 0.7630313634872437, 0.7825433611869812, 0.8048157095909119, 0.8294655084609985, 0.8365360498428345, 0.8403209447860718, 0.8596574664115906, 0.8656487464904785, 0.8690329194068909, 0.8837726712226868, 0.8717212677001953, 0.8727812767028809, 0.8873346447944641, 0.9040027260780334, 0.9065149426460266, 0.911002516746521, 0.9201103448867798, 0.9124326109886169, 0.9304652810096741, 0.935147762298584, 0.9400079250335693, 0.9364808201789856, 0.9412948489189148, 0.9458270072937012, 0.9402798414230347, 0.9455042481422424, 0.9361278414726257, 0.9289721846580505, 0.9464832544326782, 0.9431790113449097, 0.941648542881012, 0.9481320381164551, 0.9457882642745972, 0.9488576054573059, 0.945514440536499, 0.9558581709861755, 0.9543341994285583, 0.9568017721176147, 0.9651197195053101, 0.9673385620117188, 0.9667748808860779], 'precision': [0.0, 0.2374396175146103, 0.6923314332962036, 0.6759527921676636, 0.6984062194824219, 0.7131511569023132, 0.7132383584976196, 0.7347801923751831, 0.7409826517105103, 0.7575311064720154, 0.7761486172676086, 0.793977677822113, 0.7947511076927185, 0.8018383979797363, 0.811261773109436, 0.8106909394264221, 0.8282782435417175, 0.8195001482963562, 0.8223055005073547, 0.8327696323394775, 0.8469902276992798, 0.864538848400116, 0.8768863081932068, 0.8797236680984497, 0.8787060379981995, 0.8971225023269653, 0.8979547619819641, 0.8970993161201477, 0.9089463353157043, 0.8965121507644653, 0.8994858860969543, 0.90826016664505, 0.9216974377632141, 0.9208333492279053, 0.9242371320724487, 0.9333611130714417, 0.9232844710350037, 0.9411041140556335, 0.9438018202781677, 0.9462421536445618, 0.9449496269226074, 0.9475261569023132, 0.9519960284233093, 0.9459313154220581, 0.9500182867050171, 0.9424046277999878, 0.937298059463501, 0.9528914093971252, 0.9495530128479004, 0.947801411151886, 0.9519772529602051, 0.9505773782730103, 0.9534727931022644, 0.9489367008209229, 0.9602863192558289, 0.9567574262619019, 0.9599629044532776, 0.9674473404884338, 0.9688248038291931, 0.9687981605529785], 'recall': [0.0, 0.01195652224123478, 0.09630435705184937, 0.21913042664527893, 0.3193478286266327, 0.37913045287132263, 0.4247826337814331, 0.4491303861141205, 0.47065216302871704, 0.5099998712539673, 0.5328260660171509, 0.5658695101737976, 0.574999988079071, 0.6043477654457092, 0.632826030254364, 0.6476086974143982, 0.6606521010398865, 0.6604347229003906, 0.6702173352241516, 0.7049999833106995, 0.7284782528877258, 0.7534782290458679, 0.7873912453651428, 0.7978259921073914, 0.8056522607803345, 0.8256522417068481, 0.8358696699142456, 0.8430436253547668, 0.8602174520492554, 0.8484783172607422, 0.8480435013771057, 0.8676087856292725, 0.8871738314628601, 0.8928261399269104, 0.8982608914375305, 0.9073911905288696, 0.9019565582275391, 0.9202174544334412, 0.9267390966415405, 0.9339130520820618, 0.9282608032226562, 0.9352172613143921, 0.9397823214530945, 0.9347826242446899, 0.9410868883132935, 0.9299998879432678, 0.9208694696426392, 0.9402173757553101, 0.9369565844535828, 0.9356523752212524, 0.944347620010376, 0.9410869479179382, 0.9443480372428894, 0.9421737790107727, 0.9515218138694763, 0.951956570148468, 0.9536956548690796, 0.9628259539604187, 0.9658693075180054, 0.9647826552391052], 'val_loss': [1.3671046495437622, 1.3208978176116943, 1.2604577541351318, 1.1931949853897095, 1.1813642978668213, 1.1942914724349976, 1.195623755455017, 1.210976004600525, 1.2356265783309937, 1.2670589685440063, 1.3744434118270874, 1.3510842323303223, 1.3592745065689087, 1.3468977212905884, 1.2616666555404663, 1.346528172492981, 1.3286994695663452, 1.2768447399139404, 1.3482812643051147, 1.3855441808700562, 1.3865855932235718, 1.402452826499939, 1.4843238592147827, 1.5119448900222778, 1.5665489435195923, 1.6773862838745117, 1.883907675743103, 1.9357584714889526, 2.0741286277770996, 1.838368535041809, 1.6794484853744507, 1.786474347114563, 1.7333370447158813, 1.830675482749939, 1.9570600986480713, 2.1059606075286865, 1.9162330627441406, 1.9716731309890747, 2.1976170539855957, 2.1550960540771484, 2.352583646774292, 2.295563220977783, 2.481173276901245, 2.282142162322998, 2.2022533416748047, 2.487248182296753, 2.41072154045105, 2.3928115367889404, 2.5238091945648193, 2.5536675453186035, 2.526643991470337, 2.345583200454712, 2.3813817501068115, 2.492600202560425, 2.4416165351867676, 2.2775018215179443, 2.3447763919830322, 2.3459701538085938, 2.364635467529297, 2.4062340259552], 'val_accuracy': [0.3174999952316284, 0.3700000047683716, 0.4375, 0.48750001192092896, 0.5016666650772095, 0.5074999928474426, 0.5058333277702332, 0.5058333277702332, 0.5074999928474426, 0.5183333158493042, 0.5, 0.4816666543483734, 0.49000000953674316, 0.4983333349227905, 0.5091666579246521, 0.5133333206176758, 0.5199999809265137, 0.49416667222976685, 0.5141666531562805, 0.5091666579246521, 0.5274999737739563, 0.5416666865348816, 0.5508333444595337, 0.5358333587646484, 0.5558333396911621, 0.5441666841506958, 0.5441666841506958, 0.5600000023841858, 0.5516666769981384, 0.5616666674613953, 0.5699999928474426, 0.5699999928474426, 0.5716666579246521, 0.5816666483879089, 0.5699999928474426, 0.5741666555404663, 0.5633333325386047, 0.5649999976158142, 0.5741666555404663, 0.5608333349227905, 0.5600000023841858, 0.5608333349227905, 0.5641666650772095, 0.5633333325386047, 0.5799999833106995, 0.5550000071525574, 0.5716666579246521, 0.5625, 0.5708333253860474, 0.5791666507720947, 0.5799999833106995, 0.5774999856948853, 0.5791666507720947, 0.5674999952316284, 0.5874999761581421, 0.6050000190734863, 0.5958333611488342, 0.6066666841506958, 0.6016666889190674, 0.6000000238418579], 'val_f1': [0.0, 0.12035828828811646, 0.26580265164375305, 0.3588235080242157, 0.39736637473106384, 0.4406605064868927, 0.457490473985672, 0.45337092876434326, 0.47105780243873596, 0.47749629616737366, 0.46587809920310974, 0.45946136116981506, 0.4692420959472656, 0.47904172539711, 0.49504661560058594, 0.48072031140327454, 0.47522544860839844, 0.46743345260620117, 0.4959120750427246, 0.485566645860672, 0.5125477313995361, 0.53361576795578, 0.546800434589386, 0.5298234820365906, 0.5511954426765442, 0.5455520153045654, 0.5437723398208618, 0.5585380792617798, 0.5504558086395264, 0.5617645382881165, 0.5673239827156067, 0.5621200799942017, 0.5691354274749756, 0.5785327553749084, 0.5670488476753235, 0.571922242641449, 0.562268853187561, 0.5625089406967163, 0.577289879322052, 0.5598689913749695, 0.5560235381126404, 0.5599449276924133, 0.562902569770813, 0.5584032535552979, 0.5800034403800964, 0.5553720593452454, 0.5650703310966492, 0.5591064095497131, 0.571692168712616, 0.5768278241157532, 0.5758400559425354, 0.5735489726066589, 0.580219030380249, 0.5649121999740601, 0.5849000811576843, 0.6003977656364441, 0.5918260216712952, 0.6048489212989807, 0.601011335849762, 0.5992763042449951], 'val_precision': [0.0, 0.5554476380348206, 0.5608665943145752, 0.5807757377624512, 0.6245879530906677, 0.597575306892395, 0.5942339301109314, 0.5793564915657043, 0.5701916217803955, 0.5753244161605835, 0.5564525723457336, 0.5422731637954712, 0.5488962531089783, 0.5615172982215881, 0.5850769877433777, 0.5754132866859436, 0.5782803297042847, 0.5726655125617981, 0.5638968348503113, 0.560676097869873, 0.5802918076515198, 0.5818970203399658, 0.5947344899177551, 0.5714661478996277, 0.5844334959983826, 0.5741088390350342, 0.5704172253608704, 0.5786791443824768, 0.5623579025268555, 0.5845299363136292, 0.5878168940544128, 0.5796170830726624, 0.5907392501831055, 0.5956379175186157, 0.5834296941757202, 0.5900104641914368, 0.5788826942443848, 0.581359326839447, 0.5921880006790161, 0.5766335725784302, 0.5676358342170715, 0.5730505585670471, 0.5719743371009827, 0.5689562559127808, 0.5959012508392334, 0.5671348571777344, 0.5746908187866211, 0.5713374018669128, 0.5804304480552673, 0.5848008394241333, 0.5872811079025269, 0.5824840664863586, 0.5891344547271729, 0.5734706521034241, 0.5934467911720276, 0.6121318936347961, 0.5997021794319153, 0.6133339405059814, 0.6116149425506592, 0.6027995944023132], 'val_recall': [0.0, 0.06833334267139435, 0.17499999701976776, 0.26083335280418396, 0.29249998927116394, 0.35083332657814026, 0.372499942779541, 0.3733333349227905, 0.40166667103767395, 0.40916669368743896, 0.40166667103767395, 0.39916661381721497, 0.41083332896232605, 0.4183332920074463, 0.4299999475479126, 0.41333338618278503, 0.4041666090488434, 0.3958333432674408, 0.44333335757255554, 0.42916667461395264, 0.4599999487400055, 0.4933333694934845, 0.5066667199134827, 0.49416670203208923, 0.5216666460037231, 0.5199999809265137, 0.5199999809265137, 0.5400000214576721, 0.5391666889190674, 0.5408332943916321, 0.5483333468437195, 0.5458333492279053, 0.5491666793823242, 0.5624999403953552, 0.5516666769981384, 0.5550000071525574, 0.5466666221618652, 0.5450000166893005, 0.5633333325386047, 0.5441666841506958, 0.5450000166893005, 0.54749995470047, 0.5541666150093079, 0.5483333468437195, 0.5649999976158142, 0.544166624546051, 0.5558333396911621, 0.54749995470047, 0.5633333325386047, 0.5691666603088379, 0.5649999976158142, 0.5649999976158142, 0.5716667175292969, 0.5566666722297668, 0.5766666531562805, 0.5891667008399963, 0.5841667056083679, 0.596666693687439, 0.5908333659172058, 0.5958333611488342]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}