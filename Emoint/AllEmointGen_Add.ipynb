{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointGen_Add.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointGen_Add.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08140cba-5099-49bb-b168-9eea3994ec32"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87af6b6-d98c-4713-af7f-d03f159c536a"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['with', 'their', 'faggy', 'colors', 'are', 'nice', 'is', 'ok', 'too', 'even', 'tho', 'some', 'might', 'take', 'offense', 'because', 'words', 'lol'], ['unbelievable', 'takes', '10', 'minutes', 'to', 'get', 'through', 'to', 'then', 'there', 'is', 'a', 'fault', 'and', 'the', 'call', 'hangs', 'up', 'treatcustomersfairly'], ['well', 'i', 'did', 'hear', 'once', 'before', 'that', 'girls', 'are', 'attracted', 'to', 'men', 'that', 'lok', 'like', 'their', 'dad', 'ok', 'hand'], ['agreed', 'so', 'tired', 'of', 'this', 'nonsense', 'soros', 'must', 'be', 'elated'], ['by', 'the', 'way', 'i', 'am', 'wearing', 'the', 'smile', 'you', 'gave', 'me', 'today', 'n', 'you', 'me']]\n",
            "[[1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 200\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "data_tweet = [nltk.pos_tag(x) for x in data_tweet]\r\n",
        "def getsent(word,tag):\r\n",
        "  res=0\r\n",
        "  try:\r\n",
        "    x = swn.senti_synset(word+'.'+tag[0].lower()+'.01')\r\n",
        "    res =  (x.pos_score()-x.neg_score())\r\n",
        "  finally:\r\n",
        "    return res \r\n",
        "data_tweet = [[(i[0],i[1],getsent(i[0],i[1])) for i in x] for x in data_tweet]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-1,0),min(i+2,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])/2,tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-1,0),min(i+2,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])/2,tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-1,0),min(i+2,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])/2,tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-1,0),min(i+2,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])/2,tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 60"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft3p6c6UJO_V"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a140d9cc-cda8-4f47-82c9-95e75c235849"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(64))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy',f1,precision,recall])\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 14s 140ms/step - loss: 1.3822 - accuracy: 0.3057 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3742 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3787 - accuracy: 0.3054 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3688 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3771 - accuracy: 0.3106 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3703 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3755 - accuracy: 0.3164 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3703 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3765 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3687 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3770 - accuracy: 0.3119 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3716 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3755 - accuracy: 0.3144 - f1: 1.4730e-04 - precision: 0.0074 - recall: 7.4389e-05 - val_loss: 1.3716 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3761 - accuracy: 0.3113 - f1: 1.6007e-04 - precision: 0.0081 - recall: 8.0836e-05 - val_loss: 1.3694 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3749 - accuracy: 0.3150 - f1: 1.7324e-04 - precision: 0.0087 - recall: 8.7485e-05 - val_loss: 1.3717 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3760 - accuracy: 0.3110 - f1: 1.8683e-04 - precision: 0.0094 - recall: 9.4349e-05 - val_loss: 1.3710 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3761 - accuracy: 0.3147 - f1: 2.0087e-04 - precision: 0.0101 - recall: 1.0144e-04 - val_loss: 1.3711 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3760 - accuracy: 0.3105 - f1: 2.1540e-04 - precision: 0.0109 - recall: 1.0878e-04 - val_loss: 1.3709 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3767 - accuracy: 0.3104 - f1: 2.3045e-04 - precision: 0.0116 - recall: 1.1638e-04 - val_loss: 1.3692 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3765 - accuracy: 0.3105 - f1: 2.4605e-04 - precision: 0.0124 - recall: 1.2426e-04 - val_loss: 1.3695 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3740 - accuracy: 0.3187 - f1: 2.6226e-04 - precision: 0.0132 - recall: 1.3244e-04 - val_loss: 1.3695 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3755 - accuracy: 0.3118 - f1: 2.7911e-04 - precision: 0.0141 - recall: 1.4095e-04 - val_loss: 1.3685 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3756 - accuracy: 0.3137 - f1: 2.9667e-04 - precision: 0.0150 - recall: 1.4982e-04 - val_loss: 1.3713 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3752 - accuracy: 0.3159 - f1: 3.1498e-04 - precision: 0.0159 - recall: 1.5907e-04 - val_loss: 1.3715 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3731 - accuracy: 0.3220 - f1: 3.3413e-04 - precision: 0.0169 - recall: 1.6874e-04 - val_loss: 1.3687 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3756 - accuracy: 0.3129 - f1: 3.5420e-04 - precision: 0.0179 - recall: 1.7887e-04 - val_loss: 1.3717 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3761 - accuracy: 0.3132 - f1: 3.7526e-04 - precision: 0.0190 - recall: 1.8951e-04 - val_loss: 1.3709 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3759 - accuracy: 0.3140 - f1: 3.9744e-04 - precision: 0.0201 - recall: 2.0071e-04 - val_loss: 1.3708 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3773 - accuracy: 0.3113 - f1: 4.2084e-04 - precision: 0.0213 - recall: 2.1253e-04 - val_loss: 1.3708 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3774 - accuracy: 0.3075 - f1: 4.4563e-04 - precision: 0.0225 - recall: 2.2504e-04 - val_loss: 1.3619 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3722 - accuracy: 0.3239 - f1: 0.0131 - precision: 0.1474 - recall: 0.0076 - val_loss: 1.3895 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3744 - accuracy: 0.3276 - f1: 0.0085 - precision: 0.0945 - recall: 0.0048 - val_loss: 1.3570 - val_accuracy: 0.3233 - val_f1: 0.1335 - val_precision: 0.4933 - val_recall: 0.0775\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3454 - accuracy: 0.3463 - f1: 0.0399 - precision: 0.2650 - recall: 0.0229 - val_loss: 1.3815 - val_accuracy: 0.3383 - val_f1: 0.1631 - val_precision: 0.5180 - val_recall: 0.0975\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3335 - accuracy: 0.3363 - f1: 0.0989 - precision: 0.5578 - recall: 0.0575 - val_loss: 1.3626 - val_accuracy: 0.3558 - val_f1: 0.2229 - val_precision: 0.4937 - val_recall: 0.1450\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3237 - accuracy: 0.3692 - f1: 0.1461 - precision: 0.5846 - recall: 0.0873 - val_loss: 1.3547 - val_accuracy: 0.3525 - val_f1: 0.2375 - val_precision: 0.4860 - val_recall: 0.1575\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2997 - accuracy: 0.3678 - f1: 0.1951 - precision: 0.6243 - recall: 0.1182 - val_loss: 1.3530 - val_accuracy: 0.3633 - val_f1: 0.2588 - val_precision: 0.4805 - val_recall: 0.1775\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3123 - accuracy: 0.3599 - f1: 0.1792 - precision: 0.6519 - recall: 0.1093 - val_loss: 1.3823 - val_accuracy: 0.3483 - val_f1: 0.2529 - val_precision: 0.4414 - val_recall: 0.1775\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.2987 - accuracy: 0.3816 - f1: 0.1745 - precision: 0.6188 - recall: 0.1124 - val_loss: 1.3464 - val_accuracy: 0.3542 - val_f1: 0.2208 - val_precision: 0.5038 - val_recall: 0.1425\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2832 - accuracy: 0.3852 - f1: 0.1963 - precision: 0.6262 - recall: 0.1199 - val_loss: 1.3375 - val_accuracy: 0.3533 - val_f1: 0.2241 - val_precision: 0.5377 - val_recall: 0.1425\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2741 - accuracy: 0.3929 - f1: 0.2134 - precision: 0.6460 - recall: 0.1303 - val_loss: 1.3318 - val_accuracy: 0.3683 - val_f1: 0.2161 - val_precision: 0.5754 - val_recall: 0.1342\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.2675 - accuracy: 0.3934 - f1: 0.2210 - precision: 0.6605 - recall: 0.1347 - val_loss: 1.3316 - val_accuracy: 0.3775 - val_f1: 0.2233 - val_precision: 0.5973 - val_recall: 0.1392\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2572 - accuracy: 0.3898 - f1: 0.2265 - precision: 0.6802 - recall: 0.1381 - val_loss: 1.3403 - val_accuracy: 0.3650 - val_f1: 0.2014 - val_precision: 0.5844 - val_recall: 0.1233\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2484 - accuracy: 0.4108 - f1: 0.2449 - precision: 0.6948 - recall: 0.1508 - val_loss: 1.3403 - val_accuracy: 0.3617 - val_f1: 0.1994 - val_precision: 0.5969 - val_recall: 0.1208\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2319 - accuracy: 0.4165 - f1: 0.2662 - precision: 0.7133 - recall: 0.1652 - val_loss: 1.3389 - val_accuracy: 0.3683 - val_f1: 0.1921 - val_precision: 0.6242 - val_recall: 0.1150\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2231 - accuracy: 0.4308 - f1: 0.2657 - precision: 0.7067 - recall: 0.1656 - val_loss: 1.3486 - val_accuracy: 0.3658 - val_f1: 0.1662 - val_precision: 0.6047 - val_recall: 0.0975\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2027 - accuracy: 0.4392 - f1: 0.2818 - precision: 0.7153 - recall: 0.1778 - val_loss: 1.3327 - val_accuracy: 0.3492 - val_f1: 0.1621 - val_precision: 0.6371 - val_recall: 0.0942\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.1766 - accuracy: 0.4603 - f1: 0.2864 - precision: 0.7722 - recall: 0.1790 - val_loss: 1.3341 - val_accuracy: 0.3700 - val_f1: 0.2283 - val_precision: 0.5808 - val_recall: 0.1433\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.1718 - accuracy: 0.4575 - f1: 0.2978 - precision: 0.7667 - recall: 0.1886 - val_loss: 1.3461 - val_accuracy: 0.3867 - val_f1: 0.2110 - val_precision: 0.5755 - val_recall: 0.1308\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.1507 - accuracy: 0.4753 - f1: 0.3078 - precision: 0.7840 - recall: 0.1937 - val_loss: 1.3285 - val_accuracy: 0.3583 - val_f1: 0.1624 - val_precision: 0.6693 - val_recall: 0.0942\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1325 - accuracy: 0.4985 - f1: 0.3022 - precision: 0.8217 - recall: 0.1888 - val_loss: 1.3345 - val_accuracy: 0.3583 - val_f1: 0.1910 - val_precision: 0.6600 - val_recall: 0.1133\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.1214 - accuracy: 0.4927 - f1: 0.3132 - precision: 0.8164 - recall: 0.1966 - val_loss: 1.3288 - val_accuracy: 0.3933 - val_f1: 0.2300 - val_precision: 0.6207 - val_recall: 0.1425\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0683 - accuracy: 0.5434 - f1: 0.3442 - precision: 0.8367 - recall: 0.2192 - val_loss: 1.3289 - val_accuracy: 0.3900 - val_f1: 0.2745 - val_precision: 0.5636 - val_recall: 0.1825\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.0600 - accuracy: 0.5352 - f1: 0.3803 - precision: 0.7623 - recall: 0.2561 - val_loss: 1.3588 - val_accuracy: 0.3917 - val_f1: 0.2934 - val_precision: 0.5126 - val_recall: 0.2067\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.0654 - accuracy: 0.5361 - f1: 0.3959 - precision: 0.7253 - recall: 0.2755 - val_loss: 1.3210 - val_accuracy: 0.4117 - val_f1: 0.3057 - val_precision: 0.5526 - val_recall: 0.2125\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.0438 - accuracy: 0.5551 - f1: 0.4128 - precision: 0.7768 - recall: 0.2848 - val_loss: 1.3797 - val_accuracy: 0.4017 - val_f1: 0.2955 - val_precision: 0.5703 - val_recall: 0.2000\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0387 - accuracy: 0.5461 - f1: 0.4192 - precision: 0.7728 - recall: 0.2910 - val_loss: 1.3880 - val_accuracy: 0.4133 - val_f1: 0.3128 - val_precision: 0.4975 - val_recall: 0.2292\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0237 - accuracy: 0.5634 - f1: 0.4046 - precision: 0.7486 - recall: 0.2820 - val_loss: 1.4115 - val_accuracy: 0.4167 - val_f1: 0.3142 - val_precision: 0.5580 - val_recall: 0.2192\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.9966 - accuracy: 0.5811 - f1: 0.4594 - precision: 0.7853 - recall: 0.3290 - val_loss: 1.4107 - val_accuracy: 0.4233 - val_f1: 0.3256 - val_precision: 0.5332 - val_recall: 0.2350\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9720 - accuracy: 0.5775 - f1: 0.4859 - precision: 0.7715 - recall: 0.3565 - val_loss: 1.4513 - val_accuracy: 0.4100 - val_f1: 0.3181 - val_precision: 0.4874 - val_recall: 0.2375\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0027 - accuracy: 0.5736 - f1: 0.4669 - precision: 0.7488 - recall: 0.3428 - val_loss: 1.4106 - val_accuracy: 0.4142 - val_f1: 0.3352 - val_precision: 0.5179 - val_recall: 0.2492\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.9936 - accuracy: 0.5844 - f1: 0.4696 - precision: 0.7128 - recall: 0.3518 - val_loss: 1.4218 - val_accuracy: 0.4167 - val_f1: 0.3351 - val_precision: 0.5350 - val_recall: 0.2458\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.9789 - accuracy: 0.5833 - f1: 0.4888 - precision: 0.7627 - recall: 0.3621 - val_loss: 1.4039 - val_accuracy: 0.4400 - val_f1: 0.3545 - val_precision: 0.5693 - val_recall: 0.2583\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9442 - accuracy: 0.5919 - f1: 0.4950 - precision: 0.7500 - recall: 0.3740 - val_loss: 1.4180 - val_accuracy: 0.4167 - val_f1: 0.3453 - val_precision: 0.5386 - val_recall: 0.2550\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9231 - accuracy: 0.5970 - f1: 0.5112 - precision: 0.7711 - recall: 0.3843 - val_loss: 1.4518 - val_accuracy: 0.4208 - val_f1: 0.3475 - val_precision: 0.5095 - val_recall: 0.2650\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.8875 - accuracy: 0.6336 - f1: 0.5384 - precision: 0.7801 - recall: 0.4131 - val_loss: 1.4206 - val_accuracy: 0.4342 - val_f1: 0.4153 - val_precision: 0.5032 - val_recall: 0.3542\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.8848 - accuracy: 0.6294 - f1: 0.5556 - precision: 0.7701 - recall: 0.4380 - val_loss: 1.4073 - val_accuracy: 0.4233 - val_f1: 0.3628 - val_precision: 0.5104 - val_recall: 0.2825\n",
            "14/14 [==============================] - 1s 92ms/step - loss: 1.3074 - accuracy: 0.4600 - f1: 0.3996 - precision: 0.5612 - recall: 0.3107\n",
            "[1.3073978424072266, 0.46000000834465027, 0.39955368638038635, 0.561181366443634, 0.3107143044471741]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHtKjzc1SA8",
        "outputId": "396c364c-fd3b-490b-acfe-ea0589effac3"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 10s 146ms/step - loss: 1.3810 - accuracy: 0.2992 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3668 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 1.3645 - accuracy: 0.3215 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3480 - val_accuracy: 0.3475 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 1.3185 - accuracy: 0.3794 - f1: 0.0394 - precision: 0.3362 - recall: 0.0240 - val_loss: 1.3274 - val_accuracy: 0.3633 - val_f1: 0.0494 - val_precision: 0.6236 - val_recall: 0.0258\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 1.2599 - accuracy: 0.4282 - f1: 0.1674 - precision: 0.6796 - recall: 0.0977 - val_loss: 1.3091 - val_accuracy: 0.3858 - val_f1: 0.1874 - val_precision: 0.5376 - val_recall: 0.1142\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 1.1960 - accuracy: 0.4813 - f1: 0.2906 - precision: 0.6885 - recall: 0.1857 - val_loss: 1.2973 - val_accuracy: 0.4150 - val_f1: 0.2371 - val_precision: 0.5392 - val_recall: 0.1525\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.1435 - accuracy: 0.5100 - f1: 0.3402 - precision: 0.6912 - recall: 0.2278 - val_loss: 1.3216 - val_accuracy: 0.4217 - val_f1: 0.2699 - val_precision: 0.5095 - val_recall: 0.1842\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.1086 - accuracy: 0.5387 - f1: 0.3806 - precision: 0.6834 - recall: 0.2647 - val_loss: 1.3332 - val_accuracy: 0.4233 - val_f1: 0.3106 - val_precision: 0.5214 - val_recall: 0.2217\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.0826 - accuracy: 0.5541 - f1: 0.4149 - precision: 0.6777 - recall: 0.3008 - val_loss: 1.3547 - val_accuracy: 0.4258 - val_f1: 0.3500 - val_precision: 0.5190 - val_recall: 0.2642\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 1.0474 - accuracy: 0.5642 - f1: 0.4680 - precision: 0.6924 - recall: 0.3557 - val_loss: 1.3572 - val_accuracy: 0.4333 - val_f1: 0.3438 - val_precision: 0.4917 - val_recall: 0.2650\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.0210 - accuracy: 0.5793 - f1: 0.4945 - precision: 0.6922 - recall: 0.3874 - val_loss: 1.3400 - val_accuracy: 0.4583 - val_f1: 0.3610 - val_precision: 0.5078 - val_recall: 0.2808\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.0082 - accuracy: 0.5865 - f1: 0.5020 - precision: 0.6931 - recall: 0.3948 - val_loss: 1.3454 - val_accuracy: 0.4517 - val_f1: 0.3650 - val_precision: 0.4933 - val_recall: 0.2900\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.9720 - accuracy: 0.6005 - f1: 0.5248 - precision: 0.7089 - recall: 0.4177 - val_loss: 1.3734 - val_accuracy: 0.4583 - val_f1: 0.3909 - val_precision: 0.5040 - val_recall: 0.3200\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.9552 - accuracy: 0.6147 - f1: 0.5447 - precision: 0.7164 - recall: 0.4411 - val_loss: 1.3967 - val_accuracy: 0.4500 - val_f1: 0.3924 - val_precision: 0.4924 - val_recall: 0.3267\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.9406 - accuracy: 0.6207 - f1: 0.5512 - precision: 0.7160 - recall: 0.4495 - val_loss: 1.4229 - val_accuracy: 0.4358 - val_f1: 0.3936 - val_precision: 0.4795 - val_recall: 0.3342\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.8990 - accuracy: 0.6290 - f1: 0.5889 - precision: 0.7342 - recall: 0.4930 - val_loss: 1.4358 - val_accuracy: 0.4575 - val_f1: 0.4237 - val_precision: 0.5028 - val_recall: 0.3667\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.8466 - accuracy: 0.6634 - f1: 0.6271 - precision: 0.7521 - recall: 0.5388 - val_loss: 1.4598 - val_accuracy: 0.4733 - val_f1: 0.4369 - val_precision: 0.5179 - val_recall: 0.3783\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.8260 - accuracy: 0.6809 - f1: 0.6430 - precision: 0.7683 - recall: 0.5535 - val_loss: 1.5174 - val_accuracy: 0.4667 - val_f1: 0.4458 - val_precision: 0.5150 - val_recall: 0.3933\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.8002 - accuracy: 0.6901 - f1: 0.6523 - precision: 0.7692 - recall: 0.5674 - val_loss: 1.5234 - val_accuracy: 0.4717 - val_f1: 0.4497 - val_precision: 0.5140 - val_recall: 0.4000\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.7730 - accuracy: 0.6989 - f1: 0.6740 - precision: 0.7797 - recall: 0.5946 - val_loss: 1.5408 - val_accuracy: 0.4767 - val_f1: 0.4491 - val_precision: 0.5138 - val_recall: 0.3992\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.7260 - accuracy: 0.7224 - f1: 0.7019 - precision: 0.7977 - recall: 0.6278 - val_loss: 1.5589 - val_accuracy: 0.4800 - val_f1: 0.4503 - val_precision: 0.5080 - val_recall: 0.4050\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.7112 - accuracy: 0.7258 - f1: 0.7055 - precision: 0.8026 - recall: 0.6303 - val_loss: 1.6686 - val_accuracy: 0.5008 - val_f1: 0.4749 - val_precision: 0.5276 - val_recall: 0.4325\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.6599 - accuracy: 0.7584 - f1: 0.7470 - precision: 0.8249 - recall: 0.6832 - val_loss: 1.5989 - val_accuracy: 0.4950 - val_f1: 0.4722 - val_precision: 0.5230 - val_recall: 0.4308\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.6713 - accuracy: 0.7437 - f1: 0.7361 - precision: 0.8140 - recall: 0.6722 - val_loss: 1.7127 - val_accuracy: 0.5042 - val_f1: 0.4853 - val_precision: 0.5342 - val_recall: 0.4450\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.6374 - accuracy: 0.7640 - f1: 0.7390 - precision: 0.8154 - recall: 0.6765 - val_loss: 1.6879 - val_accuracy: 0.5108 - val_f1: 0.4864 - val_precision: 0.5343 - val_recall: 0.4467\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.6126 - accuracy: 0.7740 - f1: 0.7613 - precision: 0.8270 - recall: 0.7059 - val_loss: 1.7153 - val_accuracy: 0.5083 - val_f1: 0.4838 - val_precision: 0.5328 - val_recall: 0.4433\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.5774 - accuracy: 0.7760 - f1: 0.7731 - precision: 0.8378 - recall: 0.7182 - val_loss: 1.8421 - val_accuracy: 0.5025 - val_f1: 0.4898 - val_precision: 0.5312 - val_recall: 0.4550\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.5608 - accuracy: 0.7889 - f1: 0.7819 - precision: 0.8392 - recall: 0.7325 - val_loss: 1.7313 - val_accuracy: 0.5033 - val_f1: 0.4799 - val_precision: 0.5209 - val_recall: 0.4450\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.5458 - accuracy: 0.8040 - f1: 0.7888 - precision: 0.8447 - recall: 0.7405 - val_loss: 1.9057 - val_accuracy: 0.5133 - val_f1: 0.5027 - val_precision: 0.5394 - val_recall: 0.4708\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.4756 - accuracy: 0.8232 - f1: 0.8236 - precision: 0.8746 - recall: 0.7786 - val_loss: 2.1335 - val_accuracy: 0.5233 - val_f1: 0.5137 - val_precision: 0.5442 - val_recall: 0.4867\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 129ms/step - loss: 0.4548 - accuracy: 0.8311 - f1: 0.8279 - precision: 0.8694 - recall: 0.7907 - val_loss: 2.2591 - val_accuracy: 0.5275 - val_f1: 0.5105 - val_precision: 0.5421 - val_recall: 0.4825\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 129ms/step - loss: 0.4331 - accuracy: 0.8371 - f1: 0.8379 - precision: 0.8766 - recall: 0.8028 - val_loss: 2.5052 - val_accuracy: 0.5167 - val_f1: 0.5173 - val_precision: 0.5449 - val_recall: 0.4925\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.4064 - accuracy: 0.8499 - f1: 0.8484 - precision: 0.8848 - recall: 0.8153 - val_loss: 2.3259 - val_accuracy: 0.5258 - val_f1: 0.5210 - val_precision: 0.5431 - val_recall: 0.5008\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.4331 - accuracy: 0.8482 - f1: 0.8410 - precision: 0.8755 - recall: 0.8094 - val_loss: 2.1268 - val_accuracy: 0.5350 - val_f1: 0.5302 - val_precision: 0.5593 - val_recall: 0.5042\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.3793 - accuracy: 0.8663 - f1: 0.8622 - precision: 0.8915 - recall: 0.8350 - val_loss: 2.4860 - val_accuracy: 0.5342 - val_f1: 0.5256 - val_precision: 0.5481 - val_recall: 0.5050\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.3935 - accuracy: 0.8640 - f1: 0.8592 - precision: 0.8865 - recall: 0.8337 - val_loss: 2.5570 - val_accuracy: 0.5217 - val_f1: 0.5184 - val_precision: 0.5354 - val_recall: 0.5025\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3669 - accuracy: 0.8711 - f1: 0.8676 - precision: 0.8951 - recall: 0.8420 - val_loss: 2.4220 - val_accuracy: 0.5392 - val_f1: 0.5357 - val_precision: 0.5526 - val_recall: 0.5200\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.3531 - accuracy: 0.8790 - f1: 0.8770 - precision: 0.9011 - recall: 0.8545 - val_loss: 2.2754 - val_accuracy: 0.5383 - val_f1: 0.5338 - val_precision: 0.5531 - val_recall: 0.5158\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2968 - accuracy: 0.8891 - f1: 0.8900 - precision: 0.9113 - recall: 0.8699 - val_loss: 2.5840 - val_accuracy: 0.5300 - val_f1: 0.5290 - val_precision: 0.5467 - val_recall: 0.5125\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.3196 - accuracy: 0.8849 - f1: 0.8855 - precision: 0.9067 - recall: 0.8655 - val_loss: 2.4606 - val_accuracy: 0.5375 - val_f1: 0.5363 - val_precision: 0.5509 - val_recall: 0.5225\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.3206 - accuracy: 0.8797 - f1: 0.8848 - precision: 0.9060 - recall: 0.8649 - val_loss: 2.6352 - val_accuracy: 0.5542 - val_f1: 0.5533 - val_precision: 0.5703 - val_recall: 0.5375\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 129ms/step - loss: 0.2650 - accuracy: 0.9022 - f1: 0.9029 - precision: 0.9195 - recall: 0.8870 - val_loss: 2.5678 - val_accuracy: 0.5550 - val_f1: 0.5527 - val_precision: 0.5681 - val_recall: 0.5383\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.2641 - accuracy: 0.9033 - f1: 0.9028 - precision: 0.9210 - recall: 0.8856 - val_loss: 2.5112 - val_accuracy: 0.5383 - val_f1: 0.5375 - val_precision: 0.5554 - val_recall: 0.5208\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.2470 - accuracy: 0.9105 - f1: 0.9115 - precision: 0.9265 - recall: 0.8971 - val_loss: 2.6681 - val_accuracy: 0.5533 - val_f1: 0.5475 - val_precision: 0.5625 - val_recall: 0.5333\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.2598 - accuracy: 0.9101 - f1: 0.9081 - precision: 0.9235 - recall: 0.8933 - val_loss: 2.5086 - val_accuracy: 0.5292 - val_f1: 0.5210 - val_precision: 0.5353 - val_recall: 0.5075\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.2515 - accuracy: 0.9145 - f1: 0.9116 - precision: 0.9280 - recall: 0.8959 - val_loss: 2.3211 - val_accuracy: 0.5542 - val_f1: 0.5470 - val_precision: 0.5633 - val_recall: 0.5317\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.2185 - accuracy: 0.9172 - f1: 0.9191 - precision: 0.9350 - recall: 0.9038 - val_loss: 2.5486 - val_accuracy: 0.5467 - val_f1: 0.5416 - val_precision: 0.5537 - val_recall: 0.5300\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.2330 - accuracy: 0.9189 - f1: 0.9178 - precision: 0.9325 - recall: 0.9036 - val_loss: 2.5838 - val_accuracy: 0.5358 - val_f1: 0.5294 - val_precision: 0.5411 - val_recall: 0.5183\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2089 - accuracy: 0.9243 - f1: 0.9226 - precision: 0.9361 - recall: 0.9095 - val_loss: 2.7763 - val_accuracy: 0.5325 - val_f1: 0.5278 - val_precision: 0.5395 - val_recall: 0.5167\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2347 - accuracy: 0.9162 - f1: 0.9136 - precision: 0.9275 - recall: 0.9003 - val_loss: 2.7668 - val_accuracy: 0.5392 - val_f1: 0.5346 - val_precision: 0.5473 - val_recall: 0.5225\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.1926 - accuracy: 0.9304 - f1: 0.9304 - precision: 0.9422 - recall: 0.9191 - val_loss: 2.6952 - val_accuracy: 0.5450 - val_f1: 0.5426 - val_precision: 0.5532 - val_recall: 0.5325\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.1698 - accuracy: 0.9466 - f1: 0.9438 - precision: 0.9523 - recall: 0.9355 - val_loss: 2.8211 - val_accuracy: 0.5483 - val_f1: 0.5485 - val_precision: 0.5556 - val_recall: 0.5417\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.1707 - accuracy: 0.9425 - f1: 0.9428 - precision: 0.9506 - recall: 0.9352 - val_loss: 2.7461 - val_accuracy: 0.5492 - val_f1: 0.5486 - val_precision: 0.5593 - val_recall: 0.5383\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.1677 - accuracy: 0.9367 - f1: 0.9371 - precision: 0.9435 - recall: 0.9309 - val_loss: 3.1237 - val_accuracy: 0.5383 - val_f1: 0.5370 - val_precision: 0.5470 - val_recall: 0.5275\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.1684 - accuracy: 0.9451 - f1: 0.9435 - precision: 0.9496 - recall: 0.9376 - val_loss: 2.8797 - val_accuracy: 0.5508 - val_f1: 0.5502 - val_precision: 0.5608 - val_recall: 0.5400\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.1333 - accuracy: 0.9575 - f1: 0.9569 - precision: 0.9642 - recall: 0.9497 - val_loss: 2.9462 - val_accuracy: 0.5450 - val_f1: 0.5445 - val_precision: 0.5509 - val_recall: 0.5383\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.1438 - accuracy: 0.9480 - f1: 0.9473 - precision: 0.9563 - recall: 0.9386 - val_loss: 2.9595 - val_accuracy: 0.5483 - val_f1: 0.5411 - val_precision: 0.5483 - val_recall: 0.5342\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.1638 - accuracy: 0.9444 - f1: 0.9464 - precision: 0.9524 - recall: 0.9406 - val_loss: 2.7616 - val_accuracy: 0.5392 - val_f1: 0.5365 - val_precision: 0.5467 - val_recall: 0.5267\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 129ms/step - loss: 0.1482 - accuracy: 0.9430 - f1: 0.9443 - precision: 0.9494 - recall: 0.9392 - val_loss: 2.9456 - val_accuracy: 0.5342 - val_f1: 0.5303 - val_precision: 0.5383 - val_recall: 0.5225\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 129ms/step - loss: 0.1385 - accuracy: 0.9525 - f1: 0.9528 - precision: 0.9581 - recall: 0.9476 - val_loss: 2.8582 - val_accuracy: 0.5458 - val_f1: 0.5439 - val_precision: 0.5541 - val_recall: 0.5342\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.1337 - accuracy: 0.9549 - f1: 0.9552 - precision: 0.9606 - recall: 0.9499 - val_loss: 2.8331 - val_accuracy: 0.5400 - val_f1: 0.5427 - val_precision: 0.5524 - val_recall: 0.5333\n",
            "14/14 [==============================] - 1s 93ms/step - loss: 2.5894 - accuracy: 0.5700 - f1: 0.5640 - precision: 0.5750 - recall: 0.5536\n",
            "[2.5894343852996826, 0.5699999928474426, 0.5640327334403992, 0.5749759674072266, 0.5535714030265808]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opfhhYPH1VoL",
        "outputId": "8228ed1f-f07d-4d46-c9a9-d4d91d17a274"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 9s 160ms/step - loss: 1.3879 - accuracy: 0.2668 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3695 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3773 - accuracy: 0.2978 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3691 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3755 - accuracy: 0.3009 - f1: 2.1122e-04 - precision: 0.0036 - recall: 1.0878e-04 - val_loss: 1.3677 - val_accuracy: 0.3408 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3738 - accuracy: 0.3040 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3646 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3744 - accuracy: 0.3006 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3652 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3711 - accuracy: 0.3050 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3683 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3614 - accuracy: 0.3308 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3796 - val_accuracy: 0.3183 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3692 - accuracy: 0.3291 - f1: 0.0241 - precision: 0.3558 - recall: 0.0128 - val_loss: 1.3730 - val_accuracy: 0.3083 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3730 - accuracy: 0.3147 - f1: 2.2953e-04 - precision: 0.0102 - recall: 1.1620e-04 - val_loss: 1.3716 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.3767 - accuracy: 0.3164 - f1: 0.0066 - precision: 0.1621 - recall: 0.0034 - val_loss: 1.3769 - val_accuracy: 0.3192 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3769 - accuracy: 0.3104 - f1: 0.0061 - precision: 0.1583 - recall: 0.0031 - val_loss: 1.3728 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3661 - accuracy: 0.3326 - f1: 0.0106 - precision: 0.1244 - recall: 0.0056 - val_loss: 1.3714 - val_accuracy: 0.3217 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3739 - accuracy: 0.3089 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3667 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3662 - accuracy: 0.3199 - f1: 7.4162e-04 - precision: 0.0325 - recall: 3.7551e-04 - val_loss: 1.3672 - val_accuracy: 0.3475 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3670 - accuracy: 0.3128 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3653 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3710 - accuracy: 0.3140 - f1: 0.0034 - precision: 0.0994 - recall: 0.0018 - val_loss: 1.3777 - val_accuracy: 0.3158 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3790 - accuracy: 0.2979 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3659 - val_accuracy: 0.3250 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3639 - accuracy: 0.3197 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3769 - val_accuracy: 0.3117 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3604 - accuracy: 0.3145 - f1: 0.0068 - precision: 0.1242 - recall: 0.0035 - val_loss: 1.3990 - val_accuracy: 0.2950 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3808 - accuracy: 0.2955 - f1: 0.0091 - precision: 0.1484 - recall: 0.0048 - val_loss: 1.3667 - val_accuracy: 0.3417 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3640 - accuracy: 0.3228 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3733 - val_accuracy: 0.3117 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3523 - accuracy: 0.3353 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3764 - val_accuracy: 0.2733 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3716 - accuracy: 0.3087 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3698 - val_accuracy: 0.3342 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3431 - accuracy: 0.3456 - f1: 0.0044 - precision: 0.1405 - recall: 0.0023 - val_loss: 1.3763 - val_accuracy: 0.2992 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3541 - accuracy: 0.3362 - f1: 0.0095 - precision: 0.2341 - recall: 0.0049 - val_loss: 1.3942 - val_accuracy: 0.3050 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3819 - accuracy: 0.3039 - f1: 0.0059 - precision: 0.1952 - recall: 0.0030 - val_loss: 1.3700 - val_accuracy: 0.3033 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3736 - accuracy: 0.3126 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3769 - val_accuracy: 0.2867 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3449 - accuracy: 0.3513 - f1: 0.0014 - precision: 0.0732 - recall: 7.3158e-04 - val_loss: 1.3624 - val_accuracy: 0.3150 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3289 - accuracy: 0.3597 - f1: 0.0302 - precision: 0.2871 - recall: 0.0163 - val_loss: 1.3872 - val_accuracy: 0.2858 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3278 - accuracy: 0.3622 - f1: 0.0151 - precision: 0.3706 - recall: 0.0078 - val_loss: 1.3773 - val_accuracy: 0.2842 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3360 - accuracy: 0.3422 - f1: 0.0027 - precision: 0.1280 - recall: 0.0014 - val_loss: 1.3995 - val_accuracy: 0.3050 - val_f1: 0.0131 - val_precision: 0.4167 - val_recall: 0.0067\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3071 - accuracy: 0.3768 - f1: 0.0659 - precision: 0.5651 - recall: 0.0355 - val_loss: 1.4397 - val_accuracy: 0.2642 - val_f1: 0.0081 - val_precision: 0.1667 - val_recall: 0.0042\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3491 - accuracy: 0.3354 - f1: 0.0180 - precision: 0.2379 - recall: 0.0095 - val_loss: 1.3795 - val_accuracy: 0.2858 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3383 - accuracy: 0.3387 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3875 - val_accuracy: 0.2742 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3516 - accuracy: 0.3319 - f1: 0.0045 - precision: 0.1529 - recall: 0.0023 - val_loss: 1.3832 - val_accuracy: 0.2992 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3490 - accuracy: 0.3322 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3812 - val_accuracy: 0.3033 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3306 - accuracy: 0.3518 - f1: 0.0030 - precision: 0.1099 - recall: 0.0015 - val_loss: 1.3974 - val_accuracy: 0.2950 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.3121 - accuracy: 0.3637 - f1: 0.0364 - precision: 0.4906 - recall: 0.0192 - val_loss: 1.4044 - val_accuracy: 0.3083 - val_f1: 0.0274 - val_precision: 0.4647 - val_recall: 0.0142\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3018 - accuracy: 0.3847 - f1: 0.0567 - precision: 0.5232 - recall: 0.0302 - val_loss: 1.3860 - val_accuracy: 0.3075 - val_f1: 0.0082 - val_precision: 0.3611 - val_recall: 0.0042\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3429 - accuracy: 0.3426 - f1: 0.0169 - precision: 0.3569 - recall: 0.0088 - val_loss: 1.3713 - val_accuracy: 0.3125 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3692 - accuracy: 0.3062 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3769 - val_accuracy: 0.3050 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3886 - accuracy: 0.2922 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3624 - val_accuracy: 0.3342 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3603 - accuracy: 0.3281 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3805 - val_accuracy: 0.2933 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3185 - accuracy: 0.3656 - f1: 0.0036 - precision: 0.1091 - recall: 0.0018 - val_loss: 1.3873 - val_accuracy: 0.3167 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3863 - accuracy: 0.3155 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3727 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3891 - accuracy: 0.2932 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3722 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3877 - accuracy: 0.2946 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3688 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3823 - accuracy: 0.3022 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3703 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3816 - accuracy: 0.3036 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3655 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3847 - accuracy: 0.3001 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3699 - val_accuracy: 0.3325 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3563 - accuracy: 0.3233 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3672 - val_accuracy: 0.3258 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3517 - accuracy: 0.3253 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3767 - val_accuracy: 0.3150 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3454 - accuracy: 0.3402 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3665 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3432 - accuracy: 0.3416 - f1: 6.9583e-04 - precision: 0.0247 - recall: 3.5348e-04 - val_loss: 1.3660 - val_accuracy: 0.3017 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3314 - accuracy: 0.3566 - f1: 1.7324e-04 - precision: 0.0087 - recall: 8.7485e-05 - val_loss: 1.3804 - val_accuracy: 0.3117 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3249 - accuracy: 0.3689 - f1: 0.0030 - precision: 0.1037 - recall: 0.0015 - val_loss: 1.3869 - val_accuracy: 0.2933 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3265 - accuracy: 0.3486 - f1: 0.0168 - precision: 0.3302 - recall: 0.0086 - val_loss: 1.3789 - val_accuracy: 0.3075 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.3094 - accuracy: 0.3730 - f1: 0.0159 - precision: 0.5327 - recall: 0.0081 - val_loss: 1.3890 - val_accuracy: 0.3033 - val_f1: 0.0098 - val_precision: 0.2778 - val_recall: 0.0050\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.2930 - accuracy: 0.3889 - f1: 0.0342 - precision: 0.4768 - recall: 0.0180 - val_loss: 1.3888 - val_accuracy: 0.2858 - val_f1: 0.0097 - val_precision: 0.1597 - val_recall: 0.0050\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3053 - accuracy: 0.3779 - f1: 0.0367 - precision: 0.4991 - recall: 0.0192 - val_loss: 1.3904 - val_accuracy: 0.2875 - val_f1: 0.0177 - val_precision: 0.2750 - val_recall: 0.0092\n",
            "14/14 [==============================] - 1s 92ms/step - loss: 1.4023 - accuracy: 0.2914 - f1: 0.0193 - precision: 0.3524 - recall: 0.0100\n",
            "[1.4022798538208008, 0.2914285659790039, 0.01928798295557499, 0.3523809313774109, 0.009999999776482582]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YumD1M9Y1Xxr",
        "outputId": "23e674f4-1917-4c50-8eb1-05955907624a"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(64))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 141ms/step - loss: 1.3821 - accuracy: 0.3101 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3727 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3783 - accuracy: 0.3054 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3700 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3772 - accuracy: 0.3105 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3699 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3758 - accuracy: 0.3164 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3699 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3766 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3693 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3770 - accuracy: 0.3119 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3715 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3765 - accuracy: 0.3144 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3719 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3768 - accuracy: 0.3113 - f1: 1.6007e-04 - precision: 0.0081 - recall: 8.0836e-05 - val_loss: 1.3690 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.3751 - accuracy: 0.3150 - f1: 1.7324e-04 - precision: 0.0087 - recall: 8.7485e-05 - val_loss: 1.3719 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3769 - accuracy: 0.3110 - f1: 1.8683e-04 - precision: 0.0094 - recall: 9.4349e-05 - val_loss: 1.3711 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3748 - accuracy: 0.3147 - f1: 2.0087e-04 - precision: 0.0101 - recall: 1.0144e-04 - val_loss: 1.3708 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3760 - accuracy: 0.3107 - f1: 2.1540e-04 - precision: 0.0109 - recall: 1.0878e-04 - val_loss: 1.3708 - val_accuracy: 0.3308 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3754 - accuracy: 0.3108 - f1: 2.3045e-04 - precision: 0.0116 - recall: 1.1638e-04 - val_loss: 1.3696 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3767 - accuracy: 0.3105 - f1: 2.4605e-04 - precision: 0.0124 - recall: 1.2426e-04 - val_loss: 1.3701 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.3745 - accuracy: 0.3187 - f1: 2.6226e-04 - precision: 0.0132 - recall: 1.3244e-04 - val_loss: 1.3696 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3761 - accuracy: 0.3118 - f1: 2.7911e-04 - precision: 0.0141 - recall: 1.4095e-04 - val_loss: 1.3687 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3757 - accuracy: 0.3137 - f1: 2.9667e-04 - precision: 0.0150 - recall: 1.4982e-04 - val_loss: 1.3716 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.3749 - accuracy: 0.3159 - f1: 3.1498e-04 - precision: 0.0159 - recall: 1.5907e-04 - val_loss: 1.3717 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3735 - accuracy: 0.3220 - f1: 3.3413e-04 - precision: 0.0169 - recall: 1.6874e-04 - val_loss: 1.3688 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3758 - accuracy: 0.3129 - f1: 3.5420e-04 - precision: 0.0179 - recall: 1.7887e-04 - val_loss: 1.3717 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3757 - accuracy: 0.3132 - f1: 3.7526e-04 - precision: 0.0190 - recall: 1.8951e-04 - val_loss: 1.3706 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3756 - accuracy: 0.3140 - f1: 3.9744e-04 - precision: 0.0201 - recall: 2.0071e-04 - val_loss: 1.3711 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3762 - accuracy: 0.3113 - f1: 4.2084e-04 - precision: 0.0213 - recall: 2.1253e-04 - val_loss: 1.3706 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3775 - accuracy: 0.3072 - f1: 4.4563e-04 - precision: 0.0225 - recall: 2.2504e-04 - val_loss: 1.3688 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3749 - accuracy: 0.3139 - f1: 4.7196e-04 - precision: 0.0238 - recall: 2.3834e-04 - val_loss: 1.3700 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3737 - accuracy: 0.3167 - f1: 5.0005e-04 - precision: 0.0253 - recall: 2.5252e-04 - val_loss: 1.3885 - val_accuracy: 0.3050 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3708 - accuracy: 0.3198 - f1: 0.0085 - precision: 0.1149 - recall: 0.0045 - val_loss: 1.3350 - val_accuracy: 0.3367 - val_f1: 0.1394 - val_precision: 0.5682 - val_recall: 0.0800\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 1.3217 - accuracy: 0.3717 - f1: 0.1103 - precision: 0.6273 - recall: 0.0638 - val_loss: 1.3336 - val_accuracy: 0.3508 - val_f1: 0.1426 - val_precision: 0.5919 - val_recall: 0.0817\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2842 - accuracy: 0.3992 - f1: 0.1648 - precision: 0.6677 - recall: 0.0968 - val_loss: 1.3176 - val_accuracy: 0.3642 - val_f1: 0.1156 - val_precision: 0.6223 - val_recall: 0.0642\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2575 - accuracy: 0.4156 - f1: 0.1941 - precision: 0.6857 - recall: 0.1165 - val_loss: 1.3187 - val_accuracy: 0.3692 - val_f1: 0.1811 - val_precision: 0.5597 - val_recall: 0.1092\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2302 - accuracy: 0.4281 - f1: 0.2278 - precision: 0.6980 - recall: 0.1391 - val_loss: 1.2972 - val_accuracy: 0.3867 - val_f1: 0.1894 - val_precision: 0.5533 - val_recall: 0.1158\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.1954 - accuracy: 0.4553 - f1: 0.2950 - precision: 0.7192 - recall: 0.1907 - val_loss: 1.2826 - val_accuracy: 0.4000 - val_f1: 0.2126 - val_precision: 0.6112 - val_recall: 0.1300\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.1678 - accuracy: 0.4632 - f1: 0.3316 - precision: 0.7428 - recall: 0.2180 - val_loss: 1.3019 - val_accuracy: 0.3867 - val_f1: 0.2488 - val_precision: 0.5793 - val_recall: 0.1608\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.1505 - accuracy: 0.4687 - f1: 0.3574 - precision: 0.7270 - recall: 0.2400 - val_loss: 1.2916 - val_accuracy: 0.3933 - val_f1: 0.2054 - val_precision: 0.6383 - val_recall: 0.1233\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.1346 - accuracy: 0.4834 - f1: 0.3374 - precision: 0.7703 - recall: 0.2244 - val_loss: 1.2809 - val_accuracy: 0.3967 - val_f1: 0.2272 - val_precision: 0.6200 - val_recall: 0.1408\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1040 - accuracy: 0.4928 - f1: 0.3717 - precision: 0.7857 - recall: 0.2507 - val_loss: 1.3173 - val_accuracy: 0.4008 - val_f1: 0.2398 - val_precision: 0.5838 - val_recall: 0.1533\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.0951 - accuracy: 0.4963 - f1: 0.3831 - precision: 0.7912 - recall: 0.2594 - val_loss: 1.3050 - val_accuracy: 0.4000 - val_f1: 0.2580 - val_precision: 0.5857 - val_recall: 0.1675\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.0733 - accuracy: 0.5137 - f1: 0.3956 - precision: 0.8155 - recall: 0.2689 - val_loss: 1.3073 - val_accuracy: 0.3992 - val_f1: 0.2648 - val_precision: 0.5314 - val_recall: 0.1775\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.0928 - accuracy: 0.5107 - f1: 0.3863 - precision: 0.7816 - recall: 0.2636 - val_loss: 1.3219 - val_accuracy: 0.3917 - val_f1: 0.2864 - val_precision: 0.5830 - val_recall: 0.1925\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.0613 - accuracy: 0.5103 - f1: 0.4101 - precision: 0.8019 - recall: 0.2827 - val_loss: 1.2904 - val_accuracy: 0.4092 - val_f1: 0.2428 - val_precision: 0.6134 - val_recall: 0.1533\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.0357 - accuracy: 0.5362 - f1: 0.4195 - precision: 0.8343 - recall: 0.2885 - val_loss: 1.2954 - val_accuracy: 0.4025 - val_f1: 0.2375 - val_precision: 0.6075 - val_recall: 0.1492\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.0559 - accuracy: 0.5278 - f1: 0.4136 - precision: 0.8416 - recall: 0.2806 - val_loss: 1.3365 - val_accuracy: 0.3658 - val_f1: 0.2136 - val_precision: 0.5277 - val_recall: 0.1358\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 121ms/step - loss: 1.0646 - accuracy: 0.5152 - f1: 0.3904 - precision: 0.8130 - recall: 0.2639 - val_loss: 1.3044 - val_accuracy: 0.4083 - val_f1: 0.2720 - val_precision: 0.5930 - val_recall: 0.1783\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0387 - accuracy: 0.5465 - f1: 0.4273 - precision: 0.8412 - recall: 0.2883 - val_loss: 1.3399 - val_accuracy: 0.3992 - val_f1: 0.2571 - val_precision: 0.5473 - val_recall: 0.1700\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.9996 - accuracy: 0.5574 - f1: 0.4464 - precision: 0.8497 - recall: 0.3052 - val_loss: 1.3517 - val_accuracy: 0.3917 - val_f1: 0.2707 - val_precision: 0.5598 - val_recall: 0.1800\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.9584 - accuracy: 0.5713 - f1: 0.4697 - precision: 0.8646 - recall: 0.3250 - val_loss: 1.3752 - val_accuracy: 0.4017 - val_f1: 0.2817 - val_precision: 0.5807 - val_recall: 0.1883\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9471 - accuracy: 0.5787 - f1: 0.4658 - precision: 0.8734 - recall: 0.3208 - val_loss: 1.4077 - val_accuracy: 0.4075 - val_f1: 0.2914 - val_precision: 0.5790 - val_recall: 0.1967\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.9412 - accuracy: 0.5861 - f1: 0.4913 - precision: 0.8655 - recall: 0.3446 - val_loss: 1.4647 - val_accuracy: 0.4208 - val_f1: 0.3350 - val_precision: 0.5607 - val_recall: 0.2408\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.9137 - accuracy: 0.5898 - f1: 0.5035 - precision: 0.8706 - recall: 0.3561 - val_loss: 1.5055 - val_accuracy: 0.4300 - val_f1: 0.3231 - val_precision: 0.5383 - val_recall: 0.2325\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.8946 - accuracy: 0.5979 - f1: 0.5080 - precision: 0.8821 - recall: 0.3582 - val_loss: 1.6252 - val_accuracy: 0.4283 - val_f1: 0.3248 - val_precision: 0.5418 - val_recall: 0.2342\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9078 - accuracy: 0.5890 - f1: 0.5021 - precision: 0.8775 - recall: 0.3531 - val_loss: 1.6153 - val_accuracy: 0.4125 - val_f1: 0.3319 - val_precision: 0.5584 - val_recall: 0.2375\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9190 - accuracy: 0.5908 - f1: 0.4952 - precision: 0.8612 - recall: 0.3496 - val_loss: 1.5114 - val_accuracy: 0.4317 - val_f1: 0.3422 - val_precision: 0.6048 - val_recall: 0.2408\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.8734 - accuracy: 0.5944 - f1: 0.5171 - precision: 0.8838 - recall: 0.3670 - val_loss: 1.5846 - val_accuracy: 0.4167 - val_f1: 0.3354 - val_precision: 0.5725 - val_recall: 0.2392\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.8618 - accuracy: 0.6080 - f1: 0.5217 - precision: 0.9066 - recall: 0.3681 - val_loss: 1.5517 - val_accuracy: 0.4258 - val_f1: 0.3366 - val_precision: 0.6164 - val_recall: 0.2333\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.8557 - accuracy: 0.6081 - f1: 0.5239 - precision: 0.9123 - recall: 0.3698 - val_loss: 1.5291 - val_accuracy: 0.4383 - val_f1: 0.3264 - val_precision: 0.6264 - val_recall: 0.2242\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.8419 - accuracy: 0.6168 - f1: 0.5213 - precision: 0.9159 - recall: 0.3672 - val_loss: 1.6861 - val_accuracy: 0.4392 - val_f1: 0.3425 - val_precision: 0.5847 - val_recall: 0.2442\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.8338 - accuracy: 0.6309 - f1: 0.5360 - precision: 0.9068 - recall: 0.3830 - val_loss: 1.7877 - val_accuracy: 0.4217 - val_f1: 0.3466 - val_precision: 0.5442 - val_recall: 0.2558\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.8337 - accuracy: 0.6279 - f1: 0.5434 - precision: 0.9068 - recall: 0.3917 - val_loss: 1.6110 - val_accuracy: 0.4283 - val_f1: 0.3483 - val_precision: 0.6012 - val_recall: 0.2475\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.8405 - accuracy: 0.6131 - f1: 0.5368 - precision: 0.8944 - recall: 0.3870 - val_loss: 1.5360 - val_accuracy: 0.4525 - val_f1: 0.3718 - val_precision: 0.6187 - val_recall: 0.2675\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.7725 - accuracy: 0.6461 - f1: 0.5734 - precision: 0.9171 - recall: 0.4201 - val_loss: 1.6768 - val_accuracy: 0.4367 - val_f1: 0.3739 - val_precision: 0.5931 - val_recall: 0.2742\n",
            "14/14 [==============================] - 1s 89ms/step - loss: 1.6967 - accuracy: 0.4679 - f1: 0.3899 - precision: 0.6085 - recall: 0.2879\n",
            "[1.6967065334320068, 0.46785715222358704, 0.38992008566856384, 0.6084523797035217, 0.2878571152687073]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01yVMiF41ZtT",
        "outputId": "1c2b6980-c6df-4562-defb-8d7a120b416f"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 10s 146ms/step - loss: 1.3821 - accuracy: 0.2929 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3655 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.3630 - accuracy: 0.3222 - f1: 2.7681e-05 - precision: 0.0014 - recall: 1.3979e-05 - val_loss: 1.3481 - val_accuracy: 0.3650 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.3208 - accuracy: 0.3803 - f1: 0.0158 - precision: 0.2703 - recall: 0.0083 - val_loss: 1.3267 - val_accuracy: 0.3775 - val_f1: 0.0884 - val_precision: 0.5412 - val_recall: 0.0483\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 1.2509 - accuracy: 0.4380 - f1: 0.1585 - precision: 0.6974 - recall: 0.0908 - val_loss: 1.2963 - val_accuracy: 0.4067 - val_f1: 0.1915 - val_precision: 0.5899 - val_recall: 0.1150\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.1941 - accuracy: 0.4827 - f1: 0.2517 - precision: 0.6721 - recall: 0.1565 - val_loss: 1.2878 - val_accuracy: 0.4258 - val_f1: 0.2520 - val_precision: 0.5577 - val_recall: 0.1633\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.1486 - accuracy: 0.5170 - f1: 0.3402 - precision: 0.6703 - recall: 0.2297 - val_loss: 1.2964 - val_accuracy: 0.4275 - val_f1: 0.2627 - val_precision: 0.5225 - val_recall: 0.1758\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.1034 - accuracy: 0.5402 - f1: 0.3940 - precision: 0.6877 - recall: 0.2774 - val_loss: 1.3150 - val_accuracy: 0.4400 - val_f1: 0.3202 - val_precision: 0.4992 - val_recall: 0.2358\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.0814 - accuracy: 0.5661 - f1: 0.4405 - precision: 0.6803 - recall: 0.3269 - val_loss: 1.2999 - val_accuracy: 0.4242 - val_f1: 0.3363 - val_precision: 0.5165 - val_recall: 0.2500\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.0501 - accuracy: 0.5696 - f1: 0.4685 - precision: 0.7004 - recall: 0.3540 - val_loss: 1.3220 - val_accuracy: 0.4250 - val_f1: 0.3568 - val_precision: 0.4956 - val_recall: 0.2792\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.0275 - accuracy: 0.5903 - f1: 0.4896 - precision: 0.6848 - recall: 0.3826 - val_loss: 1.3337 - val_accuracy: 0.4308 - val_f1: 0.3859 - val_precision: 0.5140 - val_recall: 0.3092\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.0216 - accuracy: 0.5913 - f1: 0.5162 - precision: 0.7081 - recall: 0.4082 - val_loss: 1.3354 - val_accuracy: 0.4367 - val_f1: 0.3895 - val_precision: 0.5016 - val_recall: 0.3192\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.9754 - accuracy: 0.6067 - f1: 0.5379 - precision: 0.7205 - recall: 0.4301 - val_loss: 1.3496 - val_accuracy: 0.4417 - val_f1: 0.3983 - val_precision: 0.5076 - val_recall: 0.3283\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.9293 - accuracy: 0.6295 - f1: 0.5716 - precision: 0.7361 - recall: 0.4683 - val_loss: 1.3541 - val_accuracy: 0.4250 - val_f1: 0.3788 - val_precision: 0.4837 - val_recall: 0.3117\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.9389 - accuracy: 0.6175 - f1: 0.5684 - precision: 0.7358 - recall: 0.4644 - val_loss: 1.3779 - val_accuracy: 0.4467 - val_f1: 0.4032 - val_precision: 0.4977 - val_recall: 0.3392\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.8871 - accuracy: 0.6488 - f1: 0.6131 - precision: 0.7570 - recall: 0.5162 - val_loss: 1.3716 - val_accuracy: 0.4550 - val_f1: 0.4162 - val_precision: 0.5067 - val_recall: 0.3533\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.8399 - accuracy: 0.6816 - f1: 0.6290 - precision: 0.7663 - recall: 0.5343 - val_loss: 1.4219 - val_accuracy: 0.4450 - val_f1: 0.4182 - val_precision: 0.5030 - val_recall: 0.3583\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.8238 - accuracy: 0.6781 - f1: 0.6400 - precision: 0.7687 - recall: 0.5494 - val_loss: 1.4223 - val_accuracy: 0.4617 - val_f1: 0.4282 - val_precision: 0.5102 - val_recall: 0.3692\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.7756 - accuracy: 0.7080 - f1: 0.6694 - precision: 0.7906 - recall: 0.5818 - val_loss: 1.4056 - val_accuracy: 0.4725 - val_f1: 0.4451 - val_precision: 0.5164 - val_recall: 0.3917\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.7307 - accuracy: 0.7317 - f1: 0.7071 - precision: 0.8105 - recall: 0.6278 - val_loss: 1.5479 - val_accuracy: 0.4683 - val_f1: 0.4563 - val_precision: 0.5136 - val_recall: 0.4108\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.7129 - accuracy: 0.7397 - f1: 0.7141 - precision: 0.8062 - recall: 0.6414 - val_loss: 1.5650 - val_accuracy: 0.4708 - val_f1: 0.4456 - val_precision: 0.5034 - val_recall: 0.4000\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.6848 - accuracy: 0.7399 - f1: 0.7195 - precision: 0.8128 - recall: 0.6462 - val_loss: 1.5509 - val_accuracy: 0.4617 - val_f1: 0.4422 - val_precision: 0.5071 - val_recall: 0.3925\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.7431 - accuracy: 0.7218 - f1: 0.6968 - precision: 0.7975 - recall: 0.6195 - val_loss: 1.5422 - val_accuracy: 0.4908 - val_f1: 0.4690 - val_precision: 0.5227 - val_recall: 0.4258\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.6402 - accuracy: 0.7542 - f1: 0.7491 - precision: 0.8277 - recall: 0.6849 - val_loss: 1.7664 - val_accuracy: 0.4850 - val_f1: 0.4611 - val_precision: 0.5066 - val_recall: 0.4233\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.6321 - accuracy: 0.7620 - f1: 0.7575 - precision: 0.8283 - recall: 0.6985 - val_loss: 1.7731 - val_accuracy: 0.4792 - val_f1: 0.4722 - val_precision: 0.5143 - val_recall: 0.4367\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.5751 - accuracy: 0.7835 - f1: 0.7821 - precision: 0.8473 - recall: 0.7269 - val_loss: 1.7462 - val_accuracy: 0.4858 - val_f1: 0.4691 - val_precision: 0.5059 - val_recall: 0.4375\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.5483 - accuracy: 0.7968 - f1: 0.7923 - precision: 0.8503 - recall: 0.7422 - val_loss: 1.7711 - val_accuracy: 0.5000 - val_f1: 0.4851 - val_precision: 0.5187 - val_recall: 0.4558\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.5341 - accuracy: 0.8044 - f1: 0.8000 - precision: 0.8556 - recall: 0.7518 - val_loss: 1.6904 - val_accuracy: 0.5142 - val_f1: 0.4944 - val_precision: 0.5322 - val_recall: 0.4617\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.5528 - accuracy: 0.7925 - f1: 0.7860 - precision: 0.8439 - recall: 0.7360 - val_loss: 1.7873 - val_accuracy: 0.5117 - val_f1: 0.4966 - val_precision: 0.5319 - val_recall: 0.4658\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4970 - accuracy: 0.8189 - f1: 0.8167 - precision: 0.8627 - recall: 0.7758 - val_loss: 1.7258 - val_accuracy: 0.5242 - val_f1: 0.5117 - val_precision: 0.5460 - val_recall: 0.4817\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4500 - accuracy: 0.8380 - f1: 0.8321 - precision: 0.8808 - recall: 0.7889 - val_loss: 1.9307 - val_accuracy: 0.5017 - val_f1: 0.4946 - val_precision: 0.5252 - val_recall: 0.4675\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4281 - accuracy: 0.8411 - f1: 0.8436 - precision: 0.8840 - recall: 0.8071 - val_loss: 2.1309 - val_accuracy: 0.5200 - val_f1: 0.5109 - val_precision: 0.5349 - val_recall: 0.4892\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.4164 - accuracy: 0.8484 - f1: 0.8460 - precision: 0.8799 - recall: 0.8148 - val_loss: 2.0492 - val_accuracy: 0.5358 - val_f1: 0.5292 - val_precision: 0.5550 - val_recall: 0.5058\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.3939 - accuracy: 0.8582 - f1: 0.8600 - precision: 0.8925 - recall: 0.8301 - val_loss: 1.9276 - val_accuracy: 0.5242 - val_f1: 0.5125 - val_precision: 0.5383 - val_recall: 0.4892\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.4558 - accuracy: 0.8396 - f1: 0.8370 - precision: 0.8665 - recall: 0.8098 - val_loss: 1.8460 - val_accuracy: 0.5075 - val_f1: 0.4875 - val_precision: 0.5198 - val_recall: 0.4592\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4387 - accuracy: 0.8388 - f1: 0.8392 - precision: 0.8765 - recall: 0.8053 - val_loss: 1.8925 - val_accuracy: 0.5192 - val_f1: 0.5114 - val_precision: 0.5400 - val_recall: 0.4858\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.3479 - accuracy: 0.8688 - f1: 0.8715 - precision: 0.9009 - recall: 0.8441 - val_loss: 2.2118 - val_accuracy: 0.5283 - val_f1: 0.5132 - val_precision: 0.5340 - val_recall: 0.4942\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3375 - accuracy: 0.8810 - f1: 0.8788 - precision: 0.9061 - recall: 0.8534 - val_loss: 2.2337 - val_accuracy: 0.5217 - val_f1: 0.5186 - val_precision: 0.5378 - val_recall: 0.5008\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.3112 - accuracy: 0.8838 - f1: 0.8872 - precision: 0.9124 - recall: 0.8635 - val_loss: 2.2887 - val_accuracy: 0.5225 - val_f1: 0.5167 - val_precision: 0.5376 - val_recall: 0.4975\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3031 - accuracy: 0.8914 - f1: 0.8954 - precision: 0.9180 - recall: 0.8741 - val_loss: 2.3705 - val_accuracy: 0.5367 - val_f1: 0.5314 - val_precision: 0.5539 - val_recall: 0.5108\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2634 - accuracy: 0.9055 - f1: 0.9044 - precision: 0.9224 - recall: 0.8871 - val_loss: 2.6802 - val_accuracy: 0.5367 - val_f1: 0.5320 - val_precision: 0.5474 - val_recall: 0.5175\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2971 - accuracy: 0.9018 - f1: 0.9005 - precision: 0.9186 - recall: 0.8833 - val_loss: 2.3495 - val_accuracy: 0.5258 - val_f1: 0.5253 - val_precision: 0.5419 - val_recall: 0.5100\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.2957 - accuracy: 0.8940 - f1: 0.8958 - precision: 0.9183 - recall: 0.8746 - val_loss: 2.3398 - val_accuracy: 0.5350 - val_f1: 0.5321 - val_precision: 0.5514 - val_recall: 0.5142\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2968 - accuracy: 0.8918 - f1: 0.8940 - precision: 0.9124 - recall: 0.8765 - val_loss: 2.2959 - val_accuracy: 0.5400 - val_f1: 0.5434 - val_precision: 0.5616 - val_recall: 0.5267\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.2409 - accuracy: 0.9143 - f1: 0.9146 - precision: 0.9282 - recall: 0.9013 - val_loss: 2.3764 - val_accuracy: 0.5433 - val_f1: 0.5353 - val_precision: 0.5516 - val_recall: 0.5200\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.2324 - accuracy: 0.9122 - f1: 0.9134 - precision: 0.9261 - recall: 0.9012 - val_loss: 2.5535 - val_accuracy: 0.5417 - val_f1: 0.5373 - val_precision: 0.5541 - val_recall: 0.5217\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.2481 - accuracy: 0.9178 - f1: 0.9178 - precision: 0.9334 - recall: 0.9029 - val_loss: 2.3351 - val_accuracy: 0.5225 - val_f1: 0.5169 - val_precision: 0.5322 - val_recall: 0.5025\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.2781 - accuracy: 0.9036 - f1: 0.9029 - precision: 0.9202 - recall: 0.8864 - val_loss: 2.4141 - val_accuracy: 0.5533 - val_f1: 0.5485 - val_precision: 0.5618 - val_recall: 0.5358\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.2176 - accuracy: 0.9196 - f1: 0.9190 - precision: 0.9362 - recall: 0.9028 - val_loss: 2.5571 - val_accuracy: 0.5333 - val_f1: 0.5291 - val_precision: 0.5395 - val_recall: 0.5192\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2192 - accuracy: 0.9250 - f1: 0.9260 - precision: 0.9343 - recall: 0.9180 - val_loss: 2.3460 - val_accuracy: 0.5283 - val_f1: 0.5314 - val_precision: 0.5425 - val_recall: 0.5208\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.2455 - accuracy: 0.9189 - f1: 0.9151 - precision: 0.9294 - recall: 0.9014 - val_loss: 2.2731 - val_accuracy: 0.5242 - val_f1: 0.5200 - val_precision: 0.5343 - val_recall: 0.5067\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2565 - accuracy: 0.9135 - f1: 0.9130 - precision: 0.9258 - recall: 0.9007 - val_loss: 2.2689 - val_accuracy: 0.5542 - val_f1: 0.5449 - val_precision: 0.5579 - val_recall: 0.5325\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2239 - accuracy: 0.9129 - f1: 0.9150 - precision: 0.9265 - recall: 0.9038 - val_loss: 2.4574 - val_accuracy: 0.5592 - val_f1: 0.5573 - val_precision: 0.5667 - val_recall: 0.5483\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1782 - accuracy: 0.9362 - f1: 0.9346 - precision: 0.9429 - recall: 0.9265 - val_loss: 2.5107 - val_accuracy: 0.5450 - val_f1: 0.5445 - val_precision: 0.5534 - val_recall: 0.5358\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.1546 - accuracy: 0.9451 - f1: 0.9440 - precision: 0.9537 - recall: 0.9347 - val_loss: 2.5929 - val_accuracy: 0.5625 - val_f1: 0.5637 - val_precision: 0.5737 - val_recall: 0.5542\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.2121 - accuracy: 0.9317 - f1: 0.9328 - precision: 0.9396 - recall: 0.9262 - val_loss: 2.4576 - val_accuracy: 0.5483 - val_f1: 0.5499 - val_precision: 0.5611 - val_recall: 0.5392\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.1831 - accuracy: 0.9409 - f1: 0.9376 - precision: 0.9472 - recall: 0.9282 - val_loss: 2.5147 - val_accuracy: 0.5633 - val_f1: 0.5606 - val_precision: 0.5699 - val_recall: 0.5517\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.1492 - accuracy: 0.9510 - f1: 0.9511 - precision: 0.9573 - recall: 0.9451 - val_loss: 2.6685 - val_accuracy: 0.5617 - val_f1: 0.5598 - val_precision: 0.5674 - val_recall: 0.5525\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.1469 - accuracy: 0.9487 - f1: 0.9492 - precision: 0.9552 - recall: 0.9433 - val_loss: 2.7344 - val_accuracy: 0.5550 - val_f1: 0.5512 - val_precision: 0.5603 - val_recall: 0.5425\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1347 - accuracy: 0.9531 - f1: 0.9538 - precision: 0.9598 - recall: 0.9479 - val_loss: 2.6177 - val_accuracy: 0.5650 - val_f1: 0.5612 - val_precision: 0.5703 - val_recall: 0.5525\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1420 - accuracy: 0.9542 - f1: 0.9540 - precision: 0.9596 - recall: 0.9486 - val_loss: 2.6124 - val_accuracy: 0.5317 - val_f1: 0.5282 - val_precision: 0.5376 - val_recall: 0.5192\n",
            "14/14 [==============================] - 1s 93ms/step - loss: 2.6736 - accuracy: 0.5493 - f1: 0.5463 - precision: 0.5582 - recall: 0.5350\n",
            "[2.6735737323760986, 0.5492857098579407, 0.5462706089019775, 0.5581806302070618, 0.5350000262260437]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_G3Ff5r1bSD",
        "outputId": "74e1ef36-fd9a-430c-ba9b-8a6bbf1b4c7c"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 9s 161ms/step - loss: 1.3850 - accuracy: 0.2787 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3722 - val_accuracy: 0.3308 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3811 - accuracy: 0.2977 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3716 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3772 - accuracy: 0.2984 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3810 - val_accuracy: 0.3200 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3675 - accuracy: 0.3181 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3688 - val_accuracy: 0.3258 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3718 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3668 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3646 - accuracy: 0.3239 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3628 - val_accuracy: 0.3450 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3579 - accuracy: 0.3318 - f1: 5.7094e-04 - precision: 0.0148 - recall: 2.9132e-04 - val_loss: 1.3875 - val_accuracy: 0.2992 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3605 - accuracy: 0.3292 - f1: 0.0053 - precision: 0.1735 - recall: 0.0027 - val_loss: 1.3717 - val_accuracy: 0.3275 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.3563 - accuracy: 0.3328 - f1: 0.0046 - precision: 0.2091 - recall: 0.0023 - val_loss: 1.3741 - val_accuracy: 0.3200 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3563 - accuracy: 0.3290 - f1: 0.0087 - precision: 0.1977 - recall: 0.0045 - val_loss: 1.3818 - val_accuracy: 0.2983 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3599 - accuracy: 0.3240 - f1: 0.0014 - precision: 0.0674 - recall: 6.9927e-04 - val_loss: 1.3980 - val_accuracy: 0.2800 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3527 - accuracy: 0.3345 - f1: 0.0172 - precision: 0.3164 - recall: 0.0092 - val_loss: 1.3887 - val_accuracy: 0.3042 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3442 - accuracy: 0.3500 - f1: 0.0344 - precision: 0.4053 - recall: 0.0185 - val_loss: 1.3765 - val_accuracy: 0.3033 - val_f1: 0.0049 - val_precision: 0.2083 - val_recall: 0.0025\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3167 - accuracy: 0.3732 - f1: 0.0653 - precision: 0.4545 - recall: 0.0370 - val_loss: 1.3576 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3630 - accuracy: 0.3288 - f1: 0.0152 - precision: 0.1768 - recall: 0.0082 - val_loss: 1.3658 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3472 - accuracy: 0.3479 - f1: 0.0023 - precision: 0.0556 - recall: 0.0012 - val_loss: 1.3686 - val_accuracy: 0.3192 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3459 - accuracy: 0.3471 - f1: 0.0108 - precision: 0.2660 - recall: 0.0056 - val_loss: 1.3925 - val_accuracy: 0.2933 - val_f1: 0.0033 - val_precision: 0.1250 - val_recall: 0.0017\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3581 - accuracy: 0.3367 - f1: 0.0365 - precision: 0.5093 - recall: 0.0192 - val_loss: 1.3821 - val_accuracy: 0.2983 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3746 - accuracy: 0.3194 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3838 - val_accuracy: 0.3033 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3555 - accuracy: 0.3329 - f1: 0.0021 - precision: 0.0844 - recall: 0.0011 - val_loss: 1.3879 - val_accuracy: 0.2992 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3386 - accuracy: 0.3516 - f1: 0.0325 - precision: 0.3818 - recall: 0.0177 - val_loss: 1.3896 - val_accuracy: 0.2992 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3298 - accuracy: 0.3609 - f1: 0.0105 - precision: 0.3119 - recall: 0.0054 - val_loss: 1.3711 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3655 - accuracy: 0.3227 - f1: 0.0365 - precision: 0.3557 - recall: 0.0198 - val_loss: 1.3824 - val_accuracy: 0.2983 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3702 - accuracy: 0.3215 - f1: 8.5861e-04 - precision: 0.0320 - recall: 4.3588e-04 - val_loss: 1.3711 - val_accuracy: 0.3200 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3379 - accuracy: 0.3519 - f1: 0.0105 - precision: 0.1731 - recall: 0.0054 - val_loss: 1.3826 - val_accuracy: 0.3342 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3316 - accuracy: 0.3621 - f1: 0.0144 - precision: 0.3068 - recall: 0.0075 - val_loss: 1.3773 - val_accuracy: 0.3258 - val_f1: 0.0049 - val_precision: 0.0972 - val_recall: 0.0025\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3027 - accuracy: 0.3974 - f1: 0.0866 - precision: 0.5213 - recall: 0.0489 - val_loss: 1.3847 - val_accuracy: 0.3100 - val_f1: 0.0257 - val_precision: 0.4444 - val_recall: 0.0133\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.2781 - accuracy: 0.4297 - f1: 0.0959 - precision: 0.5642 - recall: 0.0536 - val_loss: 1.4403 - val_accuracy: 0.2858 - val_f1: 0.1215 - val_precision: 0.3316 - val_recall: 0.0750\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.3187 - accuracy: 0.3774 - f1: 0.1178 - precision: 0.5320 - recall: 0.0687 - val_loss: 1.4335 - val_accuracy: 0.2833 - val_f1: 0.0159 - val_precision: 0.2014 - val_recall: 0.0083\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3679 - accuracy: 0.3366 - f1: 0.0461 - precision: 0.2517 - recall: 0.0263 - val_loss: 1.3922 - val_accuracy: 0.3017 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3291 - accuracy: 0.3708 - f1: 0.0331 - precision: 0.3042 - recall: 0.0178 - val_loss: 1.3851 - val_accuracy: 0.3283 - val_f1: 0.0238 - val_precision: 0.3052 - val_recall: 0.0125\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3012 - accuracy: 0.4014 - f1: 0.0489 - precision: 0.5674 - recall: 0.0259 - val_loss: 1.3660 - val_accuracy: 0.3392 - val_f1: 0.0495 - val_precision: 0.6706 - val_recall: 0.0258\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2601 - accuracy: 0.4370 - f1: 0.1415 - precision: 0.5841 - recall: 0.0830 - val_loss: 1.3660 - val_accuracy: 0.3417 - val_f1: 0.0409 - val_precision: 0.4454 - val_recall: 0.0217\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 7s 147ms/step - loss: 1.2521 - accuracy: 0.4357 - f1: 0.1512 - precision: 0.5802 - recall: 0.0882 - val_loss: 1.4040 - val_accuracy: 0.3258 - val_f1: 0.1163 - val_precision: 0.3862 - val_recall: 0.0692\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2426 - accuracy: 0.4412 - f1: 0.2256 - precision: 0.5848 - recall: 0.1407 - val_loss: 1.4322 - val_accuracy: 0.3008 - val_f1: 0.1241 - val_precision: 0.3705 - val_recall: 0.0750\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.2951 - accuracy: 0.4049 - f1: 0.1967 - precision: 0.5216 - recall: 0.1230 - val_loss: 1.4272 - val_accuracy: 0.3375 - val_f1: 0.1663 - val_precision: 0.3638 - val_recall: 0.1083\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3448 - accuracy: 0.3723 - f1: 0.0843 - precision: 0.4910 - recall: 0.0497 - val_loss: 1.4163 - val_accuracy: 0.2992 - val_f1: 0.0813 - val_precision: 0.4306 - val_recall: 0.0450\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3788 - accuracy: 0.3323 - f1: 0.0543 - precision: 0.4141 - recall: 0.0295 - val_loss: 1.3664 - val_accuracy: 0.3292 - val_f1: 0.0192 - val_precision: 0.3232 - val_recall: 0.0100\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.3296 - accuracy: 0.3759 - f1: 0.0766 - precision: 0.5433 - recall: 0.0419 - val_loss: 1.3739 - val_accuracy: 0.3267 - val_f1: 0.0097 - val_precision: 0.1722 - val_recall: 0.0050\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3246 - accuracy: 0.3816 - f1: 0.0252 - precision: 0.4253 - recall: 0.0132 - val_loss: 1.3680 - val_accuracy: 0.3467 - val_f1: 0.0739 - val_precision: 0.5110 - val_recall: 0.0400\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.2654 - accuracy: 0.4368 - f1: 0.1377 - precision: 0.5754 - recall: 0.0794 - val_loss: 1.3821 - val_accuracy: 0.3417 - val_f1: 0.0864 - val_precision: 0.3717 - val_recall: 0.0492\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2820 - accuracy: 0.4108 - f1: 0.1658 - precision: 0.5433 - recall: 0.0985 - val_loss: 1.4042 - val_accuracy: 0.3075 - val_f1: 0.0781 - val_precision: 0.4068 - val_recall: 0.0433\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2848 - accuracy: 0.4191 - f1: 0.1701 - precision: 0.5293 - recall: 0.1023 - val_loss: 1.3739 - val_accuracy: 0.3400 - val_f1: 0.1387 - val_precision: 0.4604 - val_recall: 0.0825\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2539 - accuracy: 0.4394 - f1: 0.2357 - precision: 0.6209 - recall: 0.1464 - val_loss: 1.4087 - val_accuracy: 0.3142 - val_f1: 0.1490 - val_precision: 0.3725 - val_recall: 0.0933\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.2292 - accuracy: 0.4472 - f1: 0.2833 - precision: 0.6060 - recall: 0.1865 - val_loss: 1.4647 - val_accuracy: 0.2950 - val_f1: 0.1527 - val_precision: 0.2959 - val_recall: 0.1033\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2402 - accuracy: 0.4451 - f1: 0.3045 - precision: 0.5841 - recall: 0.2068 - val_loss: 1.4462 - val_accuracy: 0.3000 - val_f1: 0.1546 - val_precision: 0.3461 - val_recall: 0.1000\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.2280 - accuracy: 0.4550 - f1: 0.2889 - precision: 0.6035 - recall: 0.1907 - val_loss: 1.4360 - val_accuracy: 0.2933 - val_f1: 0.1415 - val_precision: 0.3649 - val_recall: 0.0892\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.2663 - accuracy: 0.4361 - f1: 0.2529 - precision: 0.5412 - recall: 0.1667 - val_loss: 1.4526 - val_accuracy: 0.2700 - val_f1: 0.0807 - val_precision: 0.3033 - val_recall: 0.0467\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3131 - accuracy: 0.3831 - f1: 0.1233 - precision: 0.4806 - recall: 0.0724 - val_loss: 1.4015 - val_accuracy: 0.2750 - val_f1: 0.0347 - val_precision: 0.4272 - val_recall: 0.0183\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.2830 - accuracy: 0.4062 - f1: 0.1187 - precision: 0.4703 - recall: 0.0697 - val_loss: 1.3837 - val_accuracy: 0.3167 - val_f1: 0.0594 - val_precision: 0.3750 - val_recall: 0.0325\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.2691 - accuracy: 0.4234 - f1: 0.1605 - precision: 0.5701 - recall: 0.0942 - val_loss: 1.4097 - val_accuracy: 0.2825 - val_f1: 0.0479 - val_precision: 0.3752 - val_recall: 0.0258\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 1.2761 - accuracy: 0.4154 - f1: 0.1650 - precision: 0.5897 - recall: 0.0975 - val_loss: 1.4047 - val_accuracy: 0.3083 - val_f1: 0.0732 - val_precision: 0.3168 - val_recall: 0.0417\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2283 - accuracy: 0.4587 - f1: 0.2122 - precision: 0.5951 - recall: 0.1315 - val_loss: 1.4141 - val_accuracy: 0.3067 - val_f1: 0.1319 - val_precision: 0.3867 - val_recall: 0.0800\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.2080 - accuracy: 0.4664 - f1: 0.2658 - precision: 0.6076 - recall: 0.1717 - val_loss: 1.4393 - val_accuracy: 0.3225 - val_f1: 0.1291 - val_precision: 0.3278 - val_recall: 0.0808\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 1.1990 - accuracy: 0.4800 - f1: 0.3147 - precision: 0.6333 - recall: 0.2125 - val_loss: 1.4522 - val_accuracy: 0.3192 - val_f1: 0.1765 - val_precision: 0.3752 - val_recall: 0.1158\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.1902 - accuracy: 0.4801 - f1: 0.3339 - precision: 0.6213 - recall: 0.2296 - val_loss: 1.4491 - val_accuracy: 0.3075 - val_f1: 0.1575 - val_precision: 0.3553 - val_recall: 0.1017\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.2034 - accuracy: 0.4681 - f1: 0.3191 - precision: 0.6092 - recall: 0.2171 - val_loss: 1.4217 - val_accuracy: 0.3092 - val_f1: 0.1522 - val_precision: 0.3541 - val_recall: 0.0975\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.1679 - accuracy: 0.5021 - f1: 0.3237 - precision: 0.6287 - recall: 0.2213 - val_loss: 1.4213 - val_accuracy: 0.3333 - val_f1: 0.1961 - val_precision: 0.4047 - val_recall: 0.1300\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.2259 - accuracy: 0.4581 - f1: 0.2959 - precision: 0.5794 - recall: 0.2000 - val_loss: 1.4385 - val_accuracy: 0.3033 - val_f1: 0.1049 - val_precision: 0.3166 - val_recall: 0.0633\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.2148 - accuracy: 0.4618 - f1: 0.2724 - precision: 0.6148 - recall: 0.1785 - val_loss: 1.4292 - val_accuracy: 0.3117 - val_f1: 0.1535 - val_precision: 0.3788 - val_recall: 0.0967\n",
            "14/14 [==============================] - 1s 88ms/step - loss: 1.4445 - accuracy: 0.3150 - f1: 0.1509 - precision: 0.3480 - recall: 0.0971\n",
            "[1.444521188735962, 0.3149999976158142, 0.15092559158802032, 0.3479810357093811, 0.09714286774396896]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czoFmEwK1dWT",
        "outputId": "86736d9a-4b66-43e4-a268-9ece9fc4b1fe"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(64))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 4s 40ms/step - loss: 1.3782 - accuracy: 0.3092 - f1: 0.0055 - precision: 0.0251 - recall: 0.0031 - val_loss: 1.2567 - val_accuracy: 0.4350 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 1.2324 - accuracy: 0.4246 - f1: 0.1676 - precision: 0.4961 - recall: 0.1049 - val_loss: 1.1843 - val_accuracy: 0.4692 - val_f1: 0.2105 - val_precision: 0.7029 - val_recall: 0.1242\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 1.1531 - accuracy: 0.4627 - f1: 0.3165 - precision: 0.6567 - recall: 0.2123 - val_loss: 1.1660 - val_accuracy: 0.4442 - val_f1: 0.3060 - val_precision: 0.6921 - val_recall: 0.1975\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 1.1207 - accuracy: 0.4796 - f1: 0.3373 - precision: 0.6793 - recall: 0.2303 - val_loss: 1.1591 - val_accuracy: 0.4458 - val_f1: 0.2729 - val_precision: 0.7464 - val_recall: 0.1683\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 1.0990 - accuracy: 0.4696 - f1: 0.3511 - precision: 0.6992 - recall: 0.2428 - val_loss: 1.1854 - val_accuracy: 0.4200 - val_f1: 0.2723 - val_precision: 0.7657 - val_recall: 0.1667\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 1.0603 - accuracy: 0.4921 - f1: 0.3859 - precision: 0.7653 - recall: 0.2670 - val_loss: 1.1254 - val_accuracy: 0.4658 - val_f1: 0.2990 - val_precision: 0.8762 - val_recall: 0.1817\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.9846 - accuracy: 0.5499 - f1: 0.4381 - precision: 0.8197 - recall: 0.3070 - val_loss: 1.0271 - val_accuracy: 0.5342 - val_f1: 0.4036 - val_precision: 0.8629 - val_recall: 0.2658\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.9108 - accuracy: 0.5991 - f1: 0.4891 - precision: 0.8552 - recall: 0.3477 - val_loss: 1.0077 - val_accuracy: 0.5733 - val_f1: 0.4699 - val_precision: 0.8293 - val_recall: 0.3300\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.9055 - accuracy: 0.6047 - f1: 0.5051 - precision: 0.8516 - recall: 0.3628 - val_loss: 1.1240 - val_accuracy: 0.4383 - val_f1: 0.3195 - val_precision: 0.9247 - val_recall: 0.1950\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.9529 - accuracy: 0.5669 - f1: 0.4724 - precision: 0.8664 - recall: 0.3350 - val_loss: 1.0226 - val_accuracy: 0.5858 - val_f1: 0.4672 - val_precision: 0.8257 - val_recall: 0.3275\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.8528 - accuracy: 0.6443 - f1: 0.5238 - precision: 0.8458 - recall: 0.3842 - val_loss: 0.9904 - val_accuracy: 0.5775 - val_f1: 0.4696 - val_precision: 0.8471 - val_recall: 0.3267\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.8097 - accuracy: 0.6692 - f1: 0.5699 - precision: 0.8467 - recall: 0.4335 - val_loss: 1.0237 - val_accuracy: 0.5725 - val_f1: 0.5480 - val_precision: 0.7339 - val_recall: 0.4392\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.8104 - accuracy: 0.6660 - f1: 0.5925 - precision: 0.8183 - recall: 0.4671 - val_loss: 0.9722 - val_accuracy: 0.6125 - val_f1: 0.5818 - val_precision: 0.6863 - val_recall: 0.5058\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.7616 - accuracy: 0.6948 - f1: 0.6529 - precision: 0.8147 - recall: 0.5468 - val_loss: 1.0099 - val_accuracy: 0.6242 - val_f1: 0.5988 - val_precision: 0.6553 - val_recall: 0.5517\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.6975 - accuracy: 0.7329 - f1: 0.7036 - precision: 0.7746 - recall: 0.6456 - val_loss: 1.0180 - val_accuracy: 0.6333 - val_f1: 0.6118 - val_precision: 0.6552 - val_recall: 0.5742\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.6691 - accuracy: 0.7588 - f1: 0.7357 - precision: 0.8016 - recall: 0.6817 - val_loss: 1.0367 - val_accuracy: 0.6142 - val_f1: 0.6034 - val_precision: 0.6359 - val_recall: 0.5742\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.5927 - accuracy: 0.7839 - f1: 0.7792 - precision: 0.8170 - recall: 0.7454 - val_loss: 1.0758 - val_accuracy: 0.6492 - val_f1: 0.6494 - val_precision: 0.6711 - val_recall: 0.6292\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.5534 - accuracy: 0.8103 - f1: 0.8114 - precision: 0.8391 - recall: 0.7858 - val_loss: 1.0279 - val_accuracy: 0.6500 - val_f1: 0.6501 - val_precision: 0.6776 - val_recall: 0.6250\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.5570 - accuracy: 0.8175 - f1: 0.8086 - precision: 0.8437 - recall: 0.7771 - val_loss: 0.9455 - val_accuracy: 0.7017 - val_f1: 0.6980 - val_precision: 0.7315 - val_recall: 0.6675\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.5333 - accuracy: 0.8233 - f1: 0.8179 - precision: 0.8548 - recall: 0.7845 - val_loss: 0.9714 - val_accuracy: 0.6992 - val_f1: 0.6996 - val_precision: 0.7214 - val_recall: 0.6792\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4689 - accuracy: 0.8522 - f1: 0.8501 - precision: 0.8718 - recall: 0.8298 - val_loss: 0.9968 - val_accuracy: 0.6850 - val_f1: 0.6873 - val_precision: 0.7223 - val_recall: 0.6558\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4602 - accuracy: 0.8501 - f1: 0.8467 - precision: 0.8757 - recall: 0.8197 - val_loss: 1.0409 - val_accuracy: 0.7100 - val_f1: 0.7105 - val_precision: 0.7354 - val_recall: 0.6875\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4325 - accuracy: 0.8688 - f1: 0.8674 - precision: 0.8885 - recall: 0.8475 - val_loss: 1.0307 - val_accuracy: 0.7150 - val_f1: 0.7124 - val_precision: 0.7309 - val_recall: 0.6950\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4147 - accuracy: 0.8658 - f1: 0.8676 - precision: 0.8908 - recall: 0.8457 - val_loss: 0.9789 - val_accuracy: 0.7242 - val_f1: 0.7184 - val_precision: 0.7416 - val_recall: 0.6967\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.3756 - accuracy: 0.8854 - f1: 0.8829 - precision: 0.9065 - recall: 0.8608 - val_loss: 1.0283 - val_accuracy: 0.7075 - val_f1: 0.7132 - val_precision: 0.7361 - val_recall: 0.6917\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.4089 - accuracy: 0.8750 - f1: 0.8740 - precision: 0.8948 - recall: 0.8546 - val_loss: 1.0116 - val_accuracy: 0.7250 - val_f1: 0.7318 - val_precision: 0.7525 - val_recall: 0.7125\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4239 - accuracy: 0.8816 - f1: 0.8780 - precision: 0.8939 - recall: 0.8629 - val_loss: 0.9694 - val_accuracy: 0.7267 - val_f1: 0.7298 - val_precision: 0.7463 - val_recall: 0.7142\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.3856 - accuracy: 0.8833 - f1: 0.8872 - precision: 0.9058 - recall: 0.8697 - val_loss: 1.0561 - val_accuracy: 0.7033 - val_f1: 0.7077 - val_precision: 0.7265 - val_recall: 0.6900\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4141 - accuracy: 0.8780 - f1: 0.8731 - precision: 0.8914 - recall: 0.8557 - val_loss: 1.0308 - val_accuracy: 0.7275 - val_f1: 0.7244 - val_precision: 0.7397 - val_recall: 0.7100\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.3884 - accuracy: 0.8813 - f1: 0.8805 - precision: 0.8991 - recall: 0.8630 - val_loss: 0.9981 - val_accuracy: 0.7517 - val_f1: 0.7571 - val_precision: 0.7769 - val_recall: 0.7383\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.3203 - accuracy: 0.9104 - f1: 0.9150 - precision: 0.9284 - recall: 0.9021 - val_loss: 1.0553 - val_accuracy: 0.7525 - val_f1: 0.7499 - val_precision: 0.7638 - val_recall: 0.7367\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2981 - accuracy: 0.9159 - f1: 0.9165 - precision: 0.9289 - recall: 0.9047 - val_loss: 1.0672 - val_accuracy: 0.7500 - val_f1: 0.7529 - val_precision: 0.7654 - val_recall: 0.7408\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2880 - accuracy: 0.9228 - f1: 0.9216 - precision: 0.9325 - recall: 0.9111 - val_loss: 1.0718 - val_accuracy: 0.7425 - val_f1: 0.7446 - val_precision: 0.7545 - val_recall: 0.7350\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2666 - accuracy: 0.9220 - f1: 0.9230 - precision: 0.9316 - recall: 0.9147 - val_loss: 0.9616 - val_accuracy: 0.7617 - val_f1: 0.7619 - val_precision: 0.7733 - val_recall: 0.7508\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2371 - accuracy: 0.9328 - f1: 0.9323 - precision: 0.9406 - recall: 0.9241 - val_loss: 0.9508 - val_accuracy: 0.7617 - val_f1: 0.7617 - val_precision: 0.7757 - val_recall: 0.7483\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2708 - accuracy: 0.9282 - f1: 0.9268 - precision: 0.9389 - recall: 0.9150 - val_loss: 0.9998 - val_accuracy: 0.7625 - val_f1: 0.7621 - val_precision: 0.7730 - val_recall: 0.7517\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.3652 - accuracy: 0.9031 - f1: 0.9040 - precision: 0.9183 - recall: 0.8905 - val_loss: 0.8518 - val_accuracy: 0.7683 - val_f1: 0.7660 - val_precision: 0.7819 - val_recall: 0.7508\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.3014 - accuracy: 0.9157 - f1: 0.9182 - precision: 0.9330 - recall: 0.9041 - val_loss: 1.0417 - val_accuracy: 0.7458 - val_f1: 0.7472 - val_precision: 0.7627 - val_recall: 0.7325\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2911 - accuracy: 0.9206 - f1: 0.9207 - precision: 0.9346 - recall: 0.9072 - val_loss: 0.9168 - val_accuracy: 0.7567 - val_f1: 0.7583 - val_precision: 0.7758 - val_recall: 0.7417\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2764 - accuracy: 0.9189 - f1: 0.9216 - precision: 0.9377 - recall: 0.9063 - val_loss: 0.8591 - val_accuracy: 0.7475 - val_f1: 0.7492 - val_precision: 0.7849 - val_recall: 0.7167\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2503 - accuracy: 0.9319 - f1: 0.9303 - precision: 0.9476 - recall: 0.9138 - val_loss: 0.9702 - val_accuracy: 0.7508 - val_f1: 0.7552 - val_precision: 0.7677 - val_recall: 0.7433\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2181 - accuracy: 0.9359 - f1: 0.9376 - precision: 0.9477 - recall: 0.9278 - val_loss: 0.9945 - val_accuracy: 0.7567 - val_f1: 0.7616 - val_precision: 0.7773 - val_recall: 0.7467\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2048 - accuracy: 0.9456 - f1: 0.9457 - precision: 0.9565 - recall: 0.9353 - val_loss: 1.0280 - val_accuracy: 0.7583 - val_f1: 0.7559 - val_precision: 0.7735 - val_recall: 0.7392\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1827 - accuracy: 0.9542 - f1: 0.9534 - precision: 0.9625 - recall: 0.9446 - val_loss: 1.0857 - val_accuracy: 0.7542 - val_f1: 0.7590 - val_precision: 0.7745 - val_recall: 0.7442\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2790 - accuracy: 0.9279 - f1: 0.9272 - precision: 0.9422 - recall: 0.9130 - val_loss: 0.8592 - val_accuracy: 0.7675 - val_f1: 0.7687 - val_precision: 0.7924 - val_recall: 0.7467\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2227 - accuracy: 0.9395 - f1: 0.9377 - precision: 0.9483 - recall: 0.9275 - val_loss: 0.9514 - val_accuracy: 0.7625 - val_f1: 0.7613 - val_precision: 0.7811 - val_recall: 0.7425\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.4057 - accuracy: 0.8845 - f1: 0.8861 - precision: 0.9052 - recall: 0.8682 - val_loss: 0.9412 - val_accuracy: 0.7317 - val_f1: 0.7418 - val_precision: 0.7700 - val_recall: 0.7158\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.3050 - accuracy: 0.9121 - f1: 0.9167 - precision: 0.9374 - recall: 0.8971 - val_loss: 0.9968 - val_accuracy: 0.7508 - val_f1: 0.7507 - val_precision: 0.7746 - val_recall: 0.7283\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.2519 - accuracy: 0.9303 - f1: 0.9302 - precision: 0.9470 - recall: 0.9142 - val_loss: 0.9689 - val_accuracy: 0.7483 - val_f1: 0.7511 - val_precision: 0.7812 - val_recall: 0.7233\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2031 - accuracy: 0.9468 - f1: 0.9481 - precision: 0.9593 - recall: 0.9372 - val_loss: 0.9768 - val_accuracy: 0.7408 - val_f1: 0.7480 - val_precision: 0.7806 - val_recall: 0.7183\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1854 - accuracy: 0.9489 - f1: 0.9505 - precision: 0.9596 - recall: 0.9417 - val_loss: 0.9908 - val_accuracy: 0.7483 - val_f1: 0.7560 - val_precision: 0.7850 - val_recall: 0.7292\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1845 - accuracy: 0.9475 - f1: 0.9508 - precision: 0.9642 - recall: 0.9381 - val_loss: 1.0455 - val_accuracy: 0.7483 - val_f1: 0.7517 - val_precision: 0.7769 - val_recall: 0.7283\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1690 - accuracy: 0.9543 - f1: 0.9563 - precision: 0.9659 - recall: 0.9470 - val_loss: 1.0999 - val_accuracy: 0.7542 - val_f1: 0.7590 - val_precision: 0.7746 - val_recall: 0.7442\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1624 - accuracy: 0.9536 - f1: 0.9566 - precision: 0.9639 - recall: 0.9496 - val_loss: 1.2558 - val_accuracy: 0.7400 - val_f1: 0.7439 - val_precision: 0.7549 - val_recall: 0.7333\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.2006 - accuracy: 0.9418 - f1: 0.9444 - precision: 0.9522 - recall: 0.9367 - val_loss: 1.0875 - val_accuracy: 0.7575 - val_f1: 0.7571 - val_precision: 0.7752 - val_recall: 0.7400\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.1812 - accuracy: 0.9506 - f1: 0.9533 - precision: 0.9621 - recall: 0.9447 - val_loss: 1.0919 - val_accuracy: 0.7450 - val_f1: 0.7451 - val_precision: 0.7786 - val_recall: 0.7150\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1853 - accuracy: 0.9534 - f1: 0.9490 - precision: 0.9638 - recall: 0.9350 - val_loss: 0.9801 - val_accuracy: 0.7508 - val_f1: 0.7401 - val_precision: 0.7732 - val_recall: 0.7100\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2080 - accuracy: 0.9409 - f1: 0.9423 - precision: 0.9568 - recall: 0.9284 - val_loss: 0.9842 - val_accuracy: 0.7508 - val_f1: 0.7539 - val_precision: 0.7731 - val_recall: 0.7358\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1546 - accuracy: 0.9589 - f1: 0.9591 - precision: 0.9662 - recall: 0.9522 - val_loss: 1.0104 - val_accuracy: 0.7583 - val_f1: 0.7573 - val_precision: 0.7747 - val_recall: 0.7408\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1499 - accuracy: 0.9610 - f1: 0.9617 - precision: 0.9687 - recall: 0.9548 - val_loss: 1.0645 - val_accuracy: 0.7575 - val_f1: 0.7568 - val_precision: 0.7772 - val_recall: 0.7375\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 0.9858 - accuracy: 0.7729 - f1: 0.7774 - precision: 0.7966 - recall: 0.7593\n",
            "[0.9857868552207947, 0.772857129573822, 0.7774186134338379, 0.7965520620346069, 0.7592857480049133]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKM9PEwm1fkb",
        "outputId": "7afea956-e2a1-44ef-fbe6-83df9195e778"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 6s 65ms/step - loss: 1.3795 - accuracy: 0.2991 - f1: 2.7681e-05 - precision: 0.0014 - recall: 1.3979e-05 - val_loss: 1.3126 - val_accuracy: 0.3992 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 1.2750 - accuracy: 0.4161 - f1: 0.0704 - precision: 0.5136 - recall: 0.0403 - val_loss: 1.1612 - val_accuracy: 0.5000 - val_f1: 0.2600 - val_precision: 0.6537 - val_recall: 0.1633\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 1.0875 - accuracy: 0.5579 - f1: 0.3803 - precision: 0.6855 - recall: 0.2712 - val_loss: 0.9972 - val_accuracy: 0.5950 - val_f1: 0.5092 - val_precision: 0.6623 - val_recall: 0.4142\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.9103 - accuracy: 0.6392 - f1: 0.5781 - precision: 0.7188 - recall: 0.4850 - val_loss: 0.9198 - val_accuracy: 0.6175 - val_f1: 0.5880 - val_precision: 0.6827 - val_recall: 0.5167\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.7855 - accuracy: 0.7109 - f1: 0.6788 - precision: 0.7822 - recall: 0.6007 - val_loss: 0.8765 - val_accuracy: 0.6367 - val_f1: 0.6138 - val_precision: 0.6976 - val_recall: 0.5483\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.7051 - accuracy: 0.7439 - f1: 0.7192 - precision: 0.8071 - recall: 0.6496 - val_loss: 0.8499 - val_accuracy: 0.6667 - val_f1: 0.6466 - val_precision: 0.7110 - val_recall: 0.5933\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.6113 - accuracy: 0.7843 - f1: 0.7732 - precision: 0.8353 - recall: 0.7203 - val_loss: 0.8057 - val_accuracy: 0.6792 - val_f1: 0.6711 - val_precision: 0.7321 - val_recall: 0.6200\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.5339 - accuracy: 0.8100 - f1: 0.8088 - precision: 0.8623 - recall: 0.7621 - val_loss: 0.7785 - val_accuracy: 0.6858 - val_f1: 0.6800 - val_precision: 0.7380 - val_recall: 0.6308\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.4901 - accuracy: 0.8263 - f1: 0.8182 - precision: 0.8649 - recall: 0.7769 - val_loss: 0.7951 - val_accuracy: 0.6967 - val_f1: 0.6849 - val_precision: 0.7316 - val_recall: 0.6442\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.4203 - accuracy: 0.8530 - f1: 0.8489 - precision: 0.8870 - recall: 0.8144 - val_loss: 0.7578 - val_accuracy: 0.7100 - val_f1: 0.7065 - val_precision: 0.7477 - val_recall: 0.6700\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.3569 - accuracy: 0.8723 - f1: 0.8692 - precision: 0.8988 - recall: 0.8417 - val_loss: 0.7953 - val_accuracy: 0.7083 - val_f1: 0.7112 - val_precision: 0.7360 - val_recall: 0.6883\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.3201 - accuracy: 0.8902 - f1: 0.8893 - precision: 0.9149 - recall: 0.8655 - val_loss: 0.8378 - val_accuracy: 0.7217 - val_f1: 0.7194 - val_precision: 0.7392 - val_recall: 0.7008\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.2842 - accuracy: 0.8955 - f1: 0.8977 - precision: 0.9189 - recall: 0.8777 - val_loss: 0.9498 - val_accuracy: 0.7100 - val_f1: 0.7066 - val_precision: 0.7233 - val_recall: 0.6908\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.2854 - accuracy: 0.8999 - f1: 0.8995 - precision: 0.9185 - recall: 0.8814 - val_loss: 0.9289 - val_accuracy: 0.7108 - val_f1: 0.7107 - val_precision: 0.7273 - val_recall: 0.6950\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.2958 - accuracy: 0.8895 - f1: 0.8899 - precision: 0.9107 - recall: 0.8704 - val_loss: 0.8399 - val_accuracy: 0.7300 - val_f1: 0.7282 - val_precision: 0.7466 - val_recall: 0.7108\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.2343 - accuracy: 0.9123 - f1: 0.9139 - precision: 0.9293 - recall: 0.8991 - val_loss: 0.9587 - val_accuracy: 0.7183 - val_f1: 0.7129 - val_precision: 0.7283 - val_recall: 0.6983\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.2205 - accuracy: 0.9224 - f1: 0.9210 - precision: 0.9344 - recall: 0.9081 - val_loss: 1.0931 - val_accuracy: 0.7150 - val_f1: 0.7132 - val_precision: 0.7235 - val_recall: 0.7033\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.1908 - accuracy: 0.9327 - f1: 0.9354 - precision: 0.9444 - recall: 0.9268 - val_loss: 1.1124 - val_accuracy: 0.7292 - val_f1: 0.7306 - val_precision: 0.7356 - val_recall: 0.7258\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.1735 - accuracy: 0.9362 - f1: 0.9375 - precision: 0.9454 - recall: 0.9298 - val_loss: 1.1166 - val_accuracy: 0.7317 - val_f1: 0.7311 - val_precision: 0.7383 - val_recall: 0.7242\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.2050 - accuracy: 0.9230 - f1: 0.9235 - precision: 0.9312 - recall: 0.9161 - val_loss: 1.1348 - val_accuracy: 0.7142 - val_f1: 0.7157 - val_precision: 0.7251 - val_recall: 0.7067\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.2047 - accuracy: 0.9230 - f1: 0.9212 - precision: 0.9294 - recall: 0.9132 - val_loss: 1.0584 - val_accuracy: 0.7400 - val_f1: 0.7365 - val_precision: 0.7441 - val_recall: 0.7292\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.1744 - accuracy: 0.9337 - f1: 0.9351 - precision: 0.9431 - recall: 0.9273 - val_loss: 1.0861 - val_accuracy: 0.7358 - val_f1: 0.7362 - val_precision: 0.7435 - val_recall: 0.7292\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.1232 - accuracy: 0.9524 - f1: 0.9509 - precision: 0.9551 - recall: 0.9468 - val_loss: 1.0942 - val_accuracy: 0.7317 - val_f1: 0.7325 - val_precision: 0.7420 - val_recall: 0.7233\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.1076 - accuracy: 0.9610 - f1: 0.9617 - precision: 0.9643 - recall: 0.9591 - val_loss: 1.1134 - val_accuracy: 0.7333 - val_f1: 0.7326 - val_precision: 0.7396 - val_recall: 0.7258\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0872 - accuracy: 0.9677 - f1: 0.9694 - precision: 0.9735 - recall: 0.9655 - val_loss: 1.1484 - val_accuracy: 0.7433 - val_f1: 0.7454 - val_precision: 0.7510 - val_recall: 0.7400\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 2s 39ms/step - loss: 0.0759 - accuracy: 0.9708 - f1: 0.9706 - precision: 0.9739 - recall: 0.9674 - val_loss: 1.1852 - val_accuracy: 0.7317 - val_f1: 0.7366 - val_precision: 0.7442 - val_recall: 0.7292\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0815 - accuracy: 0.9700 - f1: 0.9687 - precision: 0.9709 - recall: 0.9665 - val_loss: 1.3078 - val_accuracy: 0.7300 - val_f1: 0.7314 - val_precision: 0.7389 - val_recall: 0.7242\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0783 - accuracy: 0.9723 - f1: 0.9719 - precision: 0.9733 - recall: 0.9706 - val_loss: 1.3772 - val_accuracy: 0.7250 - val_f1: 0.7256 - val_precision: 0.7322 - val_recall: 0.7192\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0705 - accuracy: 0.9724 - f1: 0.9723 - precision: 0.9754 - recall: 0.9692 - val_loss: 1.3337 - val_accuracy: 0.7325 - val_f1: 0.7345 - val_precision: 0.7400 - val_recall: 0.7292\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0882 - accuracy: 0.9690 - f1: 0.9679 - precision: 0.9695 - recall: 0.9664 - val_loss: 1.2650 - val_accuracy: 0.7442 - val_f1: 0.7434 - val_precision: 0.7494 - val_recall: 0.7375\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0796 - accuracy: 0.9700 - f1: 0.9696 - precision: 0.9713 - recall: 0.9679 - val_loss: 1.5300 - val_accuracy: 0.7267 - val_f1: 0.7292 - val_precision: 0.7335 - val_recall: 0.7250\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0874 - accuracy: 0.9682 - f1: 0.9679 - precision: 0.9697 - recall: 0.9662 - val_loss: 1.3920 - val_accuracy: 0.7350 - val_f1: 0.7309 - val_precision: 0.7352 - val_recall: 0.7267\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0663 - accuracy: 0.9720 - f1: 0.9729 - precision: 0.9749 - recall: 0.9711 - val_loss: 1.5838 - val_accuracy: 0.7233 - val_f1: 0.7236 - val_precision: 0.7273 - val_recall: 0.7200\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0634 - accuracy: 0.9761 - f1: 0.9758 - precision: 0.9766 - recall: 0.9750 - val_loss: 1.4087 - val_accuracy: 0.7275 - val_f1: 0.7288 - val_precision: 0.7352 - val_recall: 0.7225\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0650 - accuracy: 0.9686 - f1: 0.9685 - precision: 0.9696 - recall: 0.9673 - val_loss: 1.3783 - val_accuracy: 0.7258 - val_f1: 0.7269 - val_precision: 0.7322 - val_recall: 0.7217\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0637 - accuracy: 0.9707 - f1: 0.9707 - precision: 0.9719 - recall: 0.9696 - val_loss: 1.4446 - val_accuracy: 0.7242 - val_f1: 0.7267 - val_precision: 0.7301 - val_recall: 0.7233\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0476 - accuracy: 0.9774 - f1: 0.9766 - precision: 0.9780 - recall: 0.9752 - val_loss: 1.5765 - val_accuracy: 0.7142 - val_f1: 0.7118 - val_precision: 0.7146 - val_recall: 0.7092\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0563 - accuracy: 0.9746 - f1: 0.9744 - precision: 0.9755 - recall: 0.9733 - val_loss: 1.6576 - val_accuracy: 0.7250 - val_f1: 0.7263 - val_precision: 0.7302 - val_recall: 0.7225\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0493 - accuracy: 0.9776 - f1: 0.9769 - precision: 0.9789 - recall: 0.9750 - val_loss: 1.6552 - val_accuracy: 0.7133 - val_f1: 0.7153 - val_precision: 0.7190 - val_recall: 0.7117\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0534 - accuracy: 0.9767 - f1: 0.9773 - precision: 0.9787 - recall: 0.9760 - val_loss: 1.6709 - val_accuracy: 0.7158 - val_f1: 0.7159 - val_precision: 0.7202 - val_recall: 0.7117\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0515 - accuracy: 0.9742 - f1: 0.9743 - precision: 0.9767 - recall: 0.9720 - val_loss: 1.6446 - val_accuracy: 0.7300 - val_f1: 0.7314 - val_precision: 0.7345 - val_recall: 0.7283\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0511 - accuracy: 0.9748 - f1: 0.9751 - precision: 0.9758 - recall: 0.9744 - val_loss: 1.6787 - val_accuracy: 0.7308 - val_f1: 0.7289 - val_precision: 0.7329 - val_recall: 0.7250\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0499 - accuracy: 0.9762 - f1: 0.9764 - precision: 0.9776 - recall: 0.9752 - val_loss: 1.6556 - val_accuracy: 0.7408 - val_f1: 0.7412 - val_precision: 0.7459 - val_recall: 0.7367\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0470 - accuracy: 0.9760 - f1: 0.9754 - precision: 0.9776 - recall: 0.9733 - val_loss: 1.7490 - val_accuracy: 0.7267 - val_f1: 0.7260 - val_precision: 0.7321 - val_recall: 0.7200\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0509 - accuracy: 0.9745 - f1: 0.9753 - precision: 0.9768 - recall: 0.9738 - val_loss: 1.8281 - val_accuracy: 0.7300 - val_f1: 0.7332 - val_precision: 0.7381 - val_recall: 0.7283\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0559 - accuracy: 0.9722 - f1: 0.9715 - precision: 0.9737 - recall: 0.9693 - val_loss: 1.6951 - val_accuracy: 0.7267 - val_f1: 0.7268 - val_precision: 0.7339 - val_recall: 0.7200\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0554 - accuracy: 0.9733 - f1: 0.9730 - precision: 0.9746 - recall: 0.9715 - val_loss: 1.4835 - val_accuracy: 0.7417 - val_f1: 0.7399 - val_precision: 0.7457 - val_recall: 0.7342\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0566 - accuracy: 0.9746 - f1: 0.9751 - precision: 0.9778 - recall: 0.9724 - val_loss: 1.7065 - val_accuracy: 0.7283 - val_f1: 0.7284 - val_precision: 0.7337 - val_recall: 0.7233\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0522 - accuracy: 0.9755 - f1: 0.9776 - precision: 0.9811 - recall: 0.9743 - val_loss: 1.8693 - val_accuracy: 0.7333 - val_f1: 0.7323 - val_precision: 0.7364 - val_recall: 0.7283\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0481 - accuracy: 0.9758 - f1: 0.9760 - precision: 0.9777 - recall: 0.9742 - val_loss: 1.4615 - val_accuracy: 0.7358 - val_f1: 0.7352 - val_precision: 0.7406 - val_recall: 0.7300\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0491 - accuracy: 0.9762 - f1: 0.9763 - precision: 0.9769 - recall: 0.9756 - val_loss: 1.6228 - val_accuracy: 0.7425 - val_f1: 0.7420 - val_precision: 0.7458 - val_recall: 0.7383\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0505 - accuracy: 0.9763 - f1: 0.9766 - precision: 0.9783 - recall: 0.9749 - val_loss: 1.4870 - val_accuracy: 0.7458 - val_f1: 0.7447 - val_precision: 0.7478 - val_recall: 0.7417\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0564 - accuracy: 0.9751 - f1: 0.9747 - precision: 0.9769 - recall: 0.9726 - val_loss: 1.6086 - val_accuracy: 0.7358 - val_f1: 0.7374 - val_precision: 0.7399 - val_recall: 0.7350\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0505 - accuracy: 0.9777 - f1: 0.9775 - precision: 0.9797 - recall: 0.9753 - val_loss: 1.7799 - val_accuracy: 0.7383 - val_f1: 0.7377 - val_precision: 0.7430 - val_recall: 0.7325\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0450 - accuracy: 0.9771 - f1: 0.9774 - precision: 0.9791 - recall: 0.9757 - val_loss: 1.8920 - val_accuracy: 0.7392 - val_f1: 0.7411 - val_precision: 0.7439 - val_recall: 0.7383\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0445 - accuracy: 0.9778 - f1: 0.9773 - precision: 0.9797 - recall: 0.9749 - val_loss: 1.7913 - val_accuracy: 0.7433 - val_f1: 0.7457 - val_precision: 0.7481 - val_recall: 0.7433\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0441 - accuracy: 0.9787 - f1: 0.9780 - precision: 0.9807 - recall: 0.9753 - val_loss: 2.0409 - val_accuracy: 0.7450 - val_f1: 0.7456 - val_precision: 0.7487 - val_recall: 0.7425\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0616 - accuracy: 0.9706 - f1: 0.9702 - precision: 0.9712 - recall: 0.9693 - val_loss: 1.7012 - val_accuracy: 0.7542 - val_f1: 0.7548 - val_precision: 0.7554 - val_recall: 0.7542\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0432 - accuracy: 0.9774 - f1: 0.9773 - precision: 0.9791 - recall: 0.9755 - val_loss: 1.9379 - val_accuracy: 0.7392 - val_f1: 0.7377 - val_precision: 0.7396 - val_recall: 0.7358\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0412 - accuracy: 0.9769 - f1: 0.9762 - precision: 0.9792 - recall: 0.9732 - val_loss: 1.9590 - val_accuracy: 0.7492 - val_f1: 0.7477 - val_precision: 0.7495 - val_recall: 0.7458\n",
            "14/14 [==============================] - 0s 23ms/step - loss: 1.5670 - accuracy: 0.7686 - f1: 0.7693 - precision: 0.7715 - recall: 0.7671\n",
            "[1.5670406818389893, 0.7685714364051819, 0.7693018317222595, 0.7714937925338745, 0.7671428322792053]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQBa5s4s1hNU",
        "outputId": "5a9b396e-4f0d-43e9-faa6-166a97bf1cd7"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 6s 96ms/step - loss: 1.3845 - accuracy: 0.2921 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3728 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 1.3776 - accuracy: 0.2919 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3569 - val_accuracy: 0.3625 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 1.3643 - accuracy: 0.3274 - f1: 0.0083 - precision: 0.2149 - recall: 0.0043 - val_loss: 1.3599 - val_accuracy: 0.3383 - val_f1: 0.1115 - val_precision: 0.5290 - val_recall: 0.0625\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 1.3410 - accuracy: 0.3512 - f1: 0.0880 - precision: 0.5583 - recall: 0.0493 - val_loss: 1.3546 - val_accuracy: 0.3658 - val_f1: 0.2715 - val_precision: 0.4279 - val_recall: 0.1992\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 1.3206 - accuracy: 0.3867 - f1: 0.1369 - precision: 0.4981 - recall: 0.0845 - val_loss: 1.3403 - val_accuracy: 0.3783 - val_f1: 0.2316 - val_precision: 0.4573 - val_recall: 0.1558\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 1.2689 - accuracy: 0.4113 - f1: 0.2410 - precision: 0.5738 - recall: 0.1546 - val_loss: 1.3017 - val_accuracy: 0.3908 - val_f1: 0.2372 - val_precision: 0.5339 - val_recall: 0.1533\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 1.2190 - accuracy: 0.4605 - f1: 0.2917 - precision: 0.6024 - recall: 0.1955 - val_loss: 1.2851 - val_accuracy: 0.3967 - val_f1: 0.2386 - val_precision: 0.6024 - val_recall: 0.1492\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 1.1682 - accuracy: 0.4940 - f1: 0.3636 - precision: 0.6445 - recall: 0.2589 - val_loss: 1.3433 - val_accuracy: 0.3867 - val_f1: 0.3146 - val_precision: 0.4953 - val_recall: 0.2317\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 1.1376 - accuracy: 0.5106 - f1: 0.4035 - precision: 0.6491 - recall: 0.2969 - val_loss: 1.3179 - val_accuracy: 0.4042 - val_f1: 0.3490 - val_precision: 0.4984 - val_recall: 0.2692\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 1.1518 - accuracy: 0.5120 - f1: 0.3841 - precision: 0.6427 - recall: 0.2820 - val_loss: 1.3441 - val_accuracy: 0.3542 - val_f1: 0.1840 - val_precision: 0.5243 - val_recall: 0.1125\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 1.1537 - accuracy: 0.4894 - f1: 0.3439 - precision: 0.6427 - recall: 0.2457 - val_loss: 1.3121 - val_accuracy: 0.4317 - val_f1: 0.3338 - val_precision: 0.5111 - val_recall: 0.2483\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.9980 - accuracy: 0.6026 - f1: 0.5125 - precision: 0.7123 - recall: 0.4029 - val_loss: 1.3061 - val_accuracy: 0.4108 - val_f1: 0.3420 - val_precision: 0.5395 - val_recall: 0.2508\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.9786 - accuracy: 0.6137 - f1: 0.5371 - precision: 0.7242 - recall: 0.4297 - val_loss: 1.3134 - val_accuracy: 0.4192 - val_f1: 0.3117 - val_precision: 0.5284 - val_recall: 0.2217\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 1.0021 - accuracy: 0.6086 - f1: 0.5284 - precision: 0.7027 - recall: 0.4271 - val_loss: 1.3273 - val_accuracy: 0.4475 - val_f1: 0.3786 - val_precision: 0.5125 - val_recall: 0.3008\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.9167 - accuracy: 0.6430 - f1: 0.5878 - precision: 0.7512 - recall: 0.4857 - val_loss: 1.3806 - val_accuracy: 0.4492 - val_f1: 0.4015 - val_precision: 0.5095 - val_recall: 0.3317\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.8775 - accuracy: 0.6601 - f1: 0.6198 - precision: 0.7360 - recall: 0.5371 - val_loss: 1.3873 - val_accuracy: 0.4367 - val_f1: 0.3864 - val_precision: 0.5108 - val_recall: 0.3117\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8710 - accuracy: 0.6612 - f1: 0.6228 - precision: 0.7379 - recall: 0.5437 - val_loss: 1.4202 - val_accuracy: 0.4250 - val_f1: 0.3840 - val_precision: 0.4887 - val_recall: 0.3167\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 0.8623 - accuracy: 0.6662 - f1: 0.6303 - precision: 0.7505 - recall: 0.5468 - val_loss: 1.4889 - val_accuracy: 0.4325 - val_f1: 0.3911 - val_precision: 0.4744 - val_recall: 0.3333\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.8072 - accuracy: 0.6997 - f1: 0.6687 - precision: 0.7651 - recall: 0.5976 - val_loss: 1.5817 - val_accuracy: 0.4425 - val_f1: 0.3879 - val_precision: 0.4497 - val_recall: 0.3417\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.8272 - accuracy: 0.6911 - f1: 0.6620 - precision: 0.7411 - recall: 0.5996 - val_loss: 1.5277 - val_accuracy: 0.4267 - val_f1: 0.3898 - val_precision: 0.4702 - val_recall: 0.3333\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8708 - accuracy: 0.6669 - f1: 0.6336 - precision: 0.7189 - recall: 0.5678 - val_loss: 1.5300 - val_accuracy: 0.4383 - val_f1: 0.4116 - val_precision: 0.4709 - val_recall: 0.3658\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8005 - accuracy: 0.6951 - f1: 0.6779 - precision: 0.7620 - recall: 0.6128 - val_loss: 1.6254 - val_accuracy: 0.4392 - val_f1: 0.4099 - val_precision: 0.4669 - val_recall: 0.3658\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.7963 - accuracy: 0.7021 - f1: 0.6788 - precision: 0.7524 - recall: 0.6200 - val_loss: 1.6753 - val_accuracy: 0.4275 - val_f1: 0.4052 - val_precision: 0.4511 - val_recall: 0.3683\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8113 - accuracy: 0.6948 - f1: 0.6667 - precision: 0.7410 - recall: 0.6071 - val_loss: 1.6232 - val_accuracy: 0.4442 - val_f1: 0.4269 - val_precision: 0.4768 - val_recall: 0.3867\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.7335 - accuracy: 0.7284 - f1: 0.7135 - precision: 0.7827 - recall: 0.6569 - val_loss: 1.5941 - val_accuracy: 0.4433 - val_f1: 0.4207 - val_precision: 0.4713 - val_recall: 0.3800\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.7730 - accuracy: 0.7204 - f1: 0.7069 - precision: 0.7694 - recall: 0.6553 - val_loss: 1.6322 - val_accuracy: 0.4267 - val_f1: 0.4063 - val_precision: 0.4564 - val_recall: 0.3667\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.7382 - accuracy: 0.7233 - f1: 0.7130 - precision: 0.7745 - recall: 0.6618 - val_loss: 1.6543 - val_accuracy: 0.4475 - val_f1: 0.4366 - val_precision: 0.4750 - val_recall: 0.4042\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.7643 - accuracy: 0.7108 - f1: 0.7013 - precision: 0.7608 - recall: 0.6515 - val_loss: 1.6478 - val_accuracy: 0.4575 - val_f1: 0.4420 - val_precision: 0.4796 - val_recall: 0.4100\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.6842 - accuracy: 0.7517 - f1: 0.7361 - precision: 0.7998 - recall: 0.6829 - val_loss: 1.6284 - val_accuracy: 0.4592 - val_f1: 0.4473 - val_precision: 0.4877 - val_recall: 0.4133\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.6170 - accuracy: 0.7859 - f1: 0.7820 - precision: 0.8291 - recall: 0.7403 - val_loss: 1.7453 - val_accuracy: 0.4333 - val_f1: 0.4320 - val_precision: 0.4558 - val_recall: 0.4108\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.5933 - accuracy: 0.7878 - f1: 0.7869 - precision: 0.8288 - recall: 0.7496 - val_loss: 1.8081 - val_accuracy: 0.4308 - val_f1: 0.4160 - val_precision: 0.4438 - val_recall: 0.3917\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 77ms/step - loss: 0.6283 - accuracy: 0.7788 - f1: 0.7729 - precision: 0.8108 - recall: 0.7391 - val_loss: 1.9466 - val_accuracy: 0.4167 - val_f1: 0.4119 - val_precision: 0.4315 - val_recall: 0.3942\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.6694 - accuracy: 0.7568 - f1: 0.7491 - precision: 0.7908 - recall: 0.7125 - val_loss: 1.7497 - val_accuracy: 0.4125 - val_f1: 0.3974 - val_precision: 0.4284 - val_recall: 0.3708\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.6916 - accuracy: 0.7352 - f1: 0.7292 - precision: 0.7812 - recall: 0.6846 - val_loss: 1.7140 - val_accuracy: 0.3558 - val_f1: 0.3217 - val_precision: 0.3802 - val_recall: 0.2792\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8292 - accuracy: 0.6940 - f1: 0.6681 - precision: 0.7286 - recall: 0.6195 - val_loss: 1.6253 - val_accuracy: 0.4267 - val_f1: 0.4033 - val_precision: 0.4387 - val_recall: 0.3733\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.7646 - accuracy: 0.7097 - f1: 0.6982 - precision: 0.7647 - recall: 0.6438 - val_loss: 1.6266 - val_accuracy: 0.4075 - val_f1: 0.3879 - val_precision: 0.4278 - val_recall: 0.3550\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.6202 - accuracy: 0.7686 - f1: 0.7633 - precision: 0.8117 - recall: 0.7213 - val_loss: 1.7922 - val_accuracy: 0.3558 - val_f1: 0.3409 - val_precision: 0.3744 - val_recall: 0.3133\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.6368 - accuracy: 0.7663 - f1: 0.7515 - precision: 0.8014 - recall: 0.7085 - val_loss: 1.7921 - val_accuracy: 0.3900 - val_f1: 0.3821 - val_precision: 0.4075 - val_recall: 0.3600\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.6500 - accuracy: 0.7584 - f1: 0.7505 - precision: 0.7982 - recall: 0.7090 - val_loss: 1.7709 - val_accuracy: 0.4292 - val_f1: 0.4152 - val_precision: 0.4356 - val_recall: 0.3967\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.6621 - accuracy: 0.7511 - f1: 0.7364 - precision: 0.7809 - recall: 0.6974 - val_loss: 1.7347 - val_accuracy: 0.4483 - val_f1: 0.4421 - val_precision: 0.4678 - val_recall: 0.4192\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.5560 - accuracy: 0.7976 - f1: 0.7914 - precision: 0.8232 - recall: 0.7624 - val_loss: 1.9016 - val_accuracy: 0.4450 - val_f1: 0.4409 - val_precision: 0.4614 - val_recall: 0.4225\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.5324 - accuracy: 0.8041 - f1: 0.7981 - precision: 0.8315 - recall: 0.7678 - val_loss: 1.9063 - val_accuracy: 0.4142 - val_f1: 0.4032 - val_precision: 0.4245 - val_recall: 0.3842\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.5995 - accuracy: 0.7735 - f1: 0.7627 - precision: 0.8008 - recall: 0.7284 - val_loss: 1.9459 - val_accuracy: 0.4267 - val_f1: 0.4165 - val_precision: 0.4355 - val_recall: 0.3992\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.6478 - accuracy: 0.7517 - f1: 0.7373 - precision: 0.7742 - recall: 0.7041 - val_loss: 1.9190 - val_accuracy: 0.4342 - val_f1: 0.4327 - val_precision: 0.4552 - val_recall: 0.4125\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.6179 - accuracy: 0.7665 - f1: 0.7574 - precision: 0.7929 - recall: 0.7253 - val_loss: 1.8147 - val_accuracy: 0.4500 - val_f1: 0.4490 - val_precision: 0.4680 - val_recall: 0.4317\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.5413 - accuracy: 0.8046 - f1: 0.7959 - precision: 0.8273 - recall: 0.7672 - val_loss: 2.0332 - val_accuracy: 0.4625 - val_f1: 0.4604 - val_precision: 0.4725 - val_recall: 0.4492\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.4498 - accuracy: 0.8437 - f1: 0.8405 - precision: 0.8603 - recall: 0.8219 - val_loss: 2.0461 - val_accuracy: 0.4808 - val_f1: 0.4769 - val_precision: 0.4926 - val_recall: 0.4625\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.3924 - accuracy: 0.8648 - f1: 0.8600 - precision: 0.8807 - recall: 0.8405 - val_loss: 2.1782 - val_accuracy: 0.4717 - val_f1: 0.4671 - val_precision: 0.4819 - val_recall: 0.4533\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.3982 - accuracy: 0.8640 - f1: 0.8624 - precision: 0.8819 - recall: 0.8439 - val_loss: 2.0402 - val_accuracy: 0.4333 - val_f1: 0.4265 - val_precision: 0.4455 - val_recall: 0.4092\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.4074 - accuracy: 0.8501 - f1: 0.8451 - precision: 0.8661 - recall: 0.8253 - val_loss: 2.0764 - val_accuracy: 0.4550 - val_f1: 0.4545 - val_precision: 0.4712 - val_recall: 0.4392\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.3750 - accuracy: 0.8704 - f1: 0.8706 - precision: 0.8897 - recall: 0.8524 - val_loss: 2.1576 - val_accuracy: 0.4283 - val_f1: 0.4149 - val_precision: 0.4301 - val_recall: 0.4008\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 0.4271 - accuracy: 0.8511 - f1: 0.8527 - precision: 0.8764 - recall: 0.8305 - val_loss: 2.1038 - val_accuracy: 0.4300 - val_f1: 0.4209 - val_precision: 0.4444 - val_recall: 0.4000\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.5222 - accuracy: 0.8128 - f1: 0.8126 - precision: 0.8399 - recall: 0.7873 - val_loss: 2.0588 - val_accuracy: 0.4467 - val_f1: 0.4398 - val_precision: 0.4588 - val_recall: 0.4225\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.4650 - accuracy: 0.8314 - f1: 0.8290 - precision: 0.8562 - recall: 0.8038 - val_loss: 2.0926 - val_accuracy: 0.4467 - val_f1: 0.4428 - val_precision: 0.4593 - val_recall: 0.4275\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.4142 - accuracy: 0.8582 - f1: 0.8538 - precision: 0.8746 - recall: 0.8340 - val_loss: 2.0519 - val_accuracy: 0.4417 - val_f1: 0.4327 - val_precision: 0.4513 - val_recall: 0.4158\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.3909 - accuracy: 0.8683 - f1: 0.8628 - precision: 0.8819 - recall: 0.8447 - val_loss: 2.1766 - val_accuracy: 0.4217 - val_f1: 0.4122 - val_precision: 0.4273 - val_recall: 0.3983\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.3739 - accuracy: 0.8774 - f1: 0.8767 - precision: 0.8914 - recall: 0.8626 - val_loss: 1.7820 - val_accuracy: 0.3950 - val_f1: 0.3686 - val_precision: 0.4142 - val_recall: 0.3325\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.4640 - accuracy: 0.8378 - f1: 0.8361 - precision: 0.8636 - recall: 0.8111 - val_loss: 2.1568 - val_accuracy: 0.3750 - val_f1: 0.3685 - val_precision: 0.3903 - val_recall: 0.3492\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.4854 - accuracy: 0.8314 - f1: 0.8339 - precision: 0.8601 - recall: 0.8097 - val_loss: 2.1432 - val_accuracy: 0.4017 - val_f1: 0.3839 - val_precision: 0.4021 - val_recall: 0.3675\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.4382 - accuracy: 0.8441 - f1: 0.8442 - precision: 0.8669 - recall: 0.8230 - val_loss: 2.0880 - val_accuracy: 0.4183 - val_f1: 0.4092 - val_precision: 0.4257 - val_recall: 0.3942\n",
            "14/14 [==============================] - 0s 22ms/step - loss: 2.0637 - accuracy: 0.4243 - f1: 0.4230 - precision: 0.4428 - recall: 0.4050\n",
            "[2.0636746883392334, 0.4242857098579407, 0.42295628786087036, 0.44284573197364807, 0.4050000309944153]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZqN_SaM3hs9",
        "outputId": "3314f1b6-3e35-4679-ef7b-cf38e56fd0ac"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "# print(history_sg_bi.history)\r\n",
        "# plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()\r\n",
        "# plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.46000000834465027, 0.5699999928474426, 0.2914285659790039]\n",
            "cbow [0.46785715222358704, 0.5492857098579407, 0.3149999976158142]\n",
            "glove [0.772857129573822, 0.7685714364051819, 0.4242857098579407]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}