{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointGen_Sent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointGen_Sent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a56556-9e04-4113-a2cf-613503f80c82"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c892a5f8-0341-4ca0-e1f8-7ca2bb6a1ae9"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['with', 'their', 'faggy', 'colors', 'are', 'nice', 'is', 'ok', 'too', 'even', 'tho', 'some', 'might', 'take', 'offense', 'because', 'words', 'lol'], ['unbelievable', 'takes', '10', 'minutes', 'to', 'get', 'through', 'to', 'then', 'there', 'is', 'a', 'fault', 'and', 'the', 'call', 'hangs', 'up', 'treatcustomersfairly'], ['well', 'i', 'did', 'hear', 'once', 'before', 'that', 'girls', 'are', 'attracted', 'to', 'men', 'that', 'lok', 'like', 'their', 'dad', 'ok', 'hand'], ['agreed', 'so', 'tired', 'of', 'this', 'nonsense', 'soros', 'must', 'be', 'elated'], ['by', 'the', 'way', 'i', 'am', 'wearing', 'the', 'smile', 'you', 'gave', 'me', 'today', 'n', 'you', 'me']]\n",
            "[[1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 200\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "data_tweet = [nltk.pos_tag(x) for x in data_tweet]\r\n",
        "def getsent(word,tag):\r\n",
        "  res=0\r\n",
        "  try:\r\n",
        "    x = swn.senti_synset(word+'.'+tag[0].lower()+'.01')\r\n",
        "    res =  (x.pos_score()-x.neg_score())\r\n",
        "  finally:\r\n",
        "    return res \r\n",
        "data_tweet = [[(i[0],i[1],getsent(i[0],i[1])) for i in x] for x in data_tweet]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][2]<0:\r\n",
        "            tweet[i]=((tweet[i][0]*(-1)),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=2):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][2]<0:\r\n",
        "            tweet[i]=((tweet[i][0]*(-1)),tweet[i][1],tweet[i][2])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 60"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1J7fulgHwl9"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20d5a32-aef0-4f45-cacc-38ebc4c940b1"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(64))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy',f1,precision,recall])\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 11s 116ms/step - loss: 1.3815 - accuracy: 0.3049 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3729 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.3781 - accuracy: 0.3054 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3709 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.3771 - accuracy: 0.3105 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3704 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.3752 - accuracy: 0.3163 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3705 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.3760 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3794 - val_accuracy: 0.2958 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.3710 - accuracy: 0.3174 - f1: 0.0022 - precision: 0.0284 - recall: 0.0011 - val_loss: 1.3717 - val_accuracy: 0.3325 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.3562 - accuracy: 0.3384 - f1: 0.0410 - precision: 0.2712 - recall: 0.0238 - val_loss: 1.3651 - val_accuracy: 0.2942 - val_f1: 0.0275 - val_precision: 0.6111 - val_recall: 0.0142\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.3385 - accuracy: 0.3417 - f1: 0.1040 - precision: 0.5544 - recall: 0.0621 - val_loss: 1.3522 - val_accuracy: 0.3458 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.3348 - accuracy: 0.3578 - f1: 0.1178 - precision: 0.4735 - recall: 0.0731 - val_loss: 1.3720 - val_accuracy: 0.3317 - val_f1: 0.2261 - val_precision: 0.4476 - val_recall: 0.1517\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.3189 - accuracy: 0.3747 - f1: 0.1812 - precision: 0.5795 - recall: 0.1132 - val_loss: 1.3558 - val_accuracy: 0.3192 - val_f1: 0.2107 - val_precision: 0.5208 - val_recall: 0.1325\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.3039 - accuracy: 0.3835 - f1: 0.2046 - precision: 0.5966 - recall: 0.1260 - val_loss: 1.3528 - val_accuracy: 0.3200 - val_f1: 0.1970 - val_precision: 0.5408 - val_recall: 0.1208\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2930 - accuracy: 0.3759 - f1: 0.2114 - precision: 0.6400 - recall: 0.1297 - val_loss: 1.3523 - val_accuracy: 0.3300 - val_f1: 0.2074 - val_precision: 0.5621 - val_recall: 0.1275\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2941 - accuracy: 0.4006 - f1: 0.2094 - precision: 0.6228 - recall: 0.1288 - val_loss: 1.3493 - val_accuracy: 0.3308 - val_f1: 0.1983 - val_precision: 0.5585 - val_recall: 0.1208\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2800 - accuracy: 0.3984 - f1: 0.2425 - precision: 0.6302 - recall: 0.1518 - val_loss: 1.3658 - val_accuracy: 0.3208 - val_f1: 0.2039 - val_precision: 0.5561 - val_recall: 0.1250\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2687 - accuracy: 0.4037 - f1: 0.2710 - precision: 0.6457 - recall: 0.1731 - val_loss: 1.3653 - val_accuracy: 0.3242 - val_f1: 0.2127 - val_precision: 0.5581 - val_recall: 0.1317\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2668 - accuracy: 0.4020 - f1: 0.2679 - precision: 0.6546 - recall: 0.1700 - val_loss: 1.3674 - val_accuracy: 0.3200 - val_f1: 0.2065 - val_precision: 0.5668 - val_recall: 0.1267\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.2590 - accuracy: 0.4105 - f1: 0.2847 - precision: 0.6695 - recall: 0.1827 - val_loss: 1.3812 - val_accuracy: 0.3317 - val_f1: 0.2209 - val_precision: 0.5675 - val_recall: 0.1375\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2525 - accuracy: 0.4034 - f1: 0.2930 - precision: 0.6834 - recall: 0.1887 - val_loss: 1.4043 - val_accuracy: 0.3192 - val_f1: 0.2124 - val_precision: 0.5717 - val_recall: 0.1308\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2706 - accuracy: 0.3964 - f1: 0.2782 - precision: 0.6829 - recall: 0.1775 - val_loss: 1.3894 - val_accuracy: 0.3058 - val_f1: 0.1929 - val_precision: 0.6249 - val_recall: 0.1142\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2961 - accuracy: 0.3871 - f1: 0.2505 - precision: 0.6434 - recall: 0.1615 - val_loss: 1.3829 - val_accuracy: 0.3275 - val_f1: 0.2187 - val_precision: 0.5647 - val_recall: 0.1358\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2566 - accuracy: 0.4080 - f1: 0.2864 - precision: 0.6731 - recall: 0.1846 - val_loss: 1.3535 - val_accuracy: 0.3158 - val_f1: 0.1818 - val_precision: 0.6493 - val_recall: 0.1058\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2598 - accuracy: 0.4059 - f1: 0.2770 - precision: 0.7002 - recall: 0.1792 - val_loss: 1.3693 - val_accuracy: 0.3192 - val_f1: 0.2095 - val_precision: 0.5742 - val_recall: 0.1283\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2541 - accuracy: 0.3982 - f1: 0.2854 - precision: 0.6948 - recall: 0.1835 - val_loss: 1.3569 - val_accuracy: 0.3442 - val_f1: 0.2549 - val_precision: 0.5739 - val_recall: 0.1642\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2384 - accuracy: 0.4099 - f1: 0.3056 - precision: 0.6915 - recall: 0.1988 - val_loss: 1.3475 - val_accuracy: 0.3617 - val_f1: 0.2487 - val_precision: 0.5974 - val_recall: 0.1575\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.2362 - accuracy: 0.4195 - f1: 0.3149 - precision: 0.7006 - recall: 0.2071 - val_loss: 1.3512 - val_accuracy: 0.3450 - val_f1: 0.2142 - val_precision: 0.5969 - val_recall: 0.1308\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2135 - accuracy: 0.4355 - f1: 0.3178 - precision: 0.7272 - recall: 0.2064 - val_loss: 1.3451 - val_accuracy: 0.3625 - val_f1: 0.2446 - val_precision: 0.6102 - val_recall: 0.1533\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2031 - accuracy: 0.4404 - f1: 0.3336 - precision: 0.7342 - recall: 0.2191 - val_loss: 1.3421 - val_accuracy: 0.3658 - val_f1: 0.2471 - val_precision: 0.5904 - val_recall: 0.1567\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.1929 - accuracy: 0.4272 - f1: 0.3464 - precision: 0.7358 - recall: 0.2292 - val_loss: 1.3420 - val_accuracy: 0.3300 - val_f1: 0.2296 - val_precision: 0.6434 - val_recall: 0.1400\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2032 - accuracy: 0.4340 - f1: 0.3368 - precision: 0.7373 - recall: 0.2211 - val_loss: 1.3543 - val_accuracy: 0.3408 - val_f1: 0.2246 - val_precision: 0.6384 - val_recall: 0.1367\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.2123 - accuracy: 0.4250 - f1: 0.3307 - precision: 0.7280 - recall: 0.2166 - val_loss: 1.3276 - val_accuracy: 0.3342 - val_f1: 0.2198 - val_precision: 0.6755 - val_recall: 0.1317\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 100ms/step - loss: 1.2056 - accuracy: 0.4285 - f1: 0.3229 - precision: 0.7421 - recall: 0.2088 - val_loss: 1.3644 - val_accuracy: 0.3283 - val_f1: 0.2203 - val_precision: 0.6225 - val_recall: 0.1342\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 1.1936 - accuracy: 0.4170 - f1: 0.3256 - precision: 0.7656 - recall: 0.2088 - val_loss: 1.3597 - val_accuracy: 0.3483 - val_f1: 0.2431 - val_precision: 0.6062 - val_recall: 0.1525\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1765 - accuracy: 0.4389 - f1: 0.3344 - precision: 0.7704 - recall: 0.2156 - val_loss: 1.3639 - val_accuracy: 0.3550 - val_f1: 0.2686 - val_precision: 0.5769 - val_recall: 0.1758\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1740 - accuracy: 0.4486 - f1: 0.3375 - precision: 0.7549 - recall: 0.2192 - val_loss: 1.3648 - val_accuracy: 0.3575 - val_f1: 0.2531 - val_precision: 0.6244 - val_recall: 0.1592\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1872 - accuracy: 0.4327 - f1: 0.3401 - precision: 0.7470 - recall: 0.2223 - val_loss: 1.3517 - val_accuracy: 0.3617 - val_f1: 0.2638 - val_precision: 0.6391 - val_recall: 0.1667\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.1710 - accuracy: 0.4389 - f1: 0.3475 - precision: 0.7622 - recall: 0.2265 - val_loss: 1.3795 - val_accuracy: 0.3650 - val_f1: 0.2498 - val_precision: 0.6132 - val_recall: 0.1575\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.1638 - accuracy: 0.4495 - f1: 0.3383 - precision: 0.7720 - recall: 0.2192 - val_loss: 1.3565 - val_accuracy: 0.3792 - val_f1: 0.2711 - val_precision: 0.6237 - val_recall: 0.1742\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.1598 - accuracy: 0.4458 - f1: 0.3475 - precision: 0.7860 - recall: 0.2258 - val_loss: 1.3563 - val_accuracy: 0.3775 - val_f1: 0.2725 - val_precision: 0.6195 - val_recall: 0.1758\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.1650 - accuracy: 0.4563 - f1: 0.3478 - precision: 0.7845 - recall: 0.2262 - val_loss: 1.3581 - val_accuracy: 0.3708 - val_f1: 0.2627 - val_precision: 0.6152 - val_recall: 0.1675\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.1675 - accuracy: 0.4464 - f1: 0.3445 - precision: 0.7768 - recall: 0.2242 - val_loss: 1.3445 - val_accuracy: 0.3550 - val_f1: 0.2583 - val_precision: 0.6319 - val_recall: 0.1633\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.1496 - accuracy: 0.4594 - f1: 0.3550 - precision: 0.7882 - recall: 0.2315 - val_loss: 1.3231 - val_accuracy: 0.3692 - val_f1: 0.2654 - val_precision: 0.6614 - val_recall: 0.1667\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1372 - accuracy: 0.4624 - f1: 0.3629 - precision: 0.7999 - recall: 0.2367 - val_loss: 1.3399 - val_accuracy: 0.3383 - val_f1: 0.2152 - val_precision: 0.6990 - val_recall: 0.1275\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1644 - accuracy: 0.4285 - f1: 0.3233 - precision: 0.7965 - recall: 0.2049 - val_loss: 1.3387 - val_accuracy: 0.3500 - val_f1: 0.2380 - val_precision: 0.6892 - val_recall: 0.1442\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1199 - accuracy: 0.4639 - f1: 0.3579 - precision: 0.8403 - recall: 0.2296 - val_loss: 1.3542 - val_accuracy: 0.3500 - val_f1: 0.2493 - val_precision: 0.6743 - val_recall: 0.1533\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1042 - accuracy: 0.4692 - f1: 0.3634 - precision: 0.8569 - recall: 0.2328 - val_loss: 1.3319 - val_accuracy: 0.3675 - val_f1: 0.2443 - val_precision: 0.6687 - val_recall: 0.1500\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.1010 - accuracy: 0.4745 - f1: 0.3667 - precision: 0.8582 - recall: 0.2354 - val_loss: 1.3706 - val_accuracy: 0.3550 - val_f1: 0.2497 - val_precision: 0.6670 - val_recall: 0.1542\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1025 - accuracy: 0.4674 - f1: 0.3647 - precision: 0.8517 - recall: 0.2340 - val_loss: 1.3729 - val_accuracy: 0.3617 - val_f1: 0.2473 - val_precision: 0.6480 - val_recall: 0.1533\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1011 - accuracy: 0.4780 - f1: 0.3704 - precision: 0.8473 - recall: 0.2388 - val_loss: 1.3791 - val_accuracy: 0.3542 - val_f1: 0.2446 - val_precision: 0.6286 - val_recall: 0.1525\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.0913 - accuracy: 0.4831 - f1: 0.3795 - precision: 0.8591 - recall: 0.2456 - val_loss: 1.3525 - val_accuracy: 0.3675 - val_f1: 0.2514 - val_precision: 0.6489 - val_recall: 0.1567\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.1021 - accuracy: 0.4647 - f1: 0.3710 - precision: 0.8355 - recall: 0.2402 - val_loss: 1.3618 - val_accuracy: 0.3842 - val_f1: 0.2554 - val_precision: 0.6575 - val_recall: 0.1592\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.0840 - accuracy: 0.4686 - f1: 0.3770 - precision: 0.8628 - recall: 0.2428 - val_loss: 1.3683 - val_accuracy: 0.3658 - val_f1: 0.2499 - val_precision: 0.6820 - val_recall: 0.1533\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0664 - accuracy: 0.4786 - f1: 0.3843 - precision: 0.8755 - recall: 0.2477 - val_loss: 1.3275 - val_accuracy: 0.3875 - val_f1: 0.2681 - val_precision: 0.7032 - val_recall: 0.1658\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0569 - accuracy: 0.4978 - f1: 0.3876 - precision: 0.8828 - recall: 0.2496 - val_loss: 1.3762 - val_accuracy: 0.3742 - val_f1: 0.2513 - val_precision: 0.6682 - val_recall: 0.1550\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.0596 - accuracy: 0.4902 - f1: 0.3865 - precision: 0.8881 - recall: 0.2487 - val_loss: 1.3643 - val_accuracy: 0.3867 - val_f1: 0.2896 - val_precision: 0.6389 - val_recall: 0.1875\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.0365 - accuracy: 0.5079 - f1: 0.3970 - precision: 0.8902 - recall: 0.2576 - val_loss: 1.3553 - val_accuracy: 0.3758 - val_f1: 0.2540 - val_precision: 0.6913 - val_recall: 0.1558\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0349 - accuracy: 0.4858 - f1: 0.3942 - precision: 0.8885 - recall: 0.2553 - val_loss: 1.3509 - val_accuracy: 0.3808 - val_f1: 0.2484 - val_precision: 0.6967 - val_recall: 0.1517\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.0451 - accuracy: 0.5099 - f1: 0.3830 - precision: 0.9058 - recall: 0.2452 - val_loss: 1.3805 - val_accuracy: 0.3733 - val_f1: 0.2888 - val_precision: 0.6519 - val_recall: 0.1858\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0282 - accuracy: 0.5156 - f1: 0.3888 - precision: 0.9012 - recall: 0.2498 - val_loss: 1.3989 - val_accuracy: 0.4017 - val_f1: 0.2992 - val_precision: 0.6281 - val_recall: 0.1967\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.9942 - accuracy: 0.5335 - f1: 0.4177 - precision: 0.8743 - recall: 0.2765 - val_loss: 1.4148 - val_accuracy: 0.3958 - val_f1: 0.3046 - val_precision: 0.6538 - val_recall: 0.1992\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.9868 - accuracy: 0.5457 - f1: 0.4391 - precision: 0.8626 - recall: 0.2970 - val_loss: 1.3603 - val_accuracy: 0.4058 - val_f1: 0.3314 - val_precision: 0.5912 - val_recall: 0.2308\n",
            "14/14 [==============================] - 1s 73ms/step - loss: 1.3381 - accuracy: 0.4157 - f1: 0.3354 - precision: 0.6067 - recall: 0.2321\n",
            "[1.3381487131118774, 0.415714293718338, 0.33538082242012024, 0.6066572070121765, 0.2321428507566452]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHtKjzc1SA8",
        "outputId": "d94174f2-f5f3-4cb0-b749-c2ee8d6d20c9"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 113ms/step - loss: 1.3818 - accuracy: 0.2951 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3680 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.3662 - accuracy: 0.3185 - f1: 4.7054e-05 - precision: 0.0024 - recall: 2.3762e-05 - val_loss: 1.3524 - val_accuracy: 0.3550 - val_f1: 0.0033 - val_precision: 0.1667 - val_recall: 0.0017\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.3291 - accuracy: 0.3789 - f1: 0.0264 - precision: 0.4032 - recall: 0.0141 - val_loss: 1.3323 - val_accuracy: 0.3692 - val_f1: 0.1332 - val_precision: 0.5163 - val_recall: 0.0767\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 1.2756 - accuracy: 0.4046 - f1: 0.1501 - precision: 0.6199 - recall: 0.0872 - val_loss: 1.3059 - val_accuracy: 0.3883 - val_f1: 0.2207 - val_precision: 0.5655 - val_recall: 0.1375\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2213 - accuracy: 0.4532 - f1: 0.2320 - precision: 0.6420 - recall: 0.1437 - val_loss: 1.2998 - val_accuracy: 0.4067 - val_f1: 0.2512 - val_precision: 0.5798 - val_recall: 0.1608\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.1741 - accuracy: 0.4946 - f1: 0.2953 - precision: 0.6646 - recall: 0.1915 - val_loss: 1.3146 - val_accuracy: 0.4083 - val_f1: 0.2954 - val_precision: 0.5245 - val_recall: 0.2058\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 1.1358 - accuracy: 0.5142 - f1: 0.3728 - precision: 0.6677 - recall: 0.2605 - val_loss: 1.2980 - val_accuracy: 0.4250 - val_f1: 0.3414 - val_precision: 0.5290 - val_recall: 0.2525\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.1117 - accuracy: 0.5395 - f1: 0.3953 - precision: 0.6591 - recall: 0.2847 - val_loss: 1.3150 - val_accuracy: 0.4167 - val_f1: 0.3499 - val_precision: 0.5196 - val_recall: 0.2642\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.0768 - accuracy: 0.5502 - f1: 0.4514 - precision: 0.6711 - recall: 0.3412 - val_loss: 1.3111 - val_accuracy: 0.4350 - val_f1: 0.3660 - val_precision: 0.5185 - val_recall: 0.2833\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.0363 - accuracy: 0.5825 - f1: 0.4849 - precision: 0.7004 - recall: 0.3729 - val_loss: 1.3167 - val_accuracy: 0.4492 - val_f1: 0.3867 - val_precision: 0.5314 - val_recall: 0.3050\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.9947 - accuracy: 0.6061 - f1: 0.5193 - precision: 0.7298 - recall: 0.4052 - val_loss: 1.3437 - val_accuracy: 0.4475 - val_f1: 0.4055 - val_precision: 0.5186 - val_recall: 0.3333\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9832 - accuracy: 0.5989 - f1: 0.5457 - precision: 0.7255 - recall: 0.4394 - val_loss: 1.3280 - val_accuracy: 0.4558 - val_f1: 0.4187 - val_precision: 0.5268 - val_recall: 0.3483\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.9477 - accuracy: 0.6268 - f1: 0.5740 - precision: 0.7487 - recall: 0.4680 - val_loss: 1.3720 - val_accuracy: 0.4617 - val_f1: 0.4238 - val_precision: 0.5279 - val_recall: 0.3550\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9360 - accuracy: 0.6375 - f1: 0.5789 - precision: 0.7427 - recall: 0.4760 - val_loss: 1.3600 - val_accuracy: 0.4517 - val_f1: 0.4145 - val_precision: 0.5199 - val_recall: 0.3450\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9332 - accuracy: 0.6309 - f1: 0.5824 - precision: 0.7240 - recall: 0.4888 - val_loss: 1.3688 - val_accuracy: 0.4525 - val_f1: 0.4286 - val_precision: 0.5153 - val_recall: 0.3675\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.8531 - accuracy: 0.6631 - f1: 0.6321 - precision: 0.7708 - recall: 0.5368 - val_loss: 1.4111 - val_accuracy: 0.4675 - val_f1: 0.4415 - val_precision: 0.5246 - val_recall: 0.3817\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.8167 - accuracy: 0.6902 - f1: 0.6523 - precision: 0.7821 - recall: 0.5604 - val_loss: 1.3820 - val_accuracy: 0.4683 - val_f1: 0.4376 - val_precision: 0.5278 - val_recall: 0.3742\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.7815 - accuracy: 0.6966 - f1: 0.6656 - precision: 0.7901 - recall: 0.5757 - val_loss: 1.4375 - val_accuracy: 0.4800 - val_f1: 0.4605 - val_precision: 0.5390 - val_recall: 0.4025\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.7384 - accuracy: 0.7292 - f1: 0.7030 - precision: 0.8091 - recall: 0.6222 - val_loss: 1.5110 - val_accuracy: 0.4742 - val_f1: 0.4511 - val_precision: 0.5151 - val_recall: 0.4017\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.7259 - accuracy: 0.7260 - f1: 0.7075 - precision: 0.7983 - recall: 0.6361 - val_loss: 1.5273 - val_accuracy: 0.4492 - val_f1: 0.4236 - val_precision: 0.4842 - val_recall: 0.3767\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.6965 - accuracy: 0.7305 - f1: 0.7188 - precision: 0.8068 - recall: 0.6489 - val_loss: 1.5716 - val_accuracy: 0.4708 - val_f1: 0.4466 - val_precision: 0.5019 - val_recall: 0.4025\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.6586 - accuracy: 0.7542 - f1: 0.7350 - precision: 0.8124 - recall: 0.6720 - val_loss: 1.6225 - val_accuracy: 0.4833 - val_f1: 0.4615 - val_precision: 0.5076 - val_recall: 0.4233\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.7331 - accuracy: 0.7286 - f1: 0.7131 - precision: 0.7944 - recall: 0.6479 - val_loss: 1.4104 - val_accuracy: 0.4958 - val_f1: 0.4712 - val_precision: 0.5388 - val_recall: 0.4192\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.6197 - accuracy: 0.7647 - f1: 0.7551 - precision: 0.8300 - recall: 0.6933 - val_loss: 1.4229 - val_accuracy: 0.4642 - val_f1: 0.4369 - val_precision: 0.5072 - val_recall: 0.3842\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.6836 - accuracy: 0.7386 - f1: 0.7217 - precision: 0.8113 - recall: 0.6507 - val_loss: 1.4465 - val_accuracy: 0.4892 - val_f1: 0.4672 - val_precision: 0.5243 - val_recall: 0.4217\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.5414 - accuracy: 0.7993 - f1: 0.7883 - precision: 0.8509 - recall: 0.7351 - val_loss: 1.5261 - val_accuracy: 0.5042 - val_f1: 0.4827 - val_precision: 0.5281 - val_recall: 0.4450\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 0.5067 - accuracy: 0.8163 - f1: 0.8082 - precision: 0.8612 - recall: 0.7617 - val_loss: 1.6108 - val_accuracy: 0.4933 - val_f1: 0.4789 - val_precision: 0.5210 - val_recall: 0.4433\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.4647 - accuracy: 0.8359 - f1: 0.8326 - precision: 0.8717 - recall: 0.7972 - val_loss: 1.5834 - val_accuracy: 0.5033 - val_f1: 0.4912 - val_precision: 0.5305 - val_recall: 0.4575\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.4535 - accuracy: 0.8384 - f1: 0.8337 - precision: 0.8758 - recall: 0.7958 - val_loss: 1.5635 - val_accuracy: 0.5250 - val_f1: 0.5204 - val_precision: 0.5531 - val_recall: 0.4917\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.4276 - accuracy: 0.8531 - f1: 0.8450 - precision: 0.8841 - recall: 0.8100 - val_loss: 1.7048 - val_accuracy: 0.5325 - val_f1: 0.5175 - val_precision: 0.5488 - val_recall: 0.4900\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.4273 - accuracy: 0.8548 - f1: 0.8503 - precision: 0.8870 - recall: 0.8169 - val_loss: 1.8531 - val_accuracy: 0.5275 - val_f1: 0.5217 - val_precision: 0.5499 - val_recall: 0.4967\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.4111 - accuracy: 0.8485 - f1: 0.8452 - precision: 0.8766 - recall: 0.8162 - val_loss: 1.6178 - val_accuracy: 0.5233 - val_f1: 0.5094 - val_precision: 0.5379 - val_recall: 0.4842\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.3701 - accuracy: 0.8694 - f1: 0.8703 - precision: 0.9003 - recall: 0.8423 - val_loss: 1.6819 - val_accuracy: 0.5317 - val_f1: 0.5307 - val_precision: 0.5573 - val_recall: 0.5067\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 0.3492 - accuracy: 0.8817 - f1: 0.8819 - precision: 0.9084 - recall: 0.8572 - val_loss: 1.7156 - val_accuracy: 0.5467 - val_f1: 0.5403 - val_precision: 0.5667 - val_recall: 0.5167\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.3445 - accuracy: 0.8813 - f1: 0.8799 - precision: 0.9038 - recall: 0.8574 - val_loss: 1.7901 - val_accuracy: 0.5350 - val_f1: 0.5322 - val_precision: 0.5587 - val_recall: 0.5083\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.3166 - accuracy: 0.8877 - f1: 0.8861 - precision: 0.9112 - recall: 0.8624 - val_loss: 1.8910 - val_accuracy: 0.5550 - val_f1: 0.5507 - val_precision: 0.5733 - val_recall: 0.5300\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.2932 - accuracy: 0.8942 - f1: 0.8969 - precision: 0.9149 - recall: 0.8797 - val_loss: 1.9814 - val_accuracy: 0.5450 - val_f1: 0.5418 - val_precision: 0.5611 - val_recall: 0.5242\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.2862 - accuracy: 0.9025 - f1: 0.9009 - precision: 0.9164 - recall: 0.8861 - val_loss: 1.9836 - val_accuracy: 0.5633 - val_f1: 0.5560 - val_precision: 0.5740 - val_recall: 0.5392\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.2577 - accuracy: 0.9106 - f1: 0.9105 - precision: 0.9230 - recall: 0.8984 - val_loss: 1.9728 - val_accuracy: 0.5550 - val_f1: 0.5469 - val_precision: 0.5669 - val_recall: 0.5283\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.3010 - accuracy: 0.8967 - f1: 0.8945 - precision: 0.9080 - recall: 0.8817 - val_loss: 1.8284 - val_accuracy: 0.5758 - val_f1: 0.5766 - val_precision: 0.5972 - val_recall: 0.5575\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.2748 - accuracy: 0.9052 - f1: 0.9051 - precision: 0.9193 - recall: 0.8914 - val_loss: 1.8913 - val_accuracy: 0.5575 - val_f1: 0.5579 - val_precision: 0.5754 - val_recall: 0.5417\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.2664 - accuracy: 0.9118 - f1: 0.9142 - precision: 0.9297 - recall: 0.8994 - val_loss: 1.9663 - val_accuracy: 0.5600 - val_f1: 0.5460 - val_precision: 0.5649 - val_recall: 0.5283\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.2475 - accuracy: 0.9152 - f1: 0.9158 - precision: 0.9276 - recall: 0.9045 - val_loss: 1.9214 - val_accuracy: 0.5700 - val_f1: 0.5672 - val_precision: 0.5875 - val_recall: 0.5483\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.2362 - accuracy: 0.9175 - f1: 0.9158 - precision: 0.9273 - recall: 0.9047 - val_loss: 1.9856 - val_accuracy: 0.5833 - val_f1: 0.5741 - val_precision: 0.5928 - val_recall: 0.5567\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.2182 - accuracy: 0.9293 - f1: 0.9272 - precision: 0.9372 - recall: 0.9176 - val_loss: 1.9937 - val_accuracy: 0.5800 - val_f1: 0.5728 - val_precision: 0.5890 - val_recall: 0.5575\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.2252 - accuracy: 0.9285 - f1: 0.9289 - precision: 0.9391 - recall: 0.9191 - val_loss: 2.0981 - val_accuracy: 0.5750 - val_f1: 0.5704 - val_precision: 0.5868 - val_recall: 0.5550\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.2017 - accuracy: 0.9347 - f1: 0.9337 - precision: 0.9427 - recall: 0.9251 - val_loss: 2.0360 - val_accuracy: 0.5725 - val_f1: 0.5712 - val_precision: 0.5866 - val_recall: 0.5567\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.1818 - accuracy: 0.9403 - f1: 0.9391 - precision: 0.9488 - recall: 0.9296 - val_loss: 2.0696 - val_accuracy: 0.5742 - val_f1: 0.5686 - val_precision: 0.5840 - val_recall: 0.5542\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.1938 - accuracy: 0.9348 - f1: 0.9346 - precision: 0.9442 - recall: 0.9252 - val_loss: 2.2028 - val_accuracy: 0.5708 - val_f1: 0.5720 - val_precision: 0.5854 - val_recall: 0.5592\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.1655 - accuracy: 0.9477 - f1: 0.9478 - precision: 0.9561 - recall: 0.9397 - val_loss: 2.1910 - val_accuracy: 0.5683 - val_f1: 0.5649 - val_precision: 0.5771 - val_recall: 0.5533\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1539 - accuracy: 0.9468 - f1: 0.9471 - precision: 0.9546 - recall: 0.9399 - val_loss: 1.9259 - val_accuracy: 0.5908 - val_f1: 0.5866 - val_precision: 0.5988 - val_recall: 0.5750\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.1502 - accuracy: 0.9507 - f1: 0.9523 - precision: 0.9592 - recall: 0.9455 - val_loss: 2.1716 - val_accuracy: 0.5875 - val_f1: 0.5831 - val_precision: 0.5969 - val_recall: 0.5700\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1585 - accuracy: 0.9495 - f1: 0.9484 - precision: 0.9565 - recall: 0.9404 - val_loss: 2.0428 - val_accuracy: 0.5808 - val_f1: 0.5823 - val_precision: 0.5962 - val_recall: 0.5692\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 0.1574 - accuracy: 0.9514 - f1: 0.9520 - precision: 0.9578 - recall: 0.9464 - val_loss: 2.0830 - val_accuracy: 0.5892 - val_f1: 0.5835 - val_precision: 0.5968 - val_recall: 0.5708\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1229 - accuracy: 0.9619 - f1: 0.9620 - precision: 0.9658 - recall: 0.9581 - val_loss: 2.1067 - val_accuracy: 0.5742 - val_f1: 0.5730 - val_precision: 0.5877 - val_recall: 0.5592\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1261 - accuracy: 0.9578 - f1: 0.9567 - precision: 0.9615 - recall: 0.9520 - val_loss: 2.0979 - val_accuracy: 0.5825 - val_f1: 0.5787 - val_precision: 0.5895 - val_recall: 0.5683\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1297 - accuracy: 0.9609 - f1: 0.9583 - precision: 0.9638 - recall: 0.9529 - val_loss: 2.3244 - val_accuracy: 0.5842 - val_f1: 0.5874 - val_precision: 0.5994 - val_recall: 0.5758\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1344 - accuracy: 0.9573 - f1: 0.9567 - precision: 0.9609 - recall: 0.9526 - val_loss: 2.0617 - val_accuracy: 0.5800 - val_f1: 0.5803 - val_precision: 0.5920 - val_recall: 0.5692\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1484 - accuracy: 0.9514 - f1: 0.9523 - precision: 0.9575 - recall: 0.9472 - val_loss: 2.1852 - val_accuracy: 0.5825 - val_f1: 0.5812 - val_precision: 0.5928 - val_recall: 0.5700\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.1350 - accuracy: 0.9519 - f1: 0.9519 - precision: 0.9561 - recall: 0.9478 - val_loss: 2.3052 - val_accuracy: 0.5908 - val_f1: 0.5945 - val_precision: 0.6081 - val_recall: 0.5817\n",
            "14/14 [==============================] - 1s 72ms/step - loss: 2.3752 - accuracy: 0.6000 - f1: 0.5973 - precision: 0.6071 - recall: 0.5879\n",
            "[2.3751580715179443, 0.6000000238418579, 0.5972691178321838, 0.6070738434791565, 0.587857186794281]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opfhhYPH1VoL",
        "outputId": "362f22b7-47cb-40e8-d6c1-509c0644d5a7"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 7s 127ms/step - loss: 1.3904 - accuracy: 0.2576 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3726 - val_accuracy: 0.3150 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3778 - accuracy: 0.2952 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3894 - val_accuracy: 0.3058 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3629 - accuracy: 0.3186 - f1: 6.2563e-04 - precision: 0.0134 - recall: 3.2156e-04 - val_loss: 1.3459 - val_accuracy: 0.3617 - val_f1: 0.0033 - val_precision: 0.1667 - val_recall: 0.0017\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.3359 - accuracy: 0.3626 - f1: 0.0198 - precision: 0.3192 - recall: 0.0103 - val_loss: 1.3646 - val_accuracy: 0.3100 - val_f1: 0.0065 - val_precision: 0.1667 - val_recall: 0.0033\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.3082 - accuracy: 0.3885 - f1: 0.0591 - precision: 0.4511 - recall: 0.0321 - val_loss: 1.3290 - val_accuracy: 0.3500 - val_f1: 0.0321 - val_precision: 0.5988 - val_recall: 0.0167\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2870 - accuracy: 0.4165 - f1: 0.1138 - precision: 0.5497 - recall: 0.0669 - val_loss: 1.3426 - val_accuracy: 0.3483 - val_f1: 0.0846 - val_precision: 0.4128 - val_recall: 0.0475\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2865 - accuracy: 0.4078 - f1: 0.1530 - precision: 0.5281 - recall: 0.0942 - val_loss: 1.3292 - val_accuracy: 0.3742 - val_f1: 0.1369 - val_precision: 0.5672 - val_recall: 0.0783\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2600 - accuracy: 0.4430 - f1: 0.1379 - precision: 0.5519 - recall: 0.0812 - val_loss: 1.3328 - val_accuracy: 0.3758 - val_f1: 0.1423 - val_precision: 0.4506 - val_recall: 0.0850\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2206 - accuracy: 0.4611 - f1: 0.2373 - precision: 0.5973 - recall: 0.1523 - val_loss: 1.3241 - val_accuracy: 0.3908 - val_f1: 0.1493 - val_precision: 0.4609 - val_recall: 0.0900\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.2005 - accuracy: 0.4787 - f1: 0.2706 - precision: 0.5976 - recall: 0.1782 - val_loss: 1.3299 - val_accuracy: 0.3842 - val_f1: 0.1837 - val_precision: 0.4544 - val_recall: 0.1158\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2192 - accuracy: 0.4554 - f1: 0.2864 - precision: 0.5789 - recall: 0.1958 - val_loss: 1.3412 - val_accuracy: 0.3875 - val_f1: 0.1305 - val_precision: 0.4579 - val_recall: 0.0767\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2752 - accuracy: 0.4252 - f1: 0.1992 - precision: 0.5285 - recall: 0.1368 - val_loss: 1.3605 - val_accuracy: 0.3708 - val_f1: 0.1852 - val_precision: 0.4133 - val_recall: 0.1200\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.2408 - accuracy: 0.4545 - f1: 0.2541 - precision: 0.5471 - recall: 0.1677 - val_loss: 1.3562 - val_accuracy: 0.3692 - val_f1: 0.1498 - val_precision: 0.4198 - val_recall: 0.0917\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2439 - accuracy: 0.4380 - f1: 0.2157 - precision: 0.5642 - recall: 0.1401 - val_loss: 1.3425 - val_accuracy: 0.4017 - val_f1: 0.1970 - val_precision: 0.4609 - val_recall: 0.1258\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2280 - accuracy: 0.4545 - f1: 0.2735 - precision: 0.5904 - recall: 0.1798 - val_loss: 1.3662 - val_accuracy: 0.3700 - val_f1: 0.2257 - val_precision: 0.4230 - val_recall: 0.1542\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.2196 - accuracy: 0.4623 - f1: 0.2919 - precision: 0.5713 - recall: 0.2000 - val_loss: 1.3517 - val_accuracy: 0.3858 - val_f1: 0.2233 - val_precision: 0.4566 - val_recall: 0.1483\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2512 - accuracy: 0.4377 - f1: 0.2293 - precision: 0.5629 - recall: 0.1528 - val_loss: 1.3438 - val_accuracy: 0.3792 - val_f1: 0.2034 - val_precision: 0.4401 - val_recall: 0.1325\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2129 - accuracy: 0.4710 - f1: 0.2806 - precision: 0.5891 - recall: 0.1861 - val_loss: 1.3816 - val_accuracy: 0.3833 - val_f1: 0.2683 - val_precision: 0.4284 - val_recall: 0.1958\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.2303 - accuracy: 0.4611 - f1: 0.2897 - precision: 0.5600 - recall: 0.2019 - val_loss: 1.4298 - val_accuracy: 0.3667 - val_f1: 0.2564 - val_precision: 0.3823 - val_recall: 0.1933\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.1979 - accuracy: 0.4800 - f1: 0.3338 - precision: 0.5888 - recall: 0.2363 - val_loss: 1.3792 - val_accuracy: 0.3792 - val_f1: 0.2787 - val_precision: 0.4252 - val_recall: 0.2083\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1757 - accuracy: 0.4906 - f1: 0.3510 - precision: 0.6179 - recall: 0.2488 - val_loss: 1.3978 - val_accuracy: 0.3917 - val_f1: 0.2724 - val_precision: 0.4288 - val_recall: 0.2000\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.2187 - accuracy: 0.4718 - f1: 0.3307 - precision: 0.5684 - recall: 0.2366 - val_loss: 1.4060 - val_accuracy: 0.3908 - val_f1: 0.2655 - val_precision: 0.4031 - val_recall: 0.1983\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2029 - accuracy: 0.4712 - f1: 0.3336 - precision: 0.5720 - recall: 0.2370 - val_loss: 1.4675 - val_accuracy: 0.3350 - val_f1: 0.2486 - val_precision: 0.3665 - val_recall: 0.1883\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1607 - accuracy: 0.4870 - f1: 0.3868 - precision: 0.6232 - recall: 0.2825 - val_loss: 1.4982 - val_accuracy: 0.3125 - val_f1: 0.2444 - val_precision: 0.3579 - val_recall: 0.1858\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1707 - accuracy: 0.4888 - f1: 0.3913 - precision: 0.6173 - recall: 0.2888 - val_loss: 1.5458 - val_accuracy: 0.2800 - val_f1: 0.2061 - val_precision: 0.3273 - val_recall: 0.1508\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.1854 - accuracy: 0.4920 - f1: 0.3551 - precision: 0.5956 - recall: 0.2550 - val_loss: 1.5921 - val_accuracy: 0.2650 - val_f1: 0.1856 - val_precision: 0.2914 - val_recall: 0.1367\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2273 - accuracy: 0.4462 - f1: 0.3033 - precision: 0.5667 - recall: 0.2094 - val_loss: 1.4904 - val_accuracy: 0.3025 - val_f1: 0.1941 - val_precision: 0.3521 - val_recall: 0.1342\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.1936 - accuracy: 0.4671 - f1: 0.3308 - precision: 0.6010 - recall: 0.2291 - val_loss: 1.4233 - val_accuracy: 0.3425 - val_f1: 0.2299 - val_precision: 0.3921 - val_recall: 0.1633\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.1679 - accuracy: 0.4873 - f1: 0.3692 - precision: 0.6342 - recall: 0.2622 - val_loss: 1.4521 - val_accuracy: 0.3383 - val_f1: 0.2175 - val_precision: 0.3712 - val_recall: 0.1542\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.2397 - accuracy: 0.4371 - f1: 0.2937 - precision: 0.5410 - recall: 0.2036 - val_loss: 1.3945 - val_accuracy: 0.3633 - val_f1: 0.2310 - val_precision: 0.4189 - val_recall: 0.1600\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2058 - accuracy: 0.4686 - f1: 0.3111 - precision: 0.6401 - recall: 0.2073 - val_loss: 1.3981 - val_accuracy: 0.3758 - val_f1: 0.2526 - val_precision: 0.4344 - val_recall: 0.1792\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2066 - accuracy: 0.4641 - f1: 0.3155 - precision: 0.6016 - recall: 0.2158 - val_loss: 1.4784 - val_accuracy: 0.3333 - val_f1: 0.2346 - val_precision: 0.4115 - val_recall: 0.1650\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.1997 - accuracy: 0.4652 - f1: 0.3367 - precision: 0.6148 - recall: 0.2348 - val_loss: 1.5521 - val_accuracy: 0.2967 - val_f1: 0.1904 - val_precision: 0.3263 - val_recall: 0.1350\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.2187 - accuracy: 0.4641 - f1: 0.3184 - precision: 0.5843 - recall: 0.2216 - val_loss: 1.5331 - val_accuracy: 0.3008 - val_f1: 0.2026 - val_precision: 0.3298 - val_recall: 0.1467\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.1672 - accuracy: 0.4827 - f1: 0.3654 - precision: 0.6121 - recall: 0.2613 - val_loss: 1.4733 - val_accuracy: 0.3308 - val_f1: 0.2192 - val_precision: 0.3946 - val_recall: 0.1525\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 1.2162 - accuracy: 0.4446 - f1: 0.3136 - precision: 0.5966 - recall: 0.2144 - val_loss: 1.4977 - val_accuracy: 0.3242 - val_f1: 0.2135 - val_precision: 0.3753 - val_recall: 0.1500\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.2490 - accuracy: 0.4457 - f1: 0.2785 - precision: 0.5600 - recall: 0.1870 - val_loss: 1.4503 - val_accuracy: 0.3192 - val_f1: 0.2041 - val_precision: 0.3687 - val_recall: 0.1417\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.1469 - accuracy: 0.5115 - f1: 0.3799 - precision: 0.6527 - recall: 0.2694 - val_loss: 1.5361 - val_accuracy: 0.2933 - val_f1: 0.1794 - val_precision: 0.3108 - val_recall: 0.1267\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.1635 - accuracy: 0.4998 - f1: 0.3677 - precision: 0.6033 - recall: 0.2654 - val_loss: 1.5427 - val_accuracy: 0.2983 - val_f1: 0.1948 - val_precision: 0.3325 - val_recall: 0.1383\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 1.2177 - accuracy: 0.4605 - f1: 0.3306 - precision: 0.5771 - recall: 0.2333 - val_loss: 1.5035 - val_accuracy: 0.3133 - val_f1: 0.2380 - val_precision: 0.3819 - val_recall: 0.1733\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.1454 - accuracy: 0.5041 - f1: 0.3998 - precision: 0.6345 - recall: 0.2927 - val_loss: 1.5131 - val_accuracy: 0.3258 - val_f1: 0.1845 - val_precision: 0.3290 - val_recall: 0.1292\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.1488 - accuracy: 0.5141 - f1: 0.3982 - precision: 0.6316 - recall: 0.2920 - val_loss: 1.4507 - val_accuracy: 0.3383 - val_f1: 0.1917 - val_precision: 0.3770 - val_recall: 0.1292\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.1268 - accuracy: 0.5299 - f1: 0.4083 - precision: 0.6315 - recall: 0.3038 - val_loss: 1.4013 - val_accuracy: 0.3633 - val_f1: 0.2393 - val_precision: 0.4379 - val_recall: 0.1650\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.1134 - accuracy: 0.5302 - f1: 0.4217 - precision: 0.6453 - recall: 0.3146 - val_loss: 1.4028 - val_accuracy: 0.3575 - val_f1: 0.2259 - val_precision: 0.4238 - val_recall: 0.1542\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.0961 - accuracy: 0.5251 - f1: 0.4281 - precision: 0.6653 - recall: 0.3180 - val_loss: 1.4294 - val_accuracy: 0.3667 - val_f1: 0.2450 - val_precision: 0.4157 - val_recall: 0.1742\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.0374 - accuracy: 0.5714 - f1: 0.4647 - precision: 0.6958 - recall: 0.3512 - val_loss: 1.4167 - val_accuracy: 0.3667 - val_f1: 0.2553 - val_precision: 0.4243 - val_recall: 0.1833\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0187 - accuracy: 0.5847 - f1: 0.4871 - precision: 0.7071 - recall: 0.3754 - val_loss: 1.4652 - val_accuracy: 0.3375 - val_f1: 0.2413 - val_precision: 0.4105 - val_recall: 0.1717\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.0516 - accuracy: 0.5528 - f1: 0.4686 - precision: 0.6784 - recall: 0.3606 - val_loss: 1.5015 - val_accuracy: 0.3417 - val_f1: 0.2380 - val_precision: 0.3675 - val_recall: 0.1767\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0561 - accuracy: 0.5582 - f1: 0.4810 - precision: 0.6821 - recall: 0.3736 - val_loss: 1.5293 - val_accuracy: 0.3508 - val_f1: 0.2776 - val_precision: 0.3843 - val_recall: 0.2175\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0123 - accuracy: 0.5783 - f1: 0.5090 - precision: 0.6738 - recall: 0.4106 - val_loss: 1.5462 - val_accuracy: 0.3517 - val_f1: 0.2845 - val_precision: 0.3809 - val_recall: 0.2275\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.9688 - accuracy: 0.5888 - f1: 0.5297 - precision: 0.6946 - recall: 0.4292 - val_loss: 1.5781 - val_accuracy: 0.3500 - val_f1: 0.2867 - val_precision: 0.3815 - val_recall: 0.2300\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.9654 - accuracy: 0.5988 - f1: 0.5526 - precision: 0.6993 - recall: 0.4576 - val_loss: 1.5721 - val_accuracy: 0.3425 - val_f1: 0.2833 - val_precision: 0.3887 - val_recall: 0.2233\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.0511 - accuracy: 0.5642 - f1: 0.4879 - precision: 0.6408 - recall: 0.3949 - val_loss: 1.5161 - val_accuracy: 0.3467 - val_f1: 0.2799 - val_precision: 0.4018 - val_recall: 0.2150\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.9598 - accuracy: 0.5983 - f1: 0.5377 - precision: 0.7052 - recall: 0.4358 - val_loss: 1.5306 - val_accuracy: 0.3633 - val_f1: 0.3043 - val_precision: 0.4043 - val_recall: 0.2442\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.8857 - accuracy: 0.6364 - f1: 0.6000 - precision: 0.7354 - recall: 0.5078 - val_loss: 1.5619 - val_accuracy: 0.3442 - val_f1: 0.3094 - val_precision: 0.4044 - val_recall: 0.2508\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 0.8408 - accuracy: 0.6669 - f1: 0.6311 - precision: 0.7477 - recall: 0.5468 - val_loss: 1.6306 - val_accuracy: 0.3508 - val_f1: 0.3097 - val_precision: 0.3838 - val_recall: 0.2600\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.7713 - accuracy: 0.6974 - f1: 0.6705 - precision: 0.7734 - recall: 0.5923 - val_loss: 1.6800 - val_accuracy: 0.3583 - val_f1: 0.3166 - val_precision: 0.3784 - val_recall: 0.2725\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.7679 - accuracy: 0.7088 - f1: 0.6773 - precision: 0.7730 - recall: 0.6033 - val_loss: 1.7186 - val_accuracy: 0.3650 - val_f1: 0.3299 - val_precision: 0.3832 - val_recall: 0.2900\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.8003 - accuracy: 0.6883 - f1: 0.6666 - precision: 0.7658 - recall: 0.5914 - val_loss: 1.7438 - val_accuracy: 0.3783 - val_f1: 0.3429 - val_precision: 0.4005 - val_recall: 0.3000\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.8452 - accuracy: 0.6573 - f1: 0.6337 - precision: 0.7262 - recall: 0.5629 - val_loss: 1.7027 - val_accuracy: 0.3533 - val_f1: 0.3071 - val_precision: 0.3649 - val_recall: 0.2658\n",
            "14/14 [==============================] - 1s 71ms/step - loss: 1.6702 - accuracy: 0.3679 - f1: 0.3190 - precision: 0.3766 - recall: 0.2771\n",
            "[1.6702134609222412, 0.3678571283817291, 0.3189525604248047, 0.37656909227371216, 0.27714285254478455]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YumD1M9Y1Xxr",
        "outputId": "eb825df8-49f8-410f-db0c-08836bace4b9"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(64))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 6s 106ms/step - loss: 1.3815 - accuracy: 0.2993 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3727 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.3786 - accuracy: 0.3054 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3703 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.3767 - accuracy: 0.3105 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3702 - val_accuracy: 0.3333 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.3753 - accuracy: 0.3163 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3699 - val_accuracy: 0.3350 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.3764 - accuracy: 0.3129 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3690 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.3760 - accuracy: 0.3119 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3716 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.3751 - accuracy: 0.3144 - f1: 1.4730e-04 - precision: 0.0074 - recall: 7.4389e-05 - val_loss: 1.3714 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.3778 - accuracy: 0.3111 - f1: 1.6007e-04 - precision: 0.0081 - recall: 8.0836e-05 - val_loss: 1.3692 - val_accuracy: 0.3375 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.3742 - accuracy: 0.3141 - f1: 0.0010 - precision: 0.0129 - recall: 5.8407e-04 - val_loss: 1.3570 - val_accuracy: 0.3283 - val_f1: 0.0130 - val_precision: 0.3472 - val_recall: 0.0067\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.3514 - accuracy: 0.3258 - f1: 0.0769 - precision: 0.5933 - recall: 0.0428 - val_loss: 1.3607 - val_accuracy: 0.3275 - val_f1: 0.1671 - val_precision: 0.4959 - val_recall: 0.1008\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.3218 - accuracy: 0.3574 - f1: 0.1670 - precision: 0.6434 - recall: 0.0982 - val_loss: 1.3433 - val_accuracy: 0.3350 - val_f1: 0.1365 - val_precision: 0.5905 - val_recall: 0.0775\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.3053 - accuracy: 0.3433 - f1: 0.1972 - precision: 0.7077 - recall: 0.1169 - val_loss: 1.3401 - val_accuracy: 0.3225 - val_f1: 0.1620 - val_precision: 0.5548 - val_recall: 0.0950\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 1.3000 - accuracy: 0.3638 - f1: 0.2039 - precision: 0.7021 - recall: 0.1223 - val_loss: 1.3341 - val_accuracy: 0.3458 - val_f1: 0.1244 - val_precision: 0.6382 - val_recall: 0.0692\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2941 - accuracy: 0.3546 - f1: 0.1860 - precision: 0.6993 - recall: 0.1104 - val_loss: 1.3419 - val_accuracy: 0.3200 - val_f1: 0.1862 - val_precision: 0.5860 - val_recall: 0.1108\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2785 - accuracy: 0.3754 - f1: 0.2329 - precision: 0.7234 - recall: 0.1413 - val_loss: 1.3580 - val_accuracy: 0.3400 - val_f1: 0.2323 - val_precision: 0.4985 - val_recall: 0.1517\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2804 - accuracy: 0.3928 - f1: 0.2271 - precision: 0.7097 - recall: 0.1394 - val_loss: 1.3519 - val_accuracy: 0.3633 - val_f1: 0.2469 - val_precision: 0.5093 - val_recall: 0.1633\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2693 - accuracy: 0.3665 - f1: 0.2508 - precision: 0.7225 - recall: 0.1554 - val_loss: 1.3400 - val_accuracy: 0.3542 - val_f1: 0.2353 - val_precision: 0.5145 - val_recall: 0.1533\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2637 - accuracy: 0.3873 - f1: 0.2551 - precision: 0.7293 - recall: 0.1579 - val_loss: 1.3484 - val_accuracy: 0.3767 - val_f1: 0.2497 - val_precision: 0.5274 - val_recall: 0.1642\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2638 - accuracy: 0.3971 - f1: 0.2476 - precision: 0.7156 - recall: 0.1548 - val_loss: 1.3293 - val_accuracy: 0.3467 - val_f1: 0.2306 - val_precision: 0.5545 - val_recall: 0.1458\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.2569 - accuracy: 0.3992 - f1: 0.2780 - precision: 0.6949 - recall: 0.1753 - val_loss: 1.3302 - val_accuracy: 0.3817 - val_f1: 0.2520 - val_precision: 0.5458 - val_recall: 0.1642\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2471 - accuracy: 0.4096 - f1: 0.2699 - precision: 0.7254 - recall: 0.1686 - val_loss: 1.3310 - val_accuracy: 0.3858 - val_f1: 0.2736 - val_precision: 0.5414 - val_recall: 0.1833\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2487 - accuracy: 0.4028 - f1: 0.2710 - precision: 0.7329 - recall: 0.1704 - val_loss: 1.3726 - val_accuracy: 0.3642 - val_f1: 0.2746 - val_precision: 0.5090 - val_recall: 0.1883\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.2288 - accuracy: 0.4096 - f1: 0.2921 - precision: 0.7337 - recall: 0.1841 - val_loss: 1.3469 - val_accuracy: 0.3617 - val_f1: 0.2818 - val_precision: 0.5420 - val_recall: 0.1908\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2011 - accuracy: 0.4131 - f1: 0.3079 - precision: 0.7498 - recall: 0.1950 - val_loss: 1.3446 - val_accuracy: 0.3983 - val_f1: 0.2837 - val_precision: 0.5779 - val_recall: 0.1883\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.1666 - accuracy: 0.4392 - f1: 0.3277 - precision: 0.7940 - recall: 0.2076 - val_loss: 1.3343 - val_accuracy: 0.3942 - val_f1: 0.2652 - val_precision: 0.5886 - val_recall: 0.1717\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 1.1473 - accuracy: 0.4781 - f1: 0.3373 - precision: 0.7752 - recall: 0.2175 - val_loss: 1.3255 - val_accuracy: 0.4058 - val_f1: 0.2706 - val_precision: 0.6146 - val_recall: 0.1742\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1330 - accuracy: 0.4774 - f1: 0.3559 - precision: 0.7850 - recall: 0.2325 - val_loss: 1.3398 - val_accuracy: 0.4033 - val_f1: 0.2533 - val_precision: 0.6283 - val_recall: 0.1592\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1113 - accuracy: 0.4846 - f1: 0.3674 - precision: 0.8207 - recall: 0.2392 - val_loss: 1.3489 - val_accuracy: 0.4083 - val_f1: 0.2739 - val_precision: 0.5855 - val_recall: 0.1792\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.1071 - accuracy: 0.4980 - f1: 0.3806 - precision: 0.7913 - recall: 0.2540 - val_loss: 1.3463 - val_accuracy: 0.3917 - val_f1: 0.2653 - val_precision: 0.5917 - val_recall: 0.1717\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 1.0856 - accuracy: 0.5067 - f1: 0.4218 - precision: 0.7947 - recall: 0.2909 - val_loss: 1.3835 - val_accuracy: 0.3933 - val_f1: 0.2746 - val_precision: 0.6155 - val_recall: 0.1775\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.1072 - accuracy: 0.4935 - f1: 0.3993 - precision: 0.7904 - recall: 0.2707 - val_loss: 1.3903 - val_accuracy: 0.4167 - val_f1: 0.3318 - val_precision: 0.5728 - val_recall: 0.2342\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0726 - accuracy: 0.5262 - f1: 0.4301 - precision: 0.7965 - recall: 0.2968 - val_loss: 1.3823 - val_accuracy: 0.4250 - val_f1: 0.3584 - val_precision: 0.5744 - val_recall: 0.2608\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0706 - accuracy: 0.5274 - f1: 0.4350 - precision: 0.7807 - recall: 0.3039 - val_loss: 1.4061 - val_accuracy: 0.4250 - val_f1: 0.3114 - val_precision: 0.5874 - val_recall: 0.2125\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.0413 - accuracy: 0.5484 - f1: 0.4584 - precision: 0.7889 - recall: 0.3249 - val_loss: 1.3576 - val_accuracy: 0.4283 - val_f1: 0.3642 - val_precision: 0.6086 - val_recall: 0.2608\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.0587 - accuracy: 0.5225 - f1: 0.4536 - precision: 0.7919 - recall: 0.3208 - val_loss: 1.3142 - val_accuracy: 0.4192 - val_f1: 0.3389 - val_precision: 0.6361 - val_recall: 0.2317\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0169 - accuracy: 0.5632 - f1: 0.4792 - precision: 0.8067 - recall: 0.3451 - val_loss: 1.3511 - val_accuracy: 0.4517 - val_f1: 0.3848 - val_precision: 0.6097 - val_recall: 0.2817\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 1.0167 - accuracy: 0.5615 - f1: 0.4843 - precision: 0.8060 - recall: 0.3517 - val_loss: 1.3689 - val_accuracy: 0.4425 - val_f1: 0.3905 - val_precision: 0.5659 - val_recall: 0.2983\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 0.9771 - accuracy: 0.5827 - f1: 0.5125 - precision: 0.8180 - recall: 0.3782 - val_loss: 1.3649 - val_accuracy: 0.4283 - val_f1: 0.3571 - val_precision: 0.6316 - val_recall: 0.2500\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.9614 - accuracy: 0.5633 - f1: 0.5073 - precision: 0.8430 - recall: 0.3658 - val_loss: 1.3228 - val_accuracy: 0.4383 - val_f1: 0.3637 - val_precision: 0.6494 - val_recall: 0.2542\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.9356 - accuracy: 0.5927 - f1: 0.5172 - precision: 0.8510 - recall: 0.3748 - val_loss: 1.3530 - val_accuracy: 0.4217 - val_f1: 0.3426 - val_precision: 0.6266 - val_recall: 0.2367\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.9484 - accuracy: 0.5830 - f1: 0.5184 - precision: 0.8491 - recall: 0.3753 - val_loss: 1.3882 - val_accuracy: 0.4258 - val_f1: 0.3616 - val_precision: 0.6179 - val_recall: 0.2567\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.9327 - accuracy: 0.5941 - f1: 0.5241 - precision: 0.8607 - recall: 0.3796 - val_loss: 1.3534 - val_accuracy: 0.4492 - val_f1: 0.3850 - val_precision: 0.5830 - val_recall: 0.2883\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 0.9322 - accuracy: 0.5917 - f1: 0.5225 - precision: 0.8378 - recall: 0.3833 - val_loss: 1.3064 - val_accuracy: 0.4525 - val_f1: 0.3774 - val_precision: 0.6498 - val_recall: 0.2675\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9003 - accuracy: 0.6158 - f1: 0.5349 - precision: 0.8691 - recall: 0.3886 - val_loss: 1.3842 - val_accuracy: 0.4383 - val_f1: 0.3616 - val_precision: 0.5955 - val_recall: 0.2608\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9159 - accuracy: 0.6124 - f1: 0.5292 - precision: 0.8562 - recall: 0.3849 - val_loss: 1.3251 - val_accuracy: 0.4467 - val_f1: 0.3659 - val_precision: 0.6434 - val_recall: 0.2567\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 0.8871 - accuracy: 0.6310 - f1: 0.5395 - precision: 0.8791 - recall: 0.3922 - val_loss: 1.3495 - val_accuracy: 0.4567 - val_f1: 0.3811 - val_precision: 0.6569 - val_recall: 0.2692\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.8655 - accuracy: 0.6298 - f1: 0.5505 - precision: 0.8921 - recall: 0.4006 - val_loss: 1.3864 - val_accuracy: 0.4542 - val_f1: 0.3844 - val_precision: 0.6086 - val_recall: 0.2817\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.8413 - accuracy: 0.6415 - f1: 0.5676 - precision: 0.8882 - recall: 0.4186 - val_loss: 1.3643 - val_accuracy: 0.4458 - val_f1: 0.3884 - val_precision: 0.6587 - val_recall: 0.2767\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.8298 - accuracy: 0.6464 - f1: 0.5761 - precision: 0.9050 - recall: 0.4251 - val_loss: 1.4319 - val_accuracy: 0.4517 - val_f1: 0.3989 - val_precision: 0.6023 - val_recall: 0.2992\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.8196 - accuracy: 0.6480 - f1: 0.5724 - precision: 0.8973 - recall: 0.4219 - val_loss: 1.4279 - val_accuracy: 0.4550 - val_f1: 0.4123 - val_precision: 0.6158 - val_recall: 0.3108\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.7995 - accuracy: 0.6588 - f1: 0.5900 - precision: 0.9081 - recall: 0.4394 - val_loss: 1.4444 - val_accuracy: 0.4558 - val_f1: 0.3937 - val_precision: 0.6058 - val_recall: 0.2925\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.8117 - accuracy: 0.6523 - f1: 0.5840 - precision: 0.8986 - recall: 0.4351 - val_loss: 1.4204 - val_accuracy: 0.4558 - val_f1: 0.4036 - val_precision: 0.6202 - val_recall: 0.3000\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 0.8140 - accuracy: 0.6449 - f1: 0.5829 - precision: 0.9047 - recall: 0.4317 - val_loss: 1.4835 - val_accuracy: 0.4767 - val_f1: 0.4174 - val_precision: 0.5981 - val_recall: 0.3217\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.8014 - accuracy: 0.6567 - f1: 0.5819 - precision: 0.9067 - recall: 0.4307 - val_loss: 1.4321 - val_accuracy: 0.4758 - val_f1: 0.4170 - val_precision: 0.6013 - val_recall: 0.3208\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 0.8223 - accuracy: 0.6592 - f1: 0.5769 - precision: 0.8945 - recall: 0.4274 - val_loss: 1.3881 - val_accuracy: 0.4525 - val_f1: 0.4005 - val_precision: 0.6015 - val_recall: 0.3017\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.8099 - accuracy: 0.6498 - f1: 0.5746 - precision: 0.9036 - recall: 0.4240 - val_loss: 1.4317 - val_accuracy: 0.4708 - val_f1: 0.4160 - val_precision: 0.6059 - val_recall: 0.3183\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 93ms/step - loss: 0.7952 - accuracy: 0.6573 - f1: 0.5777 - precision: 0.9151 - recall: 0.4243 - val_loss: 1.4154 - val_accuracy: 0.4750 - val_f1: 0.4288 - val_precision: 0.6134 - val_recall: 0.3317\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.8033 - accuracy: 0.6614 - f1: 0.5798 - precision: 0.9206 - recall: 0.4260 - val_loss: 1.3587 - val_accuracy: 0.4600 - val_f1: 0.4099 - val_precision: 0.6331 - val_recall: 0.3050\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.8346 - accuracy: 0.6503 - f1: 0.5672 - precision: 0.9155 - recall: 0.4131 - val_loss: 1.4886 - val_accuracy: 0.4758 - val_f1: 0.4364 - val_precision: 0.5746 - val_recall: 0.3542\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.8419 - accuracy: 0.6402 - f1: 0.5650 - precision: 0.9063 - recall: 0.4154 - val_loss: 1.4960 - val_accuracy: 0.4550 - val_f1: 0.4159 - val_precision: 0.5434 - val_recall: 0.3383\n",
            "14/14 [==============================] - 1s 74ms/step - loss: 1.3969 - accuracy: 0.4950 - f1: 0.4397 - precision: 0.5854 - recall: 0.3529\n",
            "[1.3968747854232788, 0.4950000047683716, 0.4397140145301819, 0.585372269153595, 0.35285714268684387]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01yVMiF41ZtT",
        "outputId": "3b650139-8496-406a-dffb-ef4df19657d4"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 113ms/step - loss: 1.3815 - accuracy: 0.3092 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3673 - val_accuracy: 0.3292 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 1.3636 - accuracy: 0.3199 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3494 - val_accuracy: 0.3600 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 1.3251 - accuracy: 0.3727 - f1: 0.0189 - precision: 0.2770 - recall: 0.0102 - val_loss: 1.3157 - val_accuracy: 0.3825 - val_f1: 0.0772 - val_precision: 0.5745 - val_recall: 0.0417\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 1.2714 - accuracy: 0.4213 - f1: 0.1168 - precision: 0.6109 - recall: 0.0656 - val_loss: 1.3026 - val_accuracy: 0.4083 - val_f1: 0.1851 - val_precision: 0.5667 - val_recall: 0.1117\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 1.2249 - accuracy: 0.4436 - f1: 0.2192 - precision: 0.6575 - recall: 0.1333 - val_loss: 1.2811 - val_accuracy: 0.4217 - val_f1: 0.2730 - val_precision: 0.5751 - val_recall: 0.1800\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 99ms/step - loss: 1.1730 - accuracy: 0.4835 - f1: 0.2955 - precision: 0.6645 - recall: 0.1924 - val_loss: 1.3127 - val_accuracy: 0.4100 - val_f1: 0.3082 - val_precision: 0.5390 - val_recall: 0.2167\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.1364 - accuracy: 0.5107 - f1: 0.3483 - precision: 0.6613 - recall: 0.2389 - val_loss: 1.2940 - val_accuracy: 0.4275 - val_f1: 0.3442 - val_precision: 0.5417 - val_recall: 0.2533\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.0983 - accuracy: 0.5375 - f1: 0.4093 - precision: 0.6826 - recall: 0.2948 - val_loss: 1.2923 - val_accuracy: 0.4317 - val_f1: 0.3542 - val_precision: 0.5331 - val_recall: 0.2667\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 1.0649 - accuracy: 0.5590 - f1: 0.4474 - precision: 0.6837 - recall: 0.3351 - val_loss: 1.2918 - val_accuracy: 0.4342 - val_f1: 0.3603 - val_precision: 0.5272 - val_recall: 0.2742\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 1.0352 - accuracy: 0.5721 - f1: 0.4776 - precision: 0.7190 - recall: 0.3602 - val_loss: 1.3189 - val_accuracy: 0.4467 - val_f1: 0.3863 - val_precision: 0.5289 - val_recall: 0.3050\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 1.0087 - accuracy: 0.6006 - f1: 0.4947 - precision: 0.7148 - recall: 0.3803 - val_loss: 1.3120 - val_accuracy: 0.4550 - val_f1: 0.3790 - val_precision: 0.5188 - val_recall: 0.2992\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9877 - accuracy: 0.6126 - f1: 0.5187 - precision: 0.7104 - recall: 0.4109 - val_loss: 1.3128 - val_accuracy: 0.4525 - val_f1: 0.4103 - val_precision: 0.5350 - val_recall: 0.3333\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.9505 - accuracy: 0.6290 - f1: 0.5611 - precision: 0.7310 - recall: 0.4568 - val_loss: 1.3433 - val_accuracy: 0.4550 - val_f1: 0.4199 - val_precision: 0.5313 - val_recall: 0.3475\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.9280 - accuracy: 0.6355 - f1: 0.5831 - precision: 0.7390 - recall: 0.4831 - val_loss: 1.3567 - val_accuracy: 0.4508 - val_f1: 0.4095 - val_precision: 0.4990 - val_recall: 0.3475\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.9142 - accuracy: 0.6439 - f1: 0.5931 - precision: 0.7350 - recall: 0.4984 - val_loss: 1.3889 - val_accuracy: 0.4717 - val_f1: 0.4308 - val_precision: 0.5229 - val_recall: 0.3667\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.8289 - accuracy: 0.6785 - f1: 0.6449 - precision: 0.7715 - recall: 0.5548 - val_loss: 1.4418 - val_accuracy: 0.4683 - val_f1: 0.4422 - val_precision: 0.5204 - val_recall: 0.3850\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.8107 - accuracy: 0.6885 - f1: 0.6552 - precision: 0.7730 - recall: 0.5696 - val_loss: 1.4526 - val_accuracy: 0.4708 - val_f1: 0.4459 - val_precision: 0.5228 - val_recall: 0.3892\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.7787 - accuracy: 0.7055 - f1: 0.6786 - precision: 0.7854 - recall: 0.5979 - val_loss: 1.4342 - val_accuracy: 0.4725 - val_f1: 0.4378 - val_precision: 0.5117 - val_recall: 0.3833\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.7529 - accuracy: 0.7126 - f1: 0.6865 - precision: 0.7854 - recall: 0.6107 - val_loss: 1.4576 - val_accuracy: 0.4708 - val_f1: 0.4418 - val_precision: 0.5159 - val_recall: 0.3867\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.7169 - accuracy: 0.7306 - f1: 0.7078 - precision: 0.7957 - recall: 0.6382 - val_loss: 1.4970 - val_accuracy: 0.4742 - val_f1: 0.4545 - val_precision: 0.5184 - val_recall: 0.4050\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.7002 - accuracy: 0.7341 - f1: 0.7134 - precision: 0.8000 - recall: 0.6441 - val_loss: 1.5337 - val_accuracy: 0.4700 - val_f1: 0.4593 - val_precision: 0.5198 - val_recall: 0.4117\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.6507 - accuracy: 0.7591 - f1: 0.7469 - precision: 0.8165 - recall: 0.6888 - val_loss: 1.5199 - val_accuracy: 0.4808 - val_f1: 0.4613 - val_precision: 0.5237 - val_recall: 0.4125\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.6381 - accuracy: 0.7649 - f1: 0.7447 - precision: 0.8207 - recall: 0.6823 - val_loss: 1.5813 - val_accuracy: 0.4775 - val_f1: 0.4685 - val_precision: 0.5127 - val_recall: 0.4317\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.5748 - accuracy: 0.7842 - f1: 0.7816 - precision: 0.8396 - recall: 0.7317 - val_loss: 1.6316 - val_accuracy: 0.4783 - val_f1: 0.4629 - val_precision: 0.5051 - val_recall: 0.4275\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.5398 - accuracy: 0.8021 - f1: 0.7930 - precision: 0.8490 - recall: 0.7444 - val_loss: 1.7388 - val_accuracy: 0.4808 - val_f1: 0.4693 - val_precision: 0.5087 - val_recall: 0.4358\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.5237 - accuracy: 0.8025 - f1: 0.7988 - precision: 0.8456 - recall: 0.7574 - val_loss: 1.6697 - val_accuracy: 0.4992 - val_f1: 0.4810 - val_precision: 0.5191 - val_recall: 0.4483\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.5707 - accuracy: 0.7865 - f1: 0.7788 - precision: 0.8352 - recall: 0.7299 - val_loss: 1.5417 - val_accuracy: 0.5000 - val_f1: 0.4779 - val_precision: 0.5220 - val_recall: 0.4408\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 0.4890 - accuracy: 0.8151 - f1: 0.8101 - precision: 0.8600 - recall: 0.7660 - val_loss: 1.6366 - val_accuracy: 0.5050 - val_f1: 0.4900 - val_precision: 0.5312 - val_recall: 0.4550\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 94ms/step - loss: 0.4640 - accuracy: 0.8256 - f1: 0.8199 - precision: 0.8640 - recall: 0.7806 - val_loss: 1.6950 - val_accuracy: 0.4908 - val_f1: 0.4837 - val_precision: 0.5189 - val_recall: 0.4533\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.4186 - accuracy: 0.8523 - f1: 0.8452 - precision: 0.8776 - recall: 0.8153 - val_loss: 1.8657 - val_accuracy: 0.5000 - val_f1: 0.4970 - val_precision: 0.5358 - val_recall: 0.4642\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3934 - accuracy: 0.8610 - f1: 0.8555 - precision: 0.8841 - recall: 0.8289 - val_loss: 1.8938 - val_accuracy: 0.5017 - val_f1: 0.5039 - val_precision: 0.5307 - val_recall: 0.4800\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3652 - accuracy: 0.8710 - f1: 0.8698 - precision: 0.8973 - recall: 0.8442 - val_loss: 1.9621 - val_accuracy: 0.5150 - val_f1: 0.5161 - val_precision: 0.5463 - val_recall: 0.4892\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.4043 - accuracy: 0.8532 - f1: 0.8529 - precision: 0.8793 - recall: 0.8283 - val_loss: 1.5364 - val_accuracy: 0.5075 - val_f1: 0.4901 - val_precision: 0.5395 - val_recall: 0.4492\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.4818 - accuracy: 0.8191 - f1: 0.8138 - precision: 0.8525 - recall: 0.7789 - val_loss: 1.7192 - val_accuracy: 0.5275 - val_f1: 0.5255 - val_precision: 0.5524 - val_recall: 0.5017\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3552 - accuracy: 0.8694 - f1: 0.8670 - precision: 0.8946 - recall: 0.8412 - val_loss: 1.8437 - val_accuracy: 0.5117 - val_f1: 0.5117 - val_precision: 0.5385 - val_recall: 0.4875\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3175 - accuracy: 0.8922 - f1: 0.8931 - precision: 0.9103 - recall: 0.8768 - val_loss: 1.8463 - val_accuracy: 0.5258 - val_f1: 0.5177 - val_precision: 0.5429 - val_recall: 0.4950\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3389 - accuracy: 0.8826 - f1: 0.8812 - precision: 0.9022 - recall: 0.8613 - val_loss: 1.8858 - val_accuracy: 0.5425 - val_f1: 0.5385 - val_precision: 0.5634 - val_recall: 0.5158\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3372 - accuracy: 0.8873 - f1: 0.8838 - precision: 0.9048 - recall: 0.8639 - val_loss: 1.9913 - val_accuracy: 0.5275 - val_f1: 0.5220 - val_precision: 0.5462 - val_recall: 0.5000\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.3594 - accuracy: 0.8757 - f1: 0.8747 - precision: 0.8962 - recall: 0.8543 - val_loss: 1.9493 - val_accuracy: 0.5433 - val_f1: 0.5398 - val_precision: 0.5644 - val_recall: 0.5175\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.3135 - accuracy: 0.8943 - f1: 0.8911 - precision: 0.9086 - recall: 0.8743 - val_loss: 1.8573 - val_accuracy: 0.5533 - val_f1: 0.5489 - val_precision: 0.5726 - val_recall: 0.5275\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.2981 - accuracy: 0.8962 - f1: 0.8986 - precision: 0.9163 - recall: 0.8817 - val_loss: 2.0389 - val_accuracy: 0.5433 - val_f1: 0.5394 - val_precision: 0.5585 - val_recall: 0.5217\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.2812 - accuracy: 0.9057 - f1: 0.9049 - precision: 0.9201 - recall: 0.8903 - val_loss: 2.9660 - val_accuracy: 0.5017 - val_f1: 0.4978 - val_precision: 0.5086 - val_recall: 0.4875\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.8110 - accuracy: 0.7481 - f1: 0.7438 - precision: 0.7775 - recall: 0.7144 - val_loss: 1.5145 - val_accuracy: 0.5267 - val_f1: 0.5175 - val_precision: 0.5638 - val_recall: 0.4783\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.4095 - accuracy: 0.8501 - f1: 0.8425 - precision: 0.8799 - recall: 0.8087 - val_loss: 1.6385 - val_accuracy: 0.5425 - val_f1: 0.5304 - val_precision: 0.5632 - val_recall: 0.5017\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.3051 - accuracy: 0.8875 - f1: 0.8886 - precision: 0.9124 - recall: 0.8663 - val_loss: 1.8377 - val_accuracy: 0.5433 - val_f1: 0.5419 - val_precision: 0.5622 - val_recall: 0.5233\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.2677 - accuracy: 0.8964 - f1: 0.8967 - precision: 0.9149 - recall: 0.8794 - val_loss: 1.9749 - val_accuracy: 0.5475 - val_f1: 0.5364 - val_precision: 0.5569 - val_recall: 0.5175\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.2562 - accuracy: 0.9136 - f1: 0.9145 - precision: 0.9310 - recall: 0.8988 - val_loss: 2.0398 - val_accuracy: 0.5533 - val_f1: 0.5497 - val_precision: 0.5674 - val_recall: 0.5333\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.2324 - accuracy: 0.9153 - f1: 0.9132 - precision: 0.9301 - recall: 0.8970 - val_loss: 2.0589 - val_accuracy: 0.5617 - val_f1: 0.5608 - val_precision: 0.5796 - val_recall: 0.5433\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.2146 - accuracy: 0.9194 - f1: 0.9220 - precision: 0.9359 - recall: 0.9086 - val_loss: 2.1443 - val_accuracy: 0.5567 - val_f1: 0.5478 - val_precision: 0.5641 - val_recall: 0.5325\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 95ms/step - loss: 0.1931 - accuracy: 0.9300 - f1: 0.9318 - precision: 0.9399 - recall: 0.9240 - val_loss: 2.1881 - val_accuracy: 0.5550 - val_f1: 0.5538 - val_precision: 0.5676 - val_recall: 0.5408\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.1815 - accuracy: 0.9395 - f1: 0.9372 - precision: 0.9468 - recall: 0.9279 - val_loss: 2.2230 - val_accuracy: 0.5525 - val_f1: 0.5487 - val_precision: 0.5651 - val_recall: 0.5333\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 98ms/step - loss: 0.1801 - accuracy: 0.9334 - f1: 0.9348 - precision: 0.9438 - recall: 0.9259 - val_loss: 2.2894 - val_accuracy: 0.5550 - val_f1: 0.5541 - val_precision: 0.5729 - val_recall: 0.5367\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.1733 - accuracy: 0.9427 - f1: 0.9428 - precision: 0.9514 - recall: 0.9344 - val_loss: 2.3444 - val_accuracy: 0.5467 - val_f1: 0.5479 - val_precision: 0.5637 - val_recall: 0.5333\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.1801 - accuracy: 0.9395 - f1: 0.9399 - precision: 0.9500 - recall: 0.9302 - val_loss: 2.2519 - val_accuracy: 0.5633 - val_f1: 0.5611 - val_precision: 0.5763 - val_recall: 0.5467\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.2012 - accuracy: 0.9226 - f1: 0.9236 - precision: 0.9331 - recall: 0.9143 - val_loss: 2.1992 - val_accuracy: 0.5633 - val_f1: 0.5597 - val_precision: 0.5734 - val_recall: 0.5467\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.1690 - accuracy: 0.9423 - f1: 0.9426 - precision: 0.9497 - recall: 0.9357 - val_loss: 2.3641 - val_accuracy: 0.5717 - val_f1: 0.5683 - val_precision: 0.5825 - val_recall: 0.5550\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.1427 - accuracy: 0.9557 - f1: 0.9514 - precision: 0.9588 - recall: 0.9442 - val_loss: 2.4638 - val_accuracy: 0.5583 - val_f1: 0.5577 - val_precision: 0.5710 - val_recall: 0.5450\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 99ms/step - loss: 0.1293 - accuracy: 0.9581 - f1: 0.9551 - precision: 0.9624 - recall: 0.9481 - val_loss: 2.5180 - val_accuracy: 0.5683 - val_f1: 0.5694 - val_precision: 0.5782 - val_recall: 0.5608\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.1367 - accuracy: 0.9552 - f1: 0.9545 - precision: 0.9604 - recall: 0.9488 - val_loss: 2.4921 - val_accuracy: 0.5742 - val_f1: 0.5743 - val_precision: 0.5857 - val_recall: 0.5633\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.1338 - accuracy: 0.9563 - f1: 0.9547 - precision: 0.9610 - recall: 0.9486 - val_loss: 2.5164 - val_accuracy: 0.5725 - val_f1: 0.5761 - val_precision: 0.5859 - val_recall: 0.5667\n",
            "14/14 [==============================] - 1s 76ms/step - loss: 2.4974 - accuracy: 0.5800 - f1: 0.5791 - precision: 0.5894 - recall: 0.5693\n",
            "[2.497384548187256, 0.5799999833106995, 0.5790888071060181, 0.5893754363059998, 0.5692857503890991]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_G3Ff5r1bSD",
        "outputId": "47fae5fa-85fd-486c-d38b-fb6cb750c8a9"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 7s 129ms/step - loss: 1.3860 - accuracy: 0.2767 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3688 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3794 - accuracy: 0.2949 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3702 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3743 - accuracy: 0.2966 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3659 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3791 - accuracy: 0.2934 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3629 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3706 - accuracy: 0.3123 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3640 - val_accuracy: 0.3275 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3562 - accuracy: 0.3388 - f1: 5.2482e-04 - precision: 0.0265 - recall: 2.6503e-04 - val_loss: 1.3646 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3695 - accuracy: 0.3157 - f1: 0.0053 - precision: 0.1196 - recall: 0.0027 - val_loss: 1.3770 - val_accuracy: 0.3083 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3660 - accuracy: 0.3081 - f1: 0.0045 - precision: 0.0652 - recall: 0.0024 - val_loss: 1.3834 - val_accuracy: 0.3125 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3634 - accuracy: 0.3153 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3735 - val_accuracy: 0.3267 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.3669 - accuracy: 0.3183 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3754 - val_accuracy: 0.3267 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.3641 - accuracy: 0.3170 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3931 - val_accuracy: 0.2850 - val_f1: 0.0016 - val_precision: 0.0417 - val_recall: 8.3333e-04\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3535 - accuracy: 0.3344 - f1: 0.0066 - precision: 0.2600 - recall: 0.0033 - val_loss: 1.3763 - val_accuracy: 0.3150 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3517 - accuracy: 0.3387 - f1: 5.4819e-05 - precision: 0.0014 - recall: 2.7958e-05 - val_loss: 1.3818 - val_accuracy: 0.3033 - val_f1: 0.0260 - val_precision: 0.6667 - val_recall: 0.0133\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3392 - accuracy: 0.3560 - f1: 0.0176 - precision: 0.3618 - recall: 0.0091 - val_loss: 1.4071 - val_accuracy: 0.2875 - val_f1: 0.0081 - val_precision: 0.1833 - val_recall: 0.0042\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3643 - accuracy: 0.3122 - f1: 0.0114 - precision: 0.2757 - recall: 0.0059 - val_loss: 1.3975 - val_accuracy: 0.2992 - val_f1: 0.0016 - val_precision: 0.0208 - val_recall: 8.3333e-04\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3870 - accuracy: 0.3074 - f1: 0.0381 - precision: 0.4191 - recall: 0.0203 - val_loss: 1.3827 - val_accuracy: 0.3200 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3803 - accuracy: 0.2960 - f1: 0.0019 - precision: 0.0881 - recall: 9.4714e-04 - val_loss: 1.3749 - val_accuracy: 0.3192 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3622 - accuracy: 0.3188 - f1: 9.9399e-04 - precision: 0.0495 - recall: 5.0210e-04 - val_loss: 1.3713 - val_accuracy: 0.3233 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3416 - accuracy: 0.3481 - f1: 0.0127 - precision: 0.3618 - recall: 0.0065 - val_loss: 1.3729 - val_accuracy: 0.3292 - val_f1: 0.0098 - val_precision: 0.3333 - val_recall: 0.0050\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3386 - accuracy: 0.3478 - f1: 0.0261 - precision: 0.3876 - recall: 0.0136 - val_loss: 1.3792 - val_accuracy: 0.3200 - val_f1: 0.0566 - val_precision: 0.5595 - val_recall: 0.0300\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3222 - accuracy: 0.3731 - f1: 0.0573 - precision: 0.4447 - recall: 0.0313 - val_loss: 1.3812 - val_accuracy: 0.3092 - val_f1: 0.0370 - val_precision: 0.6556 - val_recall: 0.0192\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3083 - accuracy: 0.3797 - f1: 0.0862 - precision: 0.6103 - recall: 0.0481 - val_loss: 1.3945 - val_accuracy: 0.2942 - val_f1: 0.0225 - val_precision: 0.4083 - val_recall: 0.0117\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3233 - accuracy: 0.3611 - f1: 0.0643 - precision: 0.4876 - recall: 0.0350 - val_loss: 1.4187 - val_accuracy: 0.2875 - val_f1: 0.0402 - val_precision: 0.3417 - val_recall: 0.0217\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3483 - accuracy: 0.3436 - f1: 0.0786 - precision: 0.5104 - recall: 0.0431 - val_loss: 1.4295 - val_accuracy: 0.2508 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3487 - accuracy: 0.3529 - f1: 0.0352 - precision: 0.4050 - recall: 0.0187 - val_loss: 1.3802 - val_accuracy: 0.3117 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3244 - accuracy: 0.3769 - f1: 0.0289 - precision: 0.4137 - recall: 0.0153 - val_loss: 1.3894 - val_accuracy: 0.3150 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3396 - accuracy: 0.3573 - f1: 0.0073 - precision: 0.2808 - recall: 0.0038 - val_loss: 1.3705 - val_accuracy: 0.3367 - val_f1: 0.0049 - val_precision: 0.1389 - val_recall: 0.0025\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3360 - accuracy: 0.3634 - f1: 0.0466 - precision: 0.5321 - recall: 0.0248 - val_loss: 1.3874 - val_accuracy: 0.3058 - val_f1: 0.0209 - val_precision: 0.3681 - val_recall: 0.0108\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3205 - accuracy: 0.3788 - f1: 0.0676 - precision: 0.5416 - recall: 0.0366 - val_loss: 1.3892 - val_accuracy: 0.3025 - val_f1: 0.0226 - val_precision: 0.5079 - val_recall: 0.0117\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3178 - accuracy: 0.3839 - f1: 0.0655 - precision: 0.5021 - recall: 0.0355 - val_loss: 1.3893 - val_accuracy: 0.3042 - val_f1: 0.0240 - val_precision: 0.4444 - val_recall: 0.0125\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.3253 - accuracy: 0.3712 - f1: 0.0612 - precision: 0.5358 - recall: 0.0329 - val_loss: 1.3830 - val_accuracy: 0.2908 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3576 - accuracy: 0.3342 - f1: 0.0019 - precision: 0.0418 - recall: 9.6392e-04 - val_loss: 1.4123 - val_accuracy: 0.2892 - val_f1: 0.0411 - val_precision: 0.4564 - val_recall: 0.0217\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3312 - accuracy: 0.3616 - f1: 0.0472 - precision: 0.4472 - recall: 0.0253 - val_loss: 1.4089 - val_accuracy: 0.2992 - val_f1: 0.0683 - val_precision: 0.4416 - val_recall: 0.0375\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3302 - accuracy: 0.3587 - f1: 0.0681 - precision: 0.4491 - recall: 0.0372 - val_loss: 1.4022 - val_accuracy: 0.2958 - val_f1: 0.0257 - val_precision: 0.4972 - val_recall: 0.0133\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3448 - accuracy: 0.3567 - f1: 0.0211 - precision: 0.3402 - recall: 0.0112 - val_loss: 1.4110 - val_accuracy: 0.2992 - val_f1: 0.0780 - val_precision: 0.4051 - val_recall: 0.0433\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3345 - accuracy: 0.3613 - f1: 0.0481 - precision: 0.4136 - recall: 0.0261 - val_loss: 1.4085 - val_accuracy: 0.3200 - val_f1: 0.1025 - val_precision: 0.4436 - val_recall: 0.0583\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.3520 - accuracy: 0.3612 - f1: 0.0485 - precision: 0.3676 - recall: 0.0272 - val_loss: 1.3819 - val_accuracy: 0.3217 - val_f1: 0.0129 - val_precision: 0.2431 - val_recall: 0.0067\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3360 - accuracy: 0.3528 - f1: 0.0411 - precision: 0.5141 - recall: 0.0217 - val_loss: 1.3842 - val_accuracy: 0.3225 - val_f1: 0.0567 - val_precision: 0.3932 - val_recall: 0.0308\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.3026 - accuracy: 0.3978 - f1: 0.1012 - precision: 0.5664 - recall: 0.0561 - val_loss: 1.3948 - val_accuracy: 0.2983 - val_f1: 0.0996 - val_precision: 0.4191 - val_recall: 0.0567\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.3057 - accuracy: 0.3861 - f1: 0.1090 - precision: 0.4906 - recall: 0.0618 - val_loss: 1.3827 - val_accuracy: 0.3392 - val_f1: 0.1033 - val_precision: 0.4191 - val_recall: 0.0592\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2982 - accuracy: 0.4074 - f1: 0.1320 - precision: 0.5529 - recall: 0.0762 - val_loss: 1.3968 - val_accuracy: 0.3233 - val_f1: 0.0783 - val_precision: 0.4584 - val_recall: 0.0433\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.2835 - accuracy: 0.4144 - f1: 0.1433 - precision: 0.6017 - recall: 0.0823 - val_loss: 1.4032 - val_accuracy: 0.3192 - val_f1: 0.1119 - val_precision: 0.4202 - val_recall: 0.0650\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2929 - accuracy: 0.4081 - f1: 0.1748 - precision: 0.5589 - recall: 0.1043 - val_loss: 1.4283 - val_accuracy: 0.3142 - val_f1: 0.1056 - val_precision: 0.3805 - val_recall: 0.0617\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.3021 - accuracy: 0.4042 - f1: 0.1435 - precision: 0.5426 - recall: 0.0840 - val_loss: 1.4208 - val_accuracy: 0.3092 - val_f1: 0.1055 - val_precision: 0.3638 - val_recall: 0.0625\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.2991 - accuracy: 0.3922 - f1: 0.1756 - precision: 0.5510 - recall: 0.1053 - val_loss: 1.4305 - val_accuracy: 0.2958 - val_f1: 0.0856 - val_precision: 0.3170 - val_recall: 0.0500\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.3178 - accuracy: 0.3906 - f1: 0.1318 - precision: 0.4952 - recall: 0.0776 - val_loss: 1.4192 - val_accuracy: 0.2850 - val_f1: 0.0736 - val_precision: 0.3836 - val_recall: 0.0408\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2802 - accuracy: 0.4257 - f1: 0.1244 - precision: 0.5575 - recall: 0.0709 - val_loss: 1.4173 - val_accuracy: 0.3000 - val_f1: 0.1198 - val_precision: 0.4001 - val_recall: 0.0708\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2561 - accuracy: 0.4357 - f1: 0.2028 - precision: 0.6085 - recall: 0.1229 - val_loss: 1.4461 - val_accuracy: 0.2992 - val_f1: 0.1505 - val_precision: 0.3933 - val_recall: 0.0933\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2631 - accuracy: 0.4274 - f1: 0.2181 - precision: 0.5826 - recall: 0.1354 - val_loss: 1.4263 - val_accuracy: 0.3050 - val_f1: 0.1186 - val_precision: 0.3361 - val_recall: 0.0725\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2604 - accuracy: 0.4217 - f1: 0.1911 - precision: 0.5794 - recall: 0.1154 - val_loss: 1.4050 - val_accuracy: 0.3092 - val_f1: 0.0970 - val_precision: 0.3897 - val_recall: 0.0558\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2451 - accuracy: 0.4279 - f1: 0.2057 - precision: 0.6302 - recall: 0.1240 - val_loss: 1.4255 - val_accuracy: 0.3167 - val_f1: 0.1385 - val_precision: 0.3592 - val_recall: 0.0867\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 1.2225 - accuracy: 0.4458 - f1: 0.2347 - precision: 0.6157 - recall: 0.1460 - val_loss: 1.4397 - val_accuracy: 0.3233 - val_f1: 0.1550 - val_precision: 0.3736 - val_recall: 0.0983\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2310 - accuracy: 0.4444 - f1: 0.2428 - precision: 0.6356 - recall: 0.1512 - val_loss: 1.4658 - val_accuracy: 0.3075 - val_f1: 0.1426 - val_precision: 0.3473 - val_recall: 0.0900\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 120ms/step - loss: 1.2348 - accuracy: 0.4355 - f1: 0.2541 - precision: 0.6276 - recall: 0.1604 - val_loss: 1.5001 - val_accuracy: 0.3200 - val_f1: 0.1663 - val_precision: 0.3270 - val_recall: 0.1117\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.2446 - accuracy: 0.4372 - f1: 0.2534 - precision: 0.5784 - recall: 0.1636 - val_loss: 1.5096 - val_accuracy: 0.3200 - val_f1: 0.1871 - val_precision: 0.3470 - val_recall: 0.1283\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 1.2837 - accuracy: 0.4149 - f1: 0.2194 - precision: 0.5712 - recall: 0.1387 - val_loss: 1.4392 - val_accuracy: 0.3158 - val_f1: 0.1653 - val_precision: 0.3910 - val_recall: 0.1050\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.2785 - accuracy: 0.4244 - f1: 0.1935 - precision: 0.5682 - recall: 0.1175 - val_loss: 1.4227 - val_accuracy: 0.3242 - val_f1: 0.1634 - val_precision: 0.4200 - val_recall: 0.1017\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 1.2377 - accuracy: 0.4497 - f1: 0.2415 - precision: 0.6253 - recall: 0.1510 - val_loss: 1.4386 - val_accuracy: 0.3125 - val_f1: 0.1662 - val_precision: 0.3955 - val_recall: 0.1058\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2296 - accuracy: 0.4468 - f1: 0.2850 - precision: 0.6566 - recall: 0.1834 - val_loss: 1.4664 - val_accuracy: 0.3083 - val_f1: 0.1534 - val_precision: 0.3637 - val_recall: 0.0975\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.2424 - accuracy: 0.4455 - f1: 0.2628 - precision: 0.6227 - recall: 0.1690 - val_loss: 1.4387 - val_accuracy: 0.2933 - val_f1: 0.1401 - val_precision: 0.3953 - val_recall: 0.0858\n",
            "14/14 [==============================] - 1s 75ms/step - loss: 1.4539 - accuracy: 0.2921 - f1: 0.1285 - precision: 0.3580 - recall: 0.0786\n",
            "[1.4538562297821045, 0.2921428680419922, 0.1285005658864975, 0.35804182291030884, 0.07857142388820648]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czoFmEwK1dWT",
        "outputId": "c5bd20b1-011b-4112-e5fa-37f94dda9a65"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(64))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 3s 28ms/step - loss: 1.3791 - accuracy: 0.3058 - f1: 0.0030 - precision: 0.0161 - recall: 0.0018 - val_loss: 1.3006 - val_accuracy: 0.4033 - val_f1: 0.0713 - val_precision: 0.5708 - val_recall: 0.0383\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 1.2855 - accuracy: 0.4097 - f1: 0.1254 - precision: 0.5046 - recall: 0.0755 - val_loss: 1.2154 - val_accuracy: 0.4550 - val_f1: 0.2127 - val_precision: 0.6507 - val_recall: 0.1275\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 1.1762 - accuracy: 0.4515 - f1: 0.2775 - precision: 0.6992 - recall: 0.1757 - val_loss: 1.2088 - val_accuracy: 0.4517 - val_f1: 0.4135 - val_precision: 0.5496 - val_recall: 0.3325\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 1.1296 - accuracy: 0.4787 - f1: 0.3503 - precision: 0.7143 - recall: 0.2418 - val_loss: 1.1154 - val_accuracy: 0.4908 - val_f1: 0.3884 - val_precision: 0.6676 - val_recall: 0.2750\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 1.0667 - accuracy: 0.4966 - f1: 0.3909 - precision: 0.7801 - recall: 0.2691 - val_loss: 1.1072 - val_accuracy: 0.5133 - val_f1: 0.2959 - val_precision: 0.8033 - val_recall: 0.1842\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.9946 - accuracy: 0.5459 - f1: 0.4463 - precision: 0.8370 - recall: 0.3097 - val_loss: 1.1413 - val_accuracy: 0.4750 - val_f1: 0.2885 - val_precision: 0.7938 - val_recall: 0.1775\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.9789 - accuracy: 0.5452 - f1: 0.4445 - precision: 0.8683 - recall: 0.3062 - val_loss: 1.1691 - val_accuracy: 0.3967 - val_f1: 0.2253 - val_precision: 0.9159 - val_recall: 0.1300\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.9889 - accuracy: 0.5303 - f1: 0.4260 - precision: 0.8874 - recall: 0.2871 - val_loss: 1.0508 - val_accuracy: 0.5017 - val_f1: 0.3901 - val_precision: 0.8400 - val_recall: 0.2558\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.9277 - accuracy: 0.5601 - f1: 0.4752 - precision: 0.8858 - recall: 0.3284 - val_loss: 1.0630 - val_accuracy: 0.5100 - val_f1: 0.4085 - val_precision: 0.8004 - val_recall: 0.2758\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.8771 - accuracy: 0.5981 - f1: 0.5051 - precision: 0.9072 - recall: 0.3553 - val_loss: 1.0101 - val_accuracy: 0.5758 - val_f1: 0.4916 - val_precision: 0.7647 - val_recall: 0.3642\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.8192 - accuracy: 0.6268 - f1: 0.5360 - precision: 0.9091 - recall: 0.3841 - val_loss: 1.0282 - val_accuracy: 0.5950 - val_f1: 0.5287 - val_precision: 0.7365 - val_recall: 0.4133\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.7649 - accuracy: 0.6648 - f1: 0.5783 - precision: 0.8870 - recall: 0.4315 - val_loss: 0.9890 - val_accuracy: 0.6083 - val_f1: 0.5347 - val_precision: 0.7639 - val_recall: 0.4125\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.8240 - accuracy: 0.6284 - f1: 0.5603 - precision: 0.8551 - recall: 0.4196 - val_loss: 0.9298 - val_accuracy: 0.6167 - val_f1: 0.5430 - val_precision: 0.7823 - val_recall: 0.4167\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6979 - accuracy: 0.6853 - f1: 0.6477 - precision: 0.8201 - recall: 0.5379 - val_loss: 0.9353 - val_accuracy: 0.6217 - val_f1: 0.5274 - val_precision: 0.7926 - val_recall: 0.3967\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6657 - accuracy: 0.7016 - f1: 0.6304 - precision: 0.8690 - recall: 0.5062 - val_loss: 0.8496 - val_accuracy: 0.6308 - val_f1: 0.5403 - val_precision: 0.8547 - val_recall: 0.3967\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6550 - accuracy: 0.6977 - f1: 0.6441 - precision: 0.8495 - recall: 0.5285 - val_loss: 0.9780 - val_accuracy: 0.6258 - val_f1: 0.5705 - val_precision: 0.7667 - val_recall: 0.4550\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.6119 - accuracy: 0.7053 - f1: 0.6762 - precision: 0.8222 - recall: 0.5771 - val_loss: 0.9198 - val_accuracy: 0.6175 - val_f1: 0.5932 - val_precision: 0.7551 - val_recall: 0.4892\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.5792 - accuracy: 0.7264 - f1: 0.7021 - precision: 0.8212 - recall: 0.6155 - val_loss: 0.8649 - val_accuracy: 0.6300 - val_f1: 0.6043 - val_precision: 0.7728 - val_recall: 0.4967\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.5327 - accuracy: 0.7276 - f1: 0.7143 - precision: 0.8119 - recall: 0.6400 - val_loss: 0.8696 - val_accuracy: 0.6333 - val_f1: 0.6285 - val_precision: 0.7573 - val_recall: 0.5375\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.5214 - accuracy: 0.7382 - f1: 0.7251 - precision: 0.8061 - recall: 0.6600 - val_loss: 0.9488 - val_accuracy: 0.6400 - val_f1: 0.6281 - val_precision: 0.7089 - val_recall: 0.5642\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.4891 - accuracy: 0.7577 - f1: 0.7435 - precision: 0.8070 - recall: 0.6906 - val_loss: 0.9456 - val_accuracy: 0.6400 - val_f1: 0.6231 - val_precision: 0.7362 - val_recall: 0.5408\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.5154 - accuracy: 0.7553 - f1: 0.7409 - precision: 0.8119 - recall: 0.6827 - val_loss: 1.1892 - val_accuracy: 0.5442 - val_f1: 0.5609 - val_precision: 0.6201 - val_recall: 0.5125\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.6519 - accuracy: 0.6901 - f1: 0.6715 - precision: 0.7684 - recall: 0.5997 - val_loss: 0.9794 - val_accuracy: 0.6450 - val_f1: 0.5951 - val_precision: 0.7508 - val_recall: 0.4933\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.5594 - accuracy: 0.7509 - f1: 0.7200 - precision: 0.8387 - recall: 0.6352 - val_loss: 0.9729 - val_accuracy: 0.6608 - val_f1: 0.6508 - val_precision: 0.7141 - val_recall: 0.5983\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.4801 - accuracy: 0.7646 - f1: 0.7524 - precision: 0.8175 - recall: 0.6982 - val_loss: 0.9827 - val_accuracy: 0.6392 - val_f1: 0.6367 - val_precision: 0.7051 - val_recall: 0.5808\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.4357 - accuracy: 0.7953 - f1: 0.7805 - precision: 0.8347 - recall: 0.7347 - val_loss: 0.9906 - val_accuracy: 0.6625 - val_f1: 0.6600 - val_precision: 0.6853 - val_recall: 0.6367\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.4202 - accuracy: 0.8012 - f1: 0.7940 - precision: 0.8148 - recall: 0.7746 - val_loss: 1.0840 - val_accuracy: 0.6492 - val_f1: 0.6480 - val_precision: 0.6718 - val_recall: 0.6258\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.4038 - accuracy: 0.8198 - f1: 0.8132 - precision: 0.8344 - recall: 0.7934 - val_loss: 1.0582 - val_accuracy: 0.6633 - val_f1: 0.6661 - val_precision: 0.6804 - val_recall: 0.6525\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3992 - accuracy: 0.8287 - f1: 0.8271 - precision: 0.8353 - recall: 0.8193 - val_loss: 1.1055 - val_accuracy: 0.6708 - val_f1: 0.6705 - val_precision: 0.6769 - val_recall: 0.6642\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3677 - accuracy: 0.8518 - f1: 0.8526 - precision: 0.8578 - recall: 0.8475 - val_loss: 1.1125 - val_accuracy: 0.6875 - val_f1: 0.6850 - val_precision: 0.6893 - val_recall: 0.6808\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3198 - accuracy: 0.8867 - f1: 0.8852 - precision: 0.8878 - recall: 0.8826 - val_loss: 1.1335 - val_accuracy: 0.6867 - val_f1: 0.6894 - val_precision: 0.6948 - val_recall: 0.6842\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3772 - accuracy: 0.8703 - f1: 0.8719 - precision: 0.8756 - recall: 0.8683 - val_loss: 1.0470 - val_accuracy: 0.6475 - val_f1: 0.6482 - val_precision: 0.6650 - val_recall: 0.6325\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.5212 - accuracy: 0.8241 - f1: 0.8225 - precision: 0.8443 - recall: 0.8020 - val_loss: 1.0059 - val_accuracy: 0.6642 - val_f1: 0.6594 - val_precision: 0.6682 - val_recall: 0.6508\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3627 - accuracy: 0.8809 - f1: 0.8800 - precision: 0.8891 - recall: 0.8712 - val_loss: 1.0616 - val_accuracy: 0.6658 - val_f1: 0.6632 - val_precision: 0.6700 - val_recall: 0.6567\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3427 - accuracy: 0.8847 - f1: 0.8822 - precision: 0.8916 - recall: 0.8731 - val_loss: 1.1293 - val_accuracy: 0.6733 - val_f1: 0.6718 - val_precision: 0.6815 - val_recall: 0.6625\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3366 - accuracy: 0.8859 - f1: 0.8823 - precision: 0.8884 - recall: 0.8763 - val_loss: 1.0957 - val_accuracy: 0.6900 - val_f1: 0.6872 - val_precision: 0.6964 - val_recall: 0.6783\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.2774 - accuracy: 0.9142 - f1: 0.9145 - precision: 0.9185 - recall: 0.9106 - val_loss: 1.0172 - val_accuracy: 0.7067 - val_f1: 0.7052 - val_precision: 0.7148 - val_recall: 0.6958\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.2035 - accuracy: 0.9387 - f1: 0.9373 - precision: 0.9397 - recall: 0.9349 - val_loss: 1.1246 - val_accuracy: 0.6725 - val_f1: 0.6710 - val_precision: 0.6773 - val_recall: 0.6650\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.2276 - accuracy: 0.9279 - f1: 0.9293 - precision: 0.9334 - recall: 0.9254 - val_loss: 1.2187 - val_accuracy: 0.6867 - val_f1: 0.6875 - val_precision: 0.6934 - val_recall: 0.6817\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.2564 - accuracy: 0.9137 - f1: 0.9146 - precision: 0.9178 - recall: 0.9115 - val_loss: 1.1727 - val_accuracy: 0.6942 - val_f1: 0.6902 - val_precision: 0.6938 - val_recall: 0.6867\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2480 - accuracy: 0.9282 - f1: 0.9270 - precision: 0.9285 - recall: 0.9254 - val_loss: 1.1545 - val_accuracy: 0.6875 - val_f1: 0.6882 - val_precision: 0.6923 - val_recall: 0.6842\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2506 - accuracy: 0.9245 - f1: 0.9239 - precision: 0.9259 - recall: 0.9219 - val_loss: 0.9797 - val_accuracy: 0.6675 - val_f1: 0.6678 - val_precision: 0.6786 - val_recall: 0.6575\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2626 - accuracy: 0.9211 - f1: 0.9204 - precision: 0.9249 - recall: 0.9161 - val_loss: 1.0423 - val_accuracy: 0.6900 - val_f1: 0.6942 - val_precision: 0.7082 - val_recall: 0.6808\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2016 - accuracy: 0.9434 - f1: 0.9422 - precision: 0.9440 - recall: 0.9405 - val_loss: 1.0592 - val_accuracy: 0.7025 - val_f1: 0.7049 - val_precision: 0.7125 - val_recall: 0.6975\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1542 - accuracy: 0.9629 - f1: 0.9635 - precision: 0.9648 - recall: 0.9623 - val_loss: 1.0639 - val_accuracy: 0.7225 - val_f1: 0.7240 - val_precision: 0.7323 - val_recall: 0.7158\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1456 - accuracy: 0.9640 - f1: 0.9631 - precision: 0.9640 - recall: 0.9622 - val_loss: 1.1158 - val_accuracy: 0.7208 - val_f1: 0.7238 - val_precision: 0.7293 - val_recall: 0.7183\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1810 - accuracy: 0.9526 - f1: 0.9523 - precision: 0.9540 - recall: 0.9505 - val_loss: 1.1530 - val_accuracy: 0.7033 - val_f1: 0.7008 - val_precision: 0.7068 - val_recall: 0.6950\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1872 - accuracy: 0.9524 - f1: 0.9522 - precision: 0.9546 - recall: 0.9497 - val_loss: 1.1778 - val_accuracy: 0.7100 - val_f1: 0.7084 - val_precision: 0.7119 - val_recall: 0.7050\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1700 - accuracy: 0.9554 - f1: 0.9553 - precision: 0.9566 - recall: 0.9540 - val_loss: 1.2081 - val_accuracy: 0.7108 - val_f1: 0.7093 - val_precision: 0.7128 - val_recall: 0.7058\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1582 - accuracy: 0.9584 - f1: 0.9587 - precision: 0.9601 - recall: 0.9574 - val_loss: 1.2302 - val_accuracy: 0.7225 - val_f1: 0.7189 - val_precision: 0.7221 - val_recall: 0.7158\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1599 - accuracy: 0.9599 - f1: 0.9600 - precision: 0.9615 - recall: 0.9586 - val_loss: 1.1738 - val_accuracy: 0.7192 - val_f1: 0.7208 - val_precision: 0.7267 - val_recall: 0.7150\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.1625 - accuracy: 0.9609 - f1: 0.9618 - precision: 0.9636 - recall: 0.9601 - val_loss: 1.1745 - val_accuracy: 0.7225 - val_f1: 0.7226 - val_precision: 0.7252 - val_recall: 0.7200\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1407 - accuracy: 0.9677 - f1: 0.9673 - precision: 0.9682 - recall: 0.9664 - val_loss: 1.1779 - val_accuracy: 0.7225 - val_f1: 0.7226 - val_precision: 0.7288 - val_recall: 0.7167\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1272 - accuracy: 0.9699 - f1: 0.9700 - precision: 0.9706 - recall: 0.9693 - val_loss: 1.2411 - val_accuracy: 0.7250 - val_f1: 0.7247 - val_precision: 0.7278 - val_recall: 0.7217\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1175 - accuracy: 0.9736 - f1: 0.9738 - precision: 0.9741 - recall: 0.9736 - val_loss: 1.2151 - val_accuracy: 0.7283 - val_f1: 0.7291 - val_precision: 0.7351 - val_recall: 0.7233\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1474 - accuracy: 0.9661 - f1: 0.9660 - precision: 0.9670 - recall: 0.9650 - val_loss: 1.2673 - val_accuracy: 0.7033 - val_f1: 0.7035 - val_precision: 0.7062 - val_recall: 0.7008\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3683 - accuracy: 0.8915 - f1: 0.8920 - precision: 0.8969 - recall: 0.8873 - val_loss: 1.0772 - val_accuracy: 0.7058 - val_f1: 0.7047 - val_precision: 0.7104 - val_recall: 0.6992\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1947 - accuracy: 0.9458 - f1: 0.9466 - precision: 0.9478 - recall: 0.9453 - val_loss: 1.1892 - val_accuracy: 0.7242 - val_f1: 0.7249 - val_precision: 0.7265 - val_recall: 0.7233\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1546 - accuracy: 0.9620 - f1: 0.9625 - precision: 0.9632 - recall: 0.9618 - val_loss: 1.1705 - val_accuracy: 0.7417 - val_f1: 0.7384 - val_precision: 0.7453 - val_recall: 0.7317\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1327 - accuracy: 0.9642 - f1: 0.9652 - precision: 0.9662 - recall: 0.9642 - val_loss: 1.2723 - val_accuracy: 0.7317 - val_f1: 0.7327 - val_precision: 0.7354 - val_recall: 0.7300\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.1944 - accuracy: 0.7493 - f1: 0.7487 - precision: 0.7517 - recall: 0.7457\n",
            "[1.1944488286972046, 0.7492856979370117, 0.7486646771430969, 0.751689612865448, 0.7457142472267151]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKM9PEwm1fkb",
        "outputId": "1a1c805a-f634-40f1-bdbb-17d57ff54d81"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 4s 36ms/step - loss: 1.3799 - accuracy: 0.3138 - f1: 5.2753e-04 - precision: 0.0266 - recall: 2.6640e-04 - val_loss: 1.3241 - val_accuracy: 0.3975 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 1.2742 - accuracy: 0.4140 - f1: 0.0982 - precision: 0.7033 - recall: 0.0562 - val_loss: 1.2156 - val_accuracy: 0.4508 - val_f1: 0.1953 - val_precision: 0.6729 - val_recall: 0.1150\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 1.1381 - accuracy: 0.5068 - f1: 0.3167 - precision: 0.6891 - recall: 0.2089 - val_loss: 1.1012 - val_accuracy: 0.5383 - val_f1: 0.4176 - val_precision: 0.6465 - val_recall: 0.3092\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.9965 - accuracy: 0.5823 - f1: 0.4760 - precision: 0.7114 - recall: 0.3603 - val_loss: 1.0343 - val_accuracy: 0.5625 - val_f1: 0.5142 - val_precision: 0.6441 - val_recall: 0.4283\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.8478 - accuracy: 0.6674 - f1: 0.6210 - precision: 0.7565 - recall: 0.5284 - val_loss: 0.9839 - val_accuracy: 0.6042 - val_f1: 0.5840 - val_precision: 0.6779 - val_recall: 0.5133\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.7161 - accuracy: 0.7322 - f1: 0.7005 - precision: 0.7924 - recall: 0.6284 - val_loss: 0.9521 - val_accuracy: 0.6250 - val_f1: 0.6142 - val_precision: 0.6696 - val_recall: 0.5675\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.5816 - accuracy: 0.7820 - f1: 0.7704 - precision: 0.8382 - recall: 0.7138 - val_loss: 0.8660 - val_accuracy: 0.6583 - val_f1: 0.6561 - val_precision: 0.7007 - val_recall: 0.6175\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.4722 - accuracy: 0.8345 - f1: 0.8262 - precision: 0.8725 - recall: 0.7851 - val_loss: 0.8597 - val_accuracy: 0.6833 - val_f1: 0.6796 - val_precision: 0.7175 - val_recall: 0.6458\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.3910 - accuracy: 0.8600 - f1: 0.8590 - precision: 0.8880 - recall: 0.8320 - val_loss: 0.8729 - val_accuracy: 0.7000 - val_f1: 0.7041 - val_precision: 0.7302 - val_recall: 0.6800\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.3282 - accuracy: 0.8850 - f1: 0.8858 - precision: 0.9036 - recall: 0.8689 - val_loss: 0.8821 - val_accuracy: 0.6850 - val_f1: 0.6816 - val_precision: 0.7204 - val_recall: 0.6475\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.3505 - accuracy: 0.8717 - f1: 0.8715 - precision: 0.8899 - recall: 0.8540 - val_loss: 0.9124 - val_accuracy: 0.6917 - val_f1: 0.6853 - val_precision: 0.7101 - val_recall: 0.6625\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2926 - accuracy: 0.8973 - f1: 0.8979 - precision: 0.9145 - recall: 0.8821 - val_loss: 0.9271 - val_accuracy: 0.6967 - val_f1: 0.6832 - val_precision: 0.7092 - val_recall: 0.6592\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2517 - accuracy: 0.9127 - f1: 0.9103 - precision: 0.9239 - recall: 0.8974 - val_loss: 0.9169 - val_accuracy: 0.7167 - val_f1: 0.7121 - val_precision: 0.7320 - val_recall: 0.6933\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.1896 - accuracy: 0.9370 - f1: 0.9358 - precision: 0.9439 - recall: 0.9279 - val_loss: 1.0248 - val_accuracy: 0.7183 - val_f1: 0.7151 - val_precision: 0.7274 - val_recall: 0.7033\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1896 - accuracy: 0.9304 - f1: 0.9337 - precision: 0.9437 - recall: 0.9239 - val_loss: 1.0142 - val_accuracy: 0.7233 - val_f1: 0.7228 - val_precision: 0.7336 - val_recall: 0.7125\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1583 - accuracy: 0.9519 - f1: 0.9530 - precision: 0.9595 - recall: 0.9468 - val_loss: 1.1199 - val_accuracy: 0.7117 - val_f1: 0.7093 - val_precision: 0.7198 - val_recall: 0.6992\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.1659 - accuracy: 0.9436 - f1: 0.9430 - precision: 0.9483 - recall: 0.9378 - val_loss: 1.1062 - val_accuracy: 0.7133 - val_f1: 0.7150 - val_precision: 0.7236 - val_recall: 0.7067\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.1678 - accuracy: 0.9433 - f1: 0.9430 - precision: 0.9481 - recall: 0.9379 - val_loss: 1.1130 - val_accuracy: 0.7033 - val_f1: 0.7089 - val_precision: 0.7217 - val_recall: 0.6967\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.1678 - accuracy: 0.9423 - f1: 0.9427 - precision: 0.9472 - recall: 0.9384 - val_loss: 0.9217 - val_accuracy: 0.7333 - val_f1: 0.7306 - val_precision: 0.7444 - val_recall: 0.7175\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.1660 - accuracy: 0.9431 - f1: 0.9421 - precision: 0.9469 - recall: 0.9373 - val_loss: 1.1810 - val_accuracy: 0.7158 - val_f1: 0.7179 - val_precision: 0.7270 - val_recall: 0.7092\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1565 - accuracy: 0.9405 - f1: 0.9408 - precision: 0.9465 - recall: 0.9352 - val_loss: 0.9538 - val_accuracy: 0.7450 - val_f1: 0.7484 - val_precision: 0.7587 - val_recall: 0.7383\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1229 - accuracy: 0.9553 - f1: 0.9532 - precision: 0.9567 - recall: 0.9498 - val_loss: 0.9437 - val_accuracy: 0.7458 - val_f1: 0.7475 - val_precision: 0.7561 - val_recall: 0.7392\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0988 - accuracy: 0.9649 - f1: 0.9646 - precision: 0.9683 - recall: 0.9610 - val_loss: 1.1046 - val_accuracy: 0.7417 - val_f1: 0.7404 - val_precision: 0.7468 - val_recall: 0.7342\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1041 - accuracy: 0.9612 - f1: 0.9606 - precision: 0.9637 - recall: 0.9576 - val_loss: 1.1532 - val_accuracy: 0.7267 - val_f1: 0.7270 - val_precision: 0.7316 - val_recall: 0.7225\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1031 - accuracy: 0.9639 - f1: 0.9638 - precision: 0.9665 - recall: 0.9611 - val_loss: 1.1154 - val_accuracy: 0.7400 - val_f1: 0.7371 - val_precision: 0.7427 - val_recall: 0.7317\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0946 - accuracy: 0.9664 - f1: 0.9660 - precision: 0.9676 - recall: 0.9644 - val_loss: 1.1338 - val_accuracy: 0.7350 - val_f1: 0.7350 - val_precision: 0.7410 - val_recall: 0.7292\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0833 - accuracy: 0.9689 - f1: 0.9685 - precision: 0.9699 - recall: 0.9671 - val_loss: 1.0822 - val_accuracy: 0.7375 - val_f1: 0.7379 - val_precision: 0.7442 - val_recall: 0.7317\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0741 - accuracy: 0.9739 - f1: 0.9739 - precision: 0.9750 - recall: 0.9727 - val_loss: 1.1052 - val_accuracy: 0.7283 - val_f1: 0.7278 - val_precision: 0.7332 - val_recall: 0.7225\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0611 - accuracy: 0.9771 - f1: 0.9772 - precision: 0.9774 - recall: 0.9771 - val_loss: 1.1254 - val_accuracy: 0.7292 - val_f1: 0.7304 - val_precision: 0.7376 - val_recall: 0.7233\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0573 - accuracy: 0.9779 - f1: 0.9778 - precision: 0.9784 - recall: 0.9773 - val_loss: 1.1799 - val_accuracy: 0.7333 - val_f1: 0.7310 - val_precision: 0.7397 - val_recall: 0.7225\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0680 - accuracy: 0.9747 - f1: 0.9746 - precision: 0.9751 - recall: 0.9741 - val_loss: 1.2446 - val_accuracy: 0.7233 - val_f1: 0.7217 - val_precision: 0.7294 - val_recall: 0.7142\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0662 - accuracy: 0.9728 - f1: 0.9725 - precision: 0.9732 - recall: 0.9717 - val_loss: 1.3550 - val_accuracy: 0.7333 - val_f1: 0.7331 - val_precision: 0.7372 - val_recall: 0.7292\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0747 - accuracy: 0.9688 - f1: 0.9687 - precision: 0.9688 - recall: 0.9686 - val_loss: 1.2469 - val_accuracy: 0.7317 - val_f1: 0.7307 - val_precision: 0.7357 - val_recall: 0.7258\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0620 - accuracy: 0.9735 - f1: 0.9735 - precision: 0.9737 - recall: 0.9734 - val_loss: 1.2215 - val_accuracy: 0.7350 - val_f1: 0.7349 - val_precision: 0.7408 - val_recall: 0.7292\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0572 - accuracy: 0.9754 - f1: 0.9750 - precision: 0.9755 - recall: 0.9746 - val_loss: 1.3000 - val_accuracy: 0.7275 - val_f1: 0.7298 - val_precision: 0.7356 - val_recall: 0.7242\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0594 - accuracy: 0.9738 - f1: 0.9741 - precision: 0.9746 - recall: 0.9735 - val_loss: 1.3662 - val_accuracy: 0.7183 - val_f1: 0.7212 - val_precision: 0.7293 - val_recall: 0.7133\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0590 - accuracy: 0.9751 - f1: 0.9749 - precision: 0.9756 - recall: 0.9743 - val_loss: 1.3175 - val_accuracy: 0.7300 - val_f1: 0.7283 - val_precision: 0.7317 - val_recall: 0.7250\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0598 - accuracy: 0.9740 - f1: 0.9742 - precision: 0.9744 - recall: 0.9740 - val_loss: 1.3663 - val_accuracy: 0.7208 - val_f1: 0.7178 - val_precision: 0.7224 - val_recall: 0.7133\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0515 - accuracy: 0.9759 - f1: 0.9759 - precision: 0.9760 - recall: 0.9758 - val_loss: 1.4044 - val_accuracy: 0.7192 - val_f1: 0.7194 - val_precision: 0.7248 - val_recall: 0.7142\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0498 - accuracy: 0.9765 - f1: 0.9766 - precision: 0.9767 - recall: 0.9765 - val_loss: 1.3175 - val_accuracy: 0.7367 - val_f1: 0.7368 - val_precision: 0.7421 - val_recall: 0.7317\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0515 - accuracy: 0.9771 - f1: 0.9777 - precision: 0.9783 - recall: 0.9771 - val_loss: 1.3595 - val_accuracy: 0.7225 - val_f1: 0.7211 - val_precision: 0.7248 - val_recall: 0.7175\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0482 - accuracy: 0.9772 - f1: 0.9774 - precision: 0.9778 - recall: 0.9771 - val_loss: 1.3818 - val_accuracy: 0.7158 - val_f1: 0.7139 - val_precision: 0.7188 - val_recall: 0.7092\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0509 - accuracy: 0.9747 - f1: 0.9746 - precision: 0.9752 - recall: 0.9740 - val_loss: 1.3920 - val_accuracy: 0.7267 - val_f1: 0.7287 - val_precision: 0.7324 - val_recall: 0.7250\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0483 - accuracy: 0.9763 - f1: 0.9764 - precision: 0.9775 - recall: 0.9753 - val_loss: 1.4901 - val_accuracy: 0.7175 - val_f1: 0.7199 - val_precision: 0.7232 - val_recall: 0.7167\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0531 - accuracy: 0.9757 - f1: 0.9756 - precision: 0.9760 - recall: 0.9752 - val_loss: 1.4140 - val_accuracy: 0.7425 - val_f1: 0.7437 - val_precision: 0.7474 - val_recall: 0.7400\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 0.0491 - accuracy: 0.9762 - f1: 0.9767 - precision: 0.9775 - recall: 0.9760 - val_loss: 1.4229 - val_accuracy: 0.7367 - val_f1: 0.7351 - val_precision: 0.7385 - val_recall: 0.7317\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0593 - accuracy: 0.9725 - f1: 0.9725 - precision: 0.9731 - recall: 0.9720 - val_loss: 1.4795 - val_accuracy: 0.7350 - val_f1: 0.7367 - val_precision: 0.7402 - val_recall: 0.7333\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0500 - accuracy: 0.9753 - f1: 0.9755 - precision: 0.9763 - recall: 0.9746 - val_loss: 1.4740 - val_accuracy: 0.7433 - val_f1: 0.7447 - val_precision: 0.7479 - val_recall: 0.7417\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0505 - accuracy: 0.9765 - f1: 0.9763 - precision: 0.9772 - recall: 0.9754 - val_loss: 1.4285 - val_accuracy: 0.7375 - val_f1: 0.7382 - val_precision: 0.7423 - val_recall: 0.7342\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0422 - accuracy: 0.9794 - f1: 0.9795 - precision: 0.9800 - recall: 0.9790 - val_loss: 1.6663 - val_accuracy: 0.7133 - val_f1: 0.7146 - val_precision: 0.7184 - val_recall: 0.7108\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0872 - accuracy: 0.9620 - f1: 0.9619 - precision: 0.9620 - recall: 0.9617 - val_loss: 1.1909 - val_accuracy: 0.7508 - val_f1: 0.7513 - val_precision: 0.7535 - val_recall: 0.7492\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0609 - accuracy: 0.9734 - f1: 0.9729 - precision: 0.9735 - recall: 0.9723 - val_loss: 1.2997 - val_accuracy: 0.7492 - val_f1: 0.7511 - val_precision: 0.7565 - val_recall: 0.7458\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0770 - accuracy: 0.9698 - f1: 0.9695 - precision: 0.9702 - recall: 0.9688 - val_loss: 1.3421 - val_accuracy: 0.7317 - val_f1: 0.7306 - val_precision: 0.7346 - val_recall: 0.7267\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0458 - accuracy: 0.9769 - f1: 0.9770 - precision: 0.9770 - recall: 0.9769 - val_loss: 1.4611 - val_accuracy: 0.7442 - val_f1: 0.7462 - val_precision: 0.7490 - val_recall: 0.7433\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0436 - accuracy: 0.9807 - f1: 0.9803 - precision: 0.9808 - recall: 0.9799 - val_loss: 1.5263 - val_accuracy: 0.7400 - val_f1: 0.7386 - val_precision: 0.7423 - val_recall: 0.7350\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0500 - accuracy: 0.9780 - f1: 0.9777 - precision: 0.9781 - recall: 0.9773 - val_loss: 1.4132 - val_accuracy: 0.7567 - val_f1: 0.7527 - val_precision: 0.7562 - val_recall: 0.7492\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0444 - accuracy: 0.9827 - f1: 0.9827 - precision: 0.9830 - recall: 0.9824 - val_loss: 1.5220 - val_accuracy: 0.7417 - val_f1: 0.7393 - val_precision: 0.7437 - val_recall: 0.7350\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0462 - accuracy: 0.9766 - f1: 0.9764 - precision: 0.9769 - recall: 0.9759 - val_loss: 1.4402 - val_accuracy: 0.7408 - val_f1: 0.7405 - val_precision: 0.7427 - val_recall: 0.7383\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 21ms/step - loss: 0.0394 - accuracy: 0.9820 - f1: 0.9822 - precision: 0.9827 - recall: 0.9817 - val_loss: 1.4578 - val_accuracy: 0.7592 - val_f1: 0.7617 - val_precision: 0.7643 - val_recall: 0.7592\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0414 - accuracy: 0.9772 - f1: 0.9775 - precision: 0.9786 - recall: 0.9765 - val_loss: 1.4747 - val_accuracy: 0.7525 - val_f1: 0.7526 - val_precision: 0.7562 - val_recall: 0.7492\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3109 - accuracy: 0.7650 - f1: 0.7660 - precision: 0.7684 - recall: 0.7636\n",
            "[1.3108680248260498, 0.7649999856948853, 0.7659761309623718, 0.7684273719787598, 0.7635713815689087]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQBa5s4s1hNU",
        "outputId": "7da3bb5a-c01d-4067-8dac-dbca96b11552"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 4s 59ms/step - loss: 1.3902 - accuracy: 0.2604 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3675 - val_accuracy: 0.3275 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 1.3613 - accuracy: 0.3247 - f1: 0.0065 - precision: 0.1370 - recall: 0.0035 - val_loss: 1.3189 - val_accuracy: 0.3908 - val_f1: 0.0259 - val_precision: 0.5833 - val_recall: 0.0133\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 1.3204 - accuracy: 0.3843 - f1: 0.0893 - precision: 0.4581 - recall: 0.0528 - val_loss: 1.2896 - val_accuracy: 0.4108 - val_f1: 0.1245 - val_precision: 0.5282 - val_recall: 0.0708\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 1.2748 - accuracy: 0.4141 - f1: 0.1557 - precision: 0.5864 - recall: 0.0964 - val_loss: 1.3052 - val_accuracy: 0.3908 - val_f1: 0.0131 - val_precision: 0.4167 - val_recall: 0.0067\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 1.2581 - accuracy: 0.4082 - f1: 0.1649 - precision: 0.6770 - recall: 0.0977 - val_loss: 1.2744 - val_accuracy: 0.4183 - val_f1: 0.1923 - val_precision: 0.5529 - val_recall: 0.1175\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 1.2206 - accuracy: 0.4388 - f1: 0.2303 - precision: 0.6823 - recall: 0.1411 - val_loss: 1.2696 - val_accuracy: 0.4108 - val_f1: 0.1660 - val_precision: 0.6338 - val_recall: 0.0967\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 1.2716 - accuracy: 0.4141 - f1: 0.1892 - precision: 0.6706 - recall: 0.1169 - val_loss: 1.2943 - val_accuracy: 0.4083 - val_f1: 0.2120 - val_precision: 0.4935 - val_recall: 0.1358\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 1.2372 - accuracy: 0.4306 - f1: 0.2293 - precision: 0.6195 - recall: 0.1449 - val_loss: 1.2918 - val_accuracy: 0.4142 - val_f1: 0.2176 - val_precision: 0.5202 - val_recall: 0.1383\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 3s 58ms/step - loss: 1.1882 - accuracy: 0.4423 - f1: 0.2629 - precision: 0.7155 - recall: 0.1640 - val_loss: 1.3120 - val_accuracy: 0.4225 - val_f1: 0.2103 - val_precision: 0.5196 - val_recall: 0.1325\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 1.1764 - accuracy: 0.4693 - f1: 0.2595 - precision: 0.7236 - recall: 0.1609 - val_loss: 1.3344 - val_accuracy: 0.4125 - val_f1: 0.2179 - val_precision: 0.5108 - val_recall: 0.1392\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 3s 54ms/step - loss: 1.1809 - accuracy: 0.4487 - f1: 0.2577 - precision: 0.7602 - recall: 0.1570 - val_loss: 1.2994 - val_accuracy: 0.4250 - val_f1: 0.2116 - val_precision: 0.4976 - val_recall: 0.1350\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 1.1904 - accuracy: 0.4649 - f1: 0.2444 - precision: 0.7428 - recall: 0.1513 - val_loss: 1.2920 - val_accuracy: 0.4175 - val_f1: 0.2125 - val_precision: 0.5088 - val_recall: 0.1350\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 1.2068 - accuracy: 0.4628 - f1: 0.2485 - precision: 0.7364 - recall: 0.1535 - val_loss: 1.3041 - val_accuracy: 0.4217 - val_f1: 0.2155 - val_precision: 0.4935 - val_recall: 0.1383\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 1.2721 - accuracy: 0.4194 - f1: 0.2069 - precision: 0.4458 - recall: 0.1385 - val_loss: 1.3202 - val_accuracy: 0.3683 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 1.2759 - accuracy: 0.3964 - f1: 0.0445 - precision: 0.3677 - recall: 0.0246 - val_loss: 1.3219 - val_accuracy: 0.4083 - val_f1: 0.2185 - val_precision: 0.4934 - val_recall: 0.1408\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 1.1861 - accuracy: 0.4533 - f1: 0.2580 - precision: 0.6959 - recall: 0.1615 - val_loss: 1.2925 - val_accuracy: 0.4142 - val_f1: 0.2218 - val_precision: 0.5531 - val_recall: 0.1392\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 1.2073 - accuracy: 0.4478 - f1: 0.2303 - precision: 0.6713 - recall: 0.1420 - val_loss: 1.3151 - val_accuracy: 0.3967 - val_f1: 0.2471 - val_precision: 0.4747 - val_recall: 0.1675\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 2s 52ms/step - loss: 1.1759 - accuracy: 0.4763 - f1: 0.2921 - precision: 0.6736 - recall: 0.1890 - val_loss: 1.3327 - val_accuracy: 0.4075 - val_f1: 0.2759 - val_precision: 0.4691 - val_recall: 0.1958\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 1.1674 - accuracy: 0.4838 - f1: 0.3300 - precision: 0.6600 - recall: 0.2247 - val_loss: 1.3377 - val_accuracy: 0.4075 - val_f1: 0.2751 - val_precision: 0.4841 - val_recall: 0.1925\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 1.1299 - accuracy: 0.4999 - f1: 0.3458 - precision: 0.6899 - recall: 0.2356 - val_loss: 1.2822 - val_accuracy: 0.4142 - val_f1: 0.2289 - val_precision: 0.5695 - val_recall: 0.1442\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 3s 63ms/step - loss: 1.1352 - accuracy: 0.5038 - f1: 0.3371 - precision: 0.6882 - recall: 0.2308 - val_loss: 1.3192 - val_accuracy: 0.4133 - val_f1: 0.2750 - val_precision: 0.4965 - val_recall: 0.1908\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 1.0999 - accuracy: 0.5304 - f1: 0.3933 - precision: 0.7005 - recall: 0.2756 - val_loss: 1.3544 - val_accuracy: 0.3875 - val_f1: 0.2891 - val_precision: 0.4839 - val_recall: 0.2067\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 1.0638 - accuracy: 0.5525 - f1: 0.4278 - precision: 0.6897 - recall: 0.3133 - val_loss: 1.3489 - val_accuracy: 0.3850 - val_f1: 0.2951 - val_precision: 0.4820 - val_recall: 0.2133\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 1.0326 - accuracy: 0.5684 - f1: 0.4609 - precision: 0.7097 - recall: 0.3447 - val_loss: 1.3625 - val_accuracy: 0.4033 - val_f1: 0.3079 - val_precision: 0.4748 - val_recall: 0.2283\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 3s 61ms/step - loss: 1.0693 - accuracy: 0.5416 - f1: 0.4243 - precision: 0.6917 - recall: 0.3218 - val_loss: 1.3368 - val_accuracy: 0.3867 - val_f1: 0.2409 - val_precision: 0.5263 - val_recall: 0.1567\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 1.1095 - accuracy: 0.5369 - f1: 0.3903 - precision: 0.6688 - recall: 0.2805 - val_loss: 1.3437 - val_accuracy: 0.3767 - val_f1: 0.2368 - val_precision: 0.5115 - val_recall: 0.1550\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 1.0485 - accuracy: 0.5655 - f1: 0.4423 - precision: 0.7038 - recall: 0.3263 - val_loss: 1.3677 - val_accuracy: 0.3883 - val_f1: 0.2929 - val_precision: 0.4816 - val_recall: 0.2108\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.9949 - accuracy: 0.5862 - f1: 0.5045 - precision: 0.7126 - recall: 0.3953 - val_loss: 1.4418 - val_accuracy: 0.3725 - val_f1: 0.3052 - val_precision: 0.4261 - val_recall: 0.2383\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 1.0358 - accuracy: 0.5711 - f1: 0.4944 - precision: 0.6913 - recall: 0.3909 - val_loss: 1.4009 - val_accuracy: 0.3633 - val_f1: 0.2418 - val_precision: 0.4556 - val_recall: 0.1658\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 1.1271 - accuracy: 0.5200 - f1: 0.4128 - precision: 0.6864 - recall: 0.3067 - val_loss: 1.3700 - val_accuracy: 0.3808 - val_f1: 0.2877 - val_precision: 0.4776 - val_recall: 0.2067\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 2s 51ms/step - loss: 1.0085 - accuracy: 0.5891 - f1: 0.4917 - precision: 0.7078 - recall: 0.3816 - val_loss: 1.4065 - val_accuracy: 0.3975 - val_f1: 0.3244 - val_precision: 0.4544 - val_recall: 0.2525\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.9633 - accuracy: 0.6105 - f1: 0.5348 - precision: 0.7194 - recall: 0.4287 - val_loss: 1.3957 - val_accuracy: 0.3883 - val_f1: 0.3310 - val_precision: 0.4641 - val_recall: 0.2575\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 0.9650 - accuracy: 0.6069 - f1: 0.5492 - precision: 0.7182 - recall: 0.4473 - val_loss: 1.4368 - val_accuracy: 0.4042 - val_f1: 0.3372 - val_precision: 0.4433 - val_recall: 0.2725\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 0.9398 - accuracy: 0.6054 - f1: 0.5602 - precision: 0.7272 - recall: 0.4567 - val_loss: 1.5207 - val_accuracy: 0.3983 - val_f1: 0.3535 - val_precision: 0.4442 - val_recall: 0.2942\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 0.9228 - accuracy: 0.6268 - f1: 0.5654 - precision: 0.7290 - recall: 0.4652 - val_loss: 1.3937 - val_accuracy: 0.3692 - val_f1: 0.2510 - val_precision: 0.4812 - val_recall: 0.1708\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 1.1581 - accuracy: 0.4835 - f1: 0.3335 - precision: 0.6731 - recall: 0.2257 - val_loss: 1.4658 - val_accuracy: 0.3675 - val_f1: 0.3090 - val_precision: 0.4248 - val_recall: 0.2433\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 0.9857 - accuracy: 0.5800 - f1: 0.5030 - precision: 0.7062 - recall: 0.3923 - val_loss: 1.5171 - val_accuracy: 0.3992 - val_f1: 0.3523 - val_precision: 0.4244 - val_recall: 0.3017\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 0.9153 - accuracy: 0.6284 - f1: 0.5677 - precision: 0.7240 - recall: 0.4686 - val_loss: 1.4950 - val_accuracy: 0.3858 - val_f1: 0.3570 - val_precision: 0.4276 - val_recall: 0.3067\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.9050 - accuracy: 0.6222 - f1: 0.5831 - precision: 0.7431 - recall: 0.4810 - val_loss: 1.4990 - val_accuracy: 0.3892 - val_f1: 0.3394 - val_precision: 0.4660 - val_recall: 0.2675\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 3s 58ms/step - loss: 0.9867 - accuracy: 0.5577 - f1: 0.5105 - precision: 0.7099 - recall: 0.4005 - val_loss: 1.5198 - val_accuracy: 0.3825 - val_f1: 0.3058 - val_precision: 0.4144 - val_recall: 0.2425\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 1.0187 - accuracy: 0.5489 - f1: 0.4729 - precision: 0.7258 - recall: 0.3531 - val_loss: 1.3769 - val_accuracy: 0.3883 - val_f1: 0.2881 - val_precision: 0.5030 - val_recall: 0.2025\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 1.0555 - accuracy: 0.5430 - f1: 0.4477 - precision: 0.7178 - recall: 0.3284 - val_loss: 1.4486 - val_accuracy: 0.3900 - val_f1: 0.3083 - val_precision: 0.4799 - val_recall: 0.2275\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 0.9951 - accuracy: 0.5726 - f1: 0.4938 - precision: 0.7537 - recall: 0.3697 - val_loss: 1.4201 - val_accuracy: 0.4050 - val_f1: 0.3168 - val_precision: 0.4654 - val_recall: 0.2408\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 3s 58ms/step - loss: 0.9530 - accuracy: 0.5909 - f1: 0.5271 - precision: 0.7531 - recall: 0.4085 - val_loss: 1.4732 - val_accuracy: 0.3908 - val_f1: 0.3207 - val_precision: 0.4375 - val_recall: 0.2533\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 2s 54ms/step - loss: 0.9214 - accuracy: 0.6020 - f1: 0.5598 - precision: 0.7460 - recall: 0.4494 - val_loss: 1.5283 - val_accuracy: 0.4017 - val_f1: 0.3307 - val_precision: 0.4299 - val_recall: 0.2692\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 3s 59ms/step - loss: 0.9602 - accuracy: 0.6011 - f1: 0.5337 - precision: 0.7406 - recall: 0.4216 - val_loss: 1.5651 - val_accuracy: 0.4108 - val_f1: 0.3388 - val_precision: 0.4471 - val_recall: 0.2733\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.9945 - accuracy: 0.6010 - f1: 0.5475 - precision: 0.7139 - recall: 0.4463 - val_loss: 1.5102 - val_accuracy: 0.4000 - val_f1: 0.3339 - val_precision: 0.4382 - val_recall: 0.2700\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 0.9008 - accuracy: 0.6312 - f1: 0.5834 - precision: 0.7572 - recall: 0.4764 - val_loss: 1.6611 - val_accuracy: 0.3733 - val_f1: 0.3122 - val_precision: 0.4168 - val_recall: 0.2500\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 0.8287 - accuracy: 0.6642 - f1: 0.6235 - precision: 0.7790 - recall: 0.5210 - val_loss: 1.5791 - val_accuracy: 0.4017 - val_f1: 0.3361 - val_precision: 0.4635 - val_recall: 0.2642\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 0.8826 - accuracy: 0.6377 - f1: 0.5920 - precision: 0.7598 - recall: 0.4865 - val_loss: 1.6787 - val_accuracy: 0.3967 - val_f1: 0.3385 - val_precision: 0.4287 - val_recall: 0.2800\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 2s 53ms/step - loss: 0.8866 - accuracy: 0.6300 - f1: 0.5970 - precision: 0.7502 - recall: 0.4977 - val_loss: 1.5192 - val_accuracy: 0.3667 - val_f1: 0.3029 - val_precision: 0.4352 - val_recall: 0.2325\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 2s 49ms/step - loss: 0.8563 - accuracy: 0.6458 - f1: 0.6146 - precision: 0.7853 - recall: 0.5072 - val_loss: 1.5262 - val_accuracy: 0.3633 - val_f1: 0.3103 - val_precision: 0.4385 - val_recall: 0.2408\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 0.9317 - accuracy: 0.6153 - f1: 0.5639 - precision: 0.7385 - recall: 0.4575 - val_loss: 1.4629 - val_accuracy: 0.3933 - val_f1: 0.3360 - val_precision: 0.4754 - val_recall: 0.2608\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 3s 56ms/step - loss: 0.8903 - accuracy: 0.6224 - f1: 0.5844 - precision: 0.7663 - recall: 0.4742 - val_loss: 1.4815 - val_accuracy: 0.3917 - val_f1: 0.3151 - val_precision: 0.4593 - val_recall: 0.2400\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 3s 57ms/step - loss: 0.9575 - accuracy: 0.6089 - f1: 0.5541 - precision: 0.7225 - recall: 0.4513 - val_loss: 1.4025 - val_accuracy: 0.3717 - val_f1: 0.2963 - val_precision: 0.4948 - val_recall: 0.2117\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 3s 61ms/step - loss: 0.9081 - accuracy: 0.6188 - f1: 0.5508 - precision: 0.7613 - recall: 0.4348 - val_loss: 1.4765 - val_accuracy: 0.3800 - val_f1: 0.3005 - val_precision: 0.4577 - val_recall: 0.2242\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 2s 50ms/step - loss: 0.8989 - accuracy: 0.6339 - f1: 0.5884 - precision: 0.7551 - recall: 0.4846 - val_loss: 1.4071 - val_accuracy: 0.3958 - val_f1: 0.3171 - val_precision: 0.4685 - val_recall: 0.2400\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 0.9263 - accuracy: 0.6177 - f1: 0.5701 - precision: 0.7496 - recall: 0.4620 - val_loss: 1.5150 - val_accuracy: 0.3958 - val_f1: 0.3088 - val_precision: 0.4397 - val_recall: 0.2383\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 3s 64ms/step - loss: 0.8656 - accuracy: 0.6422 - f1: 0.6081 - precision: 0.7780 - recall: 0.5007 - val_loss: 1.5351 - val_accuracy: 0.3900 - val_f1: 0.3286 - val_precision: 0.4657 - val_recall: 0.2550\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 3s 55ms/step - loss: 0.8430 - accuracy: 0.6594 - f1: 0.6132 - precision: 0.7796 - recall: 0.5065 - val_loss: 1.6565 - val_accuracy: 0.3958 - val_f1: 0.3487 - val_precision: 0.4570 - val_recall: 0.2825\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.6774 - accuracy: 0.3964 - f1: 0.3531 - precision: 0.4480 - recall: 0.2921\n",
            "[1.677435278892517, 0.39642858505249023, 0.35309508442878723, 0.448011189699173, 0.2921428680419922]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZqN_SaM3hs9",
        "outputId": "16fab651-1e03-48a8-d32b-51594d47bea5"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "# print(history_sg_bi.history)\r\n",
        "# plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()\r\n",
        "# plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.415714293718338, 0.6000000238418579, 0.3678571283817291]\n",
            "cbow [0.4950000047683716, 0.5799999833106995, 0.2921428680419922]\n",
            "glove [0.7492856979370117, 0.7649999856948853, 0.39642858505249023]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}