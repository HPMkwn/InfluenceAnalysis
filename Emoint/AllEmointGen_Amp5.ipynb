{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointGen_Amp5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointGen_Amp5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41678beb-cda2-4174-ffbf-01b8e13a0994"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1,random_state=32)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85b6ab8-c99c-4a1a-dae3-554642d86933"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['with', 'their', 'faggy', 'colors', 'are', 'nice', 'is', 'ok', 'too', 'even', 'tho', 'some', 'might', 'take', 'offense', 'because', 'words', 'lol'], ['unbelievable', 'takes', '10', 'minutes', 'to', 'get', 'through', 'to', 'then', 'there', 'is', 'a', 'fault', 'and', 'the', 'call', 'hangs', 'up', 'treatcustomersfairly'], ['well', 'i', 'did', 'hear', 'once', 'before', 'that', 'girls', 'are', 'attracted', 'to', 'men', 'that', 'lok', 'like', 'their', 'dad', 'ok', 'hand'], ['agreed', 'so', 'tired', 'of', 'this', 'nonsense', 'soros', 'must', 'be', 'elated'], ['by', 'the', 'way', 'i', 'am', 'wearing', 'the', 'smile', 'you', 'gave', 'me', 'today', 'n', 'you', 'me']]\n",
            "[[1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 200\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.200d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "data_tweet = [nltk.pos_tag(x) for x in data_tweet]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=5):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='RB' or tweet[i][1][:2]=='VB':\r\n",
        "            tweet[i]=((tweet[i][0])*mul_factor,tweet[i][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor=5):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='RB' or tweet[i][1][:2]=='VB':\r\n",
        "            tweet[i]=((tweet[i][0])*mul_factor,tweet[i][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.20, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 60"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNj8uX_IBWt1"
      },
      "source": [
        "from keras import backend as K\r\n",
        "def recall(y_true, y_pred):\r\n",
        "        \"\"\"Recall metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of recall.\r\n",
        "\r\n",
        "        Computes the recall, a metric for multi-label classification of\r\n",
        "        how many relevant items are selected.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "        recalls = true_positives / (possible_positives + K.epsilon())\r\n",
        "        return recalls\r\n",
        "def precision(y_true, y_pred):\r\n",
        "        \"\"\"Precision metric.\r\n",
        "\r\n",
        "        Only computes a batch-wise average of precision.\r\n",
        "\r\n",
        "        Computes the precision, a metric for multi-label classification of\r\n",
        "        how many selected items are relevant.\r\n",
        "        \"\"\"\r\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "        precisions = true_positives / (predicted_positives + K.epsilon())\r\n",
        "        return precisions\r\n",
        "def f1(y_true, y_pred):\r\n",
        "    precisions = precision(y_true, y_pred)\r\n",
        "    recalls = recall(y_true, y_pred)\r\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cd0298-8f65-4f83-e800-88235715aa8d"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(64))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_sg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy',f1,precision,recall])\n",
        "sg_train_gen=vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size)\n",
        "sg_val_gen=vec_gen(w2v_sg,vocab_sg,X_val,y_val,batch_size,Max_input_size,Embedding_size)\n",
        "history_sg_lstm = model_sg.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 36s 131ms/step - loss: 1.3828 - accuracy: 0.2866 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3742 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3779 - accuracy: 0.3055 - f1: 7.6630e-05 - precision: 5.3187e-04 - recall: 4.1731e-05 - val_loss: 1.3379 - val_accuracy: 0.3567 - val_f1: 0.0878 - val_precision: 0.6290 - val_recall: 0.0475\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.3465 - accuracy: 0.3429 - f1: 0.0499 - precision: 0.3260 - recall: 0.0284 - val_loss: 1.3040 - val_accuracy: 0.3558 - val_f1: 0.2208 - val_precision: 0.5996 - val_recall: 0.1358\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.2777 - accuracy: 0.3967 - f1: 0.2193 - precision: 0.6892 - recall: 0.1336 - val_loss: 1.2763 - val_accuracy: 0.3850 - val_f1: 0.2129 - val_precision: 0.6884 - val_recall: 0.1267\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 1.2364 - accuracy: 0.4123 - f1: 0.2513 - precision: 0.7249 - recall: 0.1533 - val_loss: 1.2941 - val_accuracy: 0.3775 - val_f1: 0.2555 - val_precision: 0.6537 - val_recall: 0.1592\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.2235 - accuracy: 0.4295 - f1: 0.2526 - precision: 0.7622 - recall: 0.1551 - val_loss: 1.2630 - val_accuracy: 0.4017 - val_f1: 0.2749 - val_precision: 0.6749 - val_recall: 0.1733\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 1.1833 - accuracy: 0.4454 - f1: 0.2851 - precision: 0.7911 - recall: 0.1777 - val_loss: 1.3049 - val_accuracy: 0.3975 - val_f1: 0.2933 - val_precision: 0.6526 - val_recall: 0.1900\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.1825 - accuracy: 0.4532 - f1: 0.2721 - precision: 0.7964 - recall: 0.1702 - val_loss: 1.2380 - val_accuracy: 0.4275 - val_f1: 0.2596 - val_precision: 0.7263 - val_recall: 0.1583\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.1384 - accuracy: 0.4817 - f1: 0.3167 - precision: 0.8105 - recall: 0.2008 - val_loss: 1.2678 - val_accuracy: 0.4208 - val_f1: 0.2754 - val_precision: 0.6907 - val_recall: 0.1725\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 1.0907 - accuracy: 0.5047 - f1: 0.3451 - precision: 0.8258 - recall: 0.2239 - val_loss: 1.2970 - val_accuracy: 0.4183 - val_f1: 0.2797 - val_precision: 0.6573 - val_recall: 0.1783\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 1.0482 - accuracy: 0.5198 - f1: 0.3830 - precision: 0.8213 - recall: 0.2558 - val_loss: 1.2135 - val_accuracy: 0.4517 - val_f1: 0.3596 - val_precision: 0.6669 - val_recall: 0.2467\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.9943 - accuracy: 0.5634 - f1: 0.4231 - precision: 0.7967 - recall: 0.2937 - val_loss: 1.1820 - val_accuracy: 0.4750 - val_f1: 0.3693 - val_precision: 0.7054 - val_recall: 0.2508\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.9478 - accuracy: 0.5875 - f1: 0.4518 - precision: 0.7869 - recall: 0.3220 - val_loss: 1.1695 - val_accuracy: 0.4958 - val_f1: 0.3851 - val_precision: 0.6927 - val_recall: 0.2675\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.8863 - accuracy: 0.6173 - f1: 0.5205 - precision: 0.7710 - recall: 0.3968 - val_loss: 1.1680 - val_accuracy: 0.4850 - val_f1: 0.4363 - val_precision: 0.5632 - val_recall: 0.3567\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.8490 - accuracy: 0.6295 - f1: 0.5863 - precision: 0.7297 - recall: 0.4916 - val_loss: 1.1982 - val_accuracy: 0.4933 - val_f1: 0.4348 - val_precision: 0.5693 - val_recall: 0.3525\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.8437 - accuracy: 0.6419 - f1: 0.5835 - precision: 0.7425 - recall: 0.4830 - val_loss: 1.1562 - val_accuracy: 0.4892 - val_f1: 0.4549 - val_precision: 0.5535 - val_recall: 0.3867\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.8057 - accuracy: 0.6767 - f1: 0.6220 - precision: 0.7467 - recall: 0.5345 - val_loss: 1.1929 - val_accuracy: 0.4725 - val_f1: 0.4655 - val_precision: 0.5297 - val_recall: 0.4158\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.7841 - accuracy: 0.6747 - f1: 0.6330 - precision: 0.7352 - recall: 0.5573 - val_loss: 1.2087 - val_accuracy: 0.4917 - val_f1: 0.4842 - val_precision: 0.5337 - val_recall: 0.4433\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.7244 - accuracy: 0.7082 - f1: 0.6833 - precision: 0.7578 - recall: 0.6235 - val_loss: 1.2377 - val_accuracy: 0.5083 - val_f1: 0.5098 - val_precision: 0.5551 - val_recall: 0.4717\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.6898 - accuracy: 0.7243 - f1: 0.7125 - precision: 0.7662 - recall: 0.6666 - val_loss: 1.2869 - val_accuracy: 0.5142 - val_f1: 0.5155 - val_precision: 0.5481 - val_recall: 0.4867\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.6754 - accuracy: 0.7354 - f1: 0.7250 - precision: 0.7790 - recall: 0.6787 - val_loss: 1.2948 - val_accuracy: 0.5258 - val_f1: 0.5213 - val_precision: 0.5478 - val_recall: 0.4975\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.6547 - accuracy: 0.7535 - f1: 0.7459 - precision: 0.7880 - recall: 0.7087 - val_loss: 1.2854 - val_accuracy: 0.5417 - val_f1: 0.5347 - val_precision: 0.5644 - val_recall: 0.5083\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.7157 - accuracy: 0.7288 - f1: 0.7114 - precision: 0.7536 - recall: 0.6745 - val_loss: 1.2666 - val_accuracy: 0.5042 - val_f1: 0.4868 - val_precision: 0.5270 - val_recall: 0.4525\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.7146 - accuracy: 0.7186 - f1: 0.6972 - precision: 0.7552 - recall: 0.6485 - val_loss: 1.2823 - val_accuracy: 0.5450 - val_f1: 0.5418 - val_precision: 0.5668 - val_recall: 0.5192\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.5985 - accuracy: 0.7817 - f1: 0.7754 - precision: 0.8079 - recall: 0.7459 - val_loss: 1.3417 - val_accuracy: 0.5517 - val_f1: 0.5527 - val_precision: 0.5643 - val_recall: 0.5417\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.5327 - accuracy: 0.8247 - f1: 0.8206 - precision: 0.8379 - recall: 0.8043 - val_loss: 1.3841 - val_accuracy: 0.5608 - val_f1: 0.5611 - val_precision: 0.5702 - val_recall: 0.5525\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.5794 - accuracy: 0.7947 - f1: 0.7971 - precision: 0.8187 - recall: 0.7767 - val_loss: 1.2619 - val_accuracy: 0.5500 - val_f1: 0.5455 - val_precision: 0.5709 - val_recall: 0.5225\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.5892 - accuracy: 0.7894 - f1: 0.7830 - precision: 0.8104 - recall: 0.7577 - val_loss: 1.2542 - val_accuracy: 0.5675 - val_f1: 0.5598 - val_precision: 0.5766 - val_recall: 0.5442\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.5243 - accuracy: 0.8250 - f1: 0.8214 - precision: 0.8342 - recall: 0.8091 - val_loss: 1.2962 - val_accuracy: 0.5600 - val_f1: 0.5571 - val_precision: 0.5661 - val_recall: 0.5483\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.4826 - accuracy: 0.8341 - f1: 0.8333 - precision: 0.8470 - recall: 0.8202 - val_loss: 1.3339 - val_accuracy: 0.5275 - val_f1: 0.5255 - val_precision: 0.5431 - val_recall: 0.5092\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.4930 - accuracy: 0.8377 - f1: 0.8395 - precision: 0.8525 - recall: 0.8270 - val_loss: 1.3487 - val_accuracy: 0.5608 - val_f1: 0.5553 - val_precision: 0.5670 - val_recall: 0.5442\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.4818 - accuracy: 0.8458 - f1: 0.8432 - precision: 0.8543 - recall: 0.8324 - val_loss: 1.3518 - val_accuracy: 0.5458 - val_f1: 0.5420 - val_precision: 0.5538 - val_recall: 0.5308\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.4538 - accuracy: 0.8594 - f1: 0.8568 - precision: 0.8678 - recall: 0.8461 - val_loss: 1.3428 - val_accuracy: 0.5625 - val_f1: 0.5599 - val_precision: 0.5675 - val_recall: 0.5525\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.3983 - accuracy: 0.8825 - f1: 0.8793 - precision: 0.8874 - recall: 0.8715 - val_loss: 1.3759 - val_accuracy: 0.5417 - val_f1: 0.5334 - val_precision: 0.5459 - val_recall: 0.5217\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.3857 - accuracy: 0.8828 - f1: 0.8837 - precision: 0.8933 - recall: 0.8744 - val_loss: 1.3967 - val_accuracy: 0.5375 - val_f1: 0.5388 - val_precision: 0.5488 - val_recall: 0.5292\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.4165 - accuracy: 0.8693 - f1: 0.8689 - precision: 0.8781 - recall: 0.8599 - val_loss: 1.4045 - val_accuracy: 0.5558 - val_f1: 0.5537 - val_precision: 0.5627 - val_recall: 0.5450\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.3993 - accuracy: 0.8734 - f1: 0.8716 - precision: 0.8822 - recall: 0.8614 - val_loss: 1.4082 - val_accuracy: 0.5458 - val_f1: 0.5419 - val_precision: 0.5544 - val_recall: 0.5300\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.4361 - accuracy: 0.8613 - f1: 0.8597 - precision: 0.8703 - recall: 0.8494 - val_loss: 1.4802 - val_accuracy: 0.5392 - val_f1: 0.5360 - val_precision: 0.5439 - val_recall: 0.5283\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.4721 - accuracy: 0.8351 - f1: 0.8342 - precision: 0.8449 - recall: 0.8238 - val_loss: 1.5223 - val_accuracy: 0.5083 - val_f1: 0.5077 - val_precision: 0.5221 - val_recall: 0.4942\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 113ms/step - loss: 0.4457 - accuracy: 0.8505 - f1: 0.8487 - precision: 0.8629 - recall: 0.8351 - val_loss: 1.4731 - val_accuracy: 0.5408 - val_f1: 0.5349 - val_precision: 0.5461 - val_recall: 0.5242\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.3956 - accuracy: 0.8715 - f1: 0.8698 - precision: 0.8824 - recall: 0.8578 - val_loss: 1.5069 - val_accuracy: 0.5133 - val_f1: 0.5021 - val_precision: 0.5132 - val_recall: 0.4917\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 111ms/step - loss: 0.3887 - accuracy: 0.8755 - f1: 0.8756 - precision: 0.8877 - recall: 0.8640 - val_loss: 1.6107 - val_accuracy: 0.5275 - val_f1: 0.5281 - val_precision: 0.5366 - val_recall: 0.5200\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.4082 - accuracy: 0.8743 - f1: 0.8739 - precision: 0.8853 - recall: 0.8629 - val_loss: 1.6006 - val_accuracy: 0.5425 - val_f1: 0.5421 - val_precision: 0.5486 - val_recall: 0.5358\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.4025 - accuracy: 0.8754 - f1: 0.8737 - precision: 0.8834 - recall: 0.8644 - val_loss: 1.5177 - val_accuracy: 0.5858 - val_f1: 0.5801 - val_precision: 0.5880 - val_recall: 0.5725\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.3413 - accuracy: 0.8989 - f1: 0.8999 - precision: 0.9081 - recall: 0.8919 - val_loss: 1.5421 - val_accuracy: 0.5683 - val_f1: 0.5706 - val_precision: 0.5792 - val_recall: 0.5625\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.3563 - accuracy: 0.8954 - f1: 0.8954 - precision: 0.9062 - recall: 0.8850 - val_loss: 1.4694 - val_accuracy: 0.5542 - val_f1: 0.5564 - val_precision: 0.5657 - val_recall: 0.5475\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.3300 - accuracy: 0.9064 - f1: 0.9074 - precision: 0.9151 - recall: 0.8999 - val_loss: 1.5762 - val_accuracy: 0.5550 - val_f1: 0.5548 - val_precision: 0.5623 - val_recall: 0.5475\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.3349 - accuracy: 0.9031 - f1: 0.9040 - precision: 0.9101 - recall: 0.8981 - val_loss: 1.5678 - val_accuracy: 0.5625 - val_f1: 0.5614 - val_precision: 0.5690 - val_recall: 0.5542\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.3173 - accuracy: 0.9031 - f1: 0.9034 - precision: 0.9111 - recall: 0.8959 - val_loss: 1.6438 - val_accuracy: 0.5367 - val_f1: 0.5330 - val_precision: 0.5441 - val_recall: 0.5225\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.5099 - accuracy: 0.8393 - f1: 0.8393 - precision: 0.8633 - recall: 0.8169 - val_loss: 1.3857 - val_accuracy: 0.5892 - val_f1: 0.5867 - val_precision: 0.6094 - val_recall: 0.5658\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.3051 - accuracy: 0.9159 - f1: 0.9149 - precision: 0.9268 - recall: 0.9034 - val_loss: 1.5723 - val_accuracy: 0.5775 - val_f1: 0.5755 - val_precision: 0.5830 - val_recall: 0.5683\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.2653 - accuracy: 0.9288 - f1: 0.9294 - precision: 0.9348 - recall: 0.9242 - val_loss: 1.5795 - val_accuracy: 0.5900 - val_f1: 0.5928 - val_precision: 0.6008 - val_recall: 0.5850\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.2428 - accuracy: 0.9335 - f1: 0.9324 - precision: 0.9397 - recall: 0.9252 - val_loss: 1.5366 - val_accuracy: 0.6008 - val_f1: 0.6013 - val_precision: 0.6112 - val_recall: 0.5917\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.2570 - accuracy: 0.9273 - f1: 0.9260 - precision: 0.9327 - recall: 0.9195 - val_loss: 1.5402 - val_accuracy: 0.6108 - val_f1: 0.6112 - val_precision: 0.6221 - val_recall: 0.6008\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.2776 - accuracy: 0.9197 - f1: 0.9198 - precision: 0.9253 - recall: 0.9143 - val_loss: 1.4974 - val_accuracy: 0.5992 - val_f1: 0.6027 - val_precision: 0.6099 - val_recall: 0.5958\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.2683 - accuracy: 0.9197 - f1: 0.9215 - precision: 0.9318 - recall: 0.9115 - val_loss: 1.5020 - val_accuracy: 0.6008 - val_f1: 0.6050 - val_precision: 0.6164 - val_recall: 0.5942\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.2849 - accuracy: 0.9162 - f1: 0.9164 - precision: 0.9254 - recall: 0.9078 - val_loss: 1.5720 - val_accuracy: 0.6108 - val_f1: 0.6100 - val_precision: 0.6195 - val_recall: 0.6008\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 5s 115ms/step - loss: 0.3626 - accuracy: 0.8921 - f1: 0.8932 - precision: 0.9015 - recall: 0.8850 - val_loss: 1.3277 - val_accuracy: 0.6092 - val_f1: 0.6086 - val_precision: 0.6277 - val_recall: 0.5908\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 5s 114ms/step - loss: 0.3092 - accuracy: 0.9065 - f1: 0.9062 - precision: 0.9234 - recall: 0.8898 - val_loss: 1.5497 - val_accuracy: 0.6192 - val_f1: 0.6154 - val_precision: 0.6309 - val_recall: 0.6008\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.2616 - accuracy: 0.9233 - f1: 0.9250 - precision: 0.9366 - recall: 0.9139 - val_loss: 1.5232 - val_accuracy: 0.6267 - val_f1: 0.6232 - val_precision: 0.6372 - val_recall: 0.6100\n",
            "14/14 [==============================] - 1s 89ms/step - loss: 1.5247 - accuracy: 0.6000 - f1: 0.5999 - precision: 0.6111 - recall: 0.5893\n",
            "[1.5246567726135254, 0.6000000238418579, 0.5999270081520081, 0.6110873222351074, 0.5892857909202576]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHtKjzc1SA8",
        "outputId": "e393be25-91c9-4dcd-8297-ae840a4316f2"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 10s 143ms/step - loss: 1.3806 - accuracy: 0.2899 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3544 - val_accuracy: 0.3383 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.3348 - accuracy: 0.3550 - f1: 0.0097 - precision: 0.1886 - recall: 0.0051 - val_loss: 1.2943 - val_accuracy: 0.3833 - val_f1: 0.0969 - val_precision: 0.6508 - val_recall: 0.0525\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.2310 - accuracy: 0.4549 - f1: 0.1924 - precision: 0.6820 - recall: 0.1147 - val_loss: 1.2450 - val_accuracy: 0.4400 - val_f1: 0.2586 - val_precision: 0.6039 - val_recall: 0.1658\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.1167 - accuracy: 0.5252 - f1: 0.3648 - precision: 0.7206 - recall: 0.2464 - val_loss: 1.1867 - val_accuracy: 0.4950 - val_f1: 0.3596 - val_precision: 0.6296 - val_recall: 0.2525\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 1.0145 - accuracy: 0.5764 - f1: 0.4827 - precision: 0.7490 - recall: 0.3574 - val_loss: 1.1935 - val_accuracy: 0.4992 - val_f1: 0.4291 - val_precision: 0.6171 - val_recall: 0.3300\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.9108 - accuracy: 0.6449 - f1: 0.5842 - precision: 0.7747 - recall: 0.4700 - val_loss: 1.1686 - val_accuracy: 0.5233 - val_f1: 0.4829 - val_precision: 0.6141 - val_recall: 0.3983\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.8088 - accuracy: 0.6775 - f1: 0.6440 - precision: 0.7987 - recall: 0.5406 - val_loss: 1.1312 - val_accuracy: 0.5508 - val_f1: 0.5164 - val_precision: 0.6449 - val_recall: 0.4317\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.7344 - accuracy: 0.7182 - f1: 0.6966 - precision: 0.8166 - recall: 0.6083 - val_loss: 1.1841 - val_accuracy: 0.5692 - val_f1: 0.5427 - val_precision: 0.6220 - val_recall: 0.4817\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.6566 - accuracy: 0.7558 - f1: 0.7379 - precision: 0.8378 - recall: 0.6600 - val_loss: 1.2194 - val_accuracy: 0.5742 - val_f1: 0.5583 - val_precision: 0.6220 - val_recall: 0.5067\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.6051 - accuracy: 0.7747 - f1: 0.7646 - precision: 0.8404 - recall: 0.7021 - val_loss: 1.2519 - val_accuracy: 0.5800 - val_f1: 0.5555 - val_precision: 0.6178 - val_recall: 0.5050\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.5639 - accuracy: 0.7953 - f1: 0.7894 - precision: 0.8633 - recall: 0.7276 - val_loss: 1.2400 - val_accuracy: 0.5808 - val_f1: 0.5713 - val_precision: 0.6268 - val_recall: 0.5250\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4975 - accuracy: 0.8175 - f1: 0.8143 - precision: 0.8771 - recall: 0.7606 - val_loss: 1.2620 - val_accuracy: 0.5767 - val_f1: 0.5662 - val_precision: 0.6197 - val_recall: 0.5217\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.5206 - accuracy: 0.8265 - f1: 0.8105 - precision: 0.8700 - recall: 0.7591 - val_loss: 1.3577 - val_accuracy: 0.5958 - val_f1: 0.5919 - val_precision: 0.6333 - val_recall: 0.5558\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4244 - accuracy: 0.8505 - f1: 0.8508 - precision: 0.8927 - recall: 0.8128 - val_loss: 1.3558 - val_accuracy: 0.5992 - val_f1: 0.5860 - val_precision: 0.6219 - val_recall: 0.5542\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.3692 - accuracy: 0.8745 - f1: 0.8778 - precision: 0.9109 - recall: 0.8472 - val_loss: 1.3433 - val_accuracy: 0.5933 - val_f1: 0.5956 - val_precision: 0.6267 - val_recall: 0.5675\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3145 - accuracy: 0.8870 - f1: 0.8896 - precision: 0.9215 - recall: 0.8603 - val_loss: 1.4472 - val_accuracy: 0.6000 - val_f1: 0.5967 - val_precision: 0.6243 - val_recall: 0.5717\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3075 - accuracy: 0.8926 - f1: 0.8900 - precision: 0.9167 - recall: 0.8650 - val_loss: 1.4497 - val_accuracy: 0.6050 - val_f1: 0.6014 - val_precision: 0.6288 - val_recall: 0.5767\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2771 - accuracy: 0.9039 - f1: 0.9030 - precision: 0.9276 - recall: 0.8800 - val_loss: 1.4701 - val_accuracy: 0.6108 - val_f1: 0.6058 - val_precision: 0.6285 - val_recall: 0.5850\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2396 - accuracy: 0.9162 - f1: 0.9184 - precision: 0.9409 - recall: 0.8970 - val_loss: 1.6047 - val_accuracy: 0.5975 - val_f1: 0.5932 - val_precision: 0.6157 - val_recall: 0.5725\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2313 - accuracy: 0.9245 - f1: 0.9232 - precision: 0.9418 - recall: 0.9054 - val_loss: 1.6076 - val_accuracy: 0.5933 - val_f1: 0.5942 - val_precision: 0.6110 - val_recall: 0.5783\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2243 - accuracy: 0.9154 - f1: 0.9178 - precision: 0.9369 - recall: 0.8997 - val_loss: 1.5983 - val_accuracy: 0.5767 - val_f1: 0.5758 - val_precision: 0.6004 - val_recall: 0.5533\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2288 - accuracy: 0.9206 - f1: 0.9177 - precision: 0.9365 - recall: 0.8998 - val_loss: 1.6466 - val_accuracy: 0.6042 - val_f1: 0.5980 - val_precision: 0.6202 - val_recall: 0.5775\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2174 - accuracy: 0.9217 - f1: 0.9251 - precision: 0.9439 - recall: 0.9073 - val_loss: 1.6466 - val_accuracy: 0.5808 - val_f1: 0.5822 - val_precision: 0.6005 - val_recall: 0.5650\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2045 - accuracy: 0.9270 - f1: 0.9285 - precision: 0.9486 - recall: 0.9095 - val_loss: 1.6055 - val_accuracy: 0.6175 - val_f1: 0.6103 - val_precision: 0.6313 - val_recall: 0.5908\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2013 - accuracy: 0.9298 - f1: 0.9311 - precision: 0.9467 - recall: 0.9161 - val_loss: 1.7124 - val_accuracy: 0.5900 - val_f1: 0.5881 - val_precision: 0.6046 - val_recall: 0.5725\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1817 - accuracy: 0.9426 - f1: 0.9436 - precision: 0.9577 - recall: 0.9301 - val_loss: 1.7356 - val_accuracy: 0.6183 - val_f1: 0.6069 - val_precision: 0.6250 - val_recall: 0.5900\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1530 - accuracy: 0.9483 - f1: 0.9502 - precision: 0.9616 - recall: 0.9392 - val_loss: 1.7589 - val_accuracy: 0.5975 - val_f1: 0.5961 - val_precision: 0.6104 - val_recall: 0.5825\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1397 - accuracy: 0.9533 - f1: 0.9516 - precision: 0.9639 - recall: 0.9398 - val_loss: 1.8442 - val_accuracy: 0.6117 - val_f1: 0.6105 - val_precision: 0.6250 - val_recall: 0.5967\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1145 - accuracy: 0.9611 - f1: 0.9561 - precision: 0.9662 - recall: 0.9463 - val_loss: 1.9564 - val_accuracy: 0.5867 - val_f1: 0.5881 - val_precision: 0.5992 - val_recall: 0.5775\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1206 - accuracy: 0.9576 - f1: 0.9576 - precision: 0.9685 - recall: 0.9470 - val_loss: 1.9628 - val_accuracy: 0.5992 - val_f1: 0.5959 - val_precision: 0.6102 - val_recall: 0.5825\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.1512 - accuracy: 0.9499 - f1: 0.9472 - precision: 0.9584 - recall: 0.9363 - val_loss: 2.0283 - val_accuracy: 0.5875 - val_f1: 0.5873 - val_precision: 0.6002 - val_recall: 0.5750\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1624 - accuracy: 0.9460 - f1: 0.9442 - precision: 0.9556 - recall: 0.9332 - val_loss: 2.2509 - val_accuracy: 0.5825 - val_f1: 0.5775 - val_precision: 0.5852 - val_recall: 0.5700\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1394 - accuracy: 0.9515 - f1: 0.9496 - precision: 0.9592 - recall: 0.9403 - val_loss: 2.1626 - val_accuracy: 0.6050 - val_f1: 0.6068 - val_precision: 0.6148 - val_recall: 0.5992\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1528 - accuracy: 0.9538 - f1: 0.9523 - precision: 0.9602 - recall: 0.9445 - val_loss: 2.0421 - val_accuracy: 0.5917 - val_f1: 0.5933 - val_precision: 0.6028 - val_recall: 0.5842\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1171 - accuracy: 0.9575 - f1: 0.9581 - precision: 0.9650 - recall: 0.9514 - val_loss: 2.1997 - val_accuracy: 0.5958 - val_f1: 0.5990 - val_precision: 0.6066 - val_recall: 0.5917\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1146 - accuracy: 0.9524 - f1: 0.9550 - precision: 0.9635 - recall: 0.9467 - val_loss: 2.1674 - val_accuracy: 0.5992 - val_f1: 0.5977 - val_precision: 0.6110 - val_recall: 0.5850\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1047 - accuracy: 0.9589 - f1: 0.9590 - precision: 0.9672 - recall: 0.9510 - val_loss: 2.1633 - val_accuracy: 0.6100 - val_f1: 0.6092 - val_precision: 0.6196 - val_recall: 0.5992\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0971 - accuracy: 0.9623 - f1: 0.9621 - precision: 0.9669 - recall: 0.9574 - val_loss: 2.1671 - val_accuracy: 0.6142 - val_f1: 0.6163 - val_precision: 0.6273 - val_recall: 0.6058\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0991 - accuracy: 0.9623 - f1: 0.9629 - precision: 0.9689 - recall: 0.9571 - val_loss: 2.3001 - val_accuracy: 0.6108 - val_f1: 0.6055 - val_precision: 0.6182 - val_recall: 0.5933\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3478 - accuracy: 0.9250 - f1: 0.9233 - precision: 0.9363 - recall: 0.9113 - val_loss: 1.4480 - val_accuracy: 0.5558 - val_f1: 0.5479 - val_precision: 0.5868 - val_recall: 0.5142\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.3950 - accuracy: 0.8494 - f1: 0.8473 - precision: 0.8771 - recall: 0.8197 - val_loss: 1.6828 - val_accuracy: 0.5758 - val_f1: 0.5797 - val_precision: 0.6002 - val_recall: 0.5608\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2676 - accuracy: 0.8957 - f1: 0.8941 - precision: 0.9126 - recall: 0.8766 - val_loss: 1.9493 - val_accuracy: 0.5742 - val_f1: 0.5735 - val_precision: 0.5850 - val_recall: 0.5625\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2212 - accuracy: 0.9170 - f1: 0.9172 - precision: 0.9304 - recall: 0.9045 - val_loss: 1.9257 - val_accuracy: 0.5933 - val_f1: 0.5922 - val_precision: 0.6061 - val_recall: 0.5792\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1737 - accuracy: 0.9372 - f1: 0.9372 - precision: 0.9444 - recall: 0.9302 - val_loss: 1.9945 - val_accuracy: 0.5967 - val_f1: 0.5965 - val_precision: 0.6114 - val_recall: 0.5825\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.1404 - accuracy: 0.9496 - f1: 0.9473 - precision: 0.9549 - recall: 0.9399 - val_loss: 2.1157 - val_accuracy: 0.6008 - val_f1: 0.5972 - val_precision: 0.6082 - val_recall: 0.5867\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.1227 - accuracy: 0.9555 - f1: 0.9558 - precision: 0.9613 - recall: 0.9503 - val_loss: 2.1028 - val_accuracy: 0.6050 - val_f1: 0.6014 - val_precision: 0.6090 - val_recall: 0.5942\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1136 - accuracy: 0.9588 - f1: 0.9574 - precision: 0.9615 - recall: 0.9533 - val_loss: 2.2359 - val_accuracy: 0.6092 - val_f1: 0.6048 - val_precision: 0.6132 - val_recall: 0.5967\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1167 - accuracy: 0.9573 - f1: 0.9562 - precision: 0.9604 - recall: 0.9522 - val_loss: 2.1708 - val_accuracy: 0.6008 - val_f1: 0.5992 - val_precision: 0.6080 - val_recall: 0.5908\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1056 - accuracy: 0.9581 - f1: 0.9574 - precision: 0.9607 - recall: 0.9541 - val_loss: 2.2670 - val_accuracy: 0.6017 - val_f1: 0.6051 - val_precision: 0.6120 - val_recall: 0.5983\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.0928 - accuracy: 0.9670 - f1: 0.9666 - precision: 0.9714 - recall: 0.9620 - val_loss: 2.3362 - val_accuracy: 0.5975 - val_f1: 0.5995 - val_precision: 0.6085 - val_recall: 0.5908\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0931 - accuracy: 0.9656 - f1: 0.9646 - precision: 0.9674 - recall: 0.9620 - val_loss: 2.4049 - val_accuracy: 0.6025 - val_f1: 0.6004 - val_precision: 0.6068 - val_recall: 0.5942\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0862 - accuracy: 0.9661 - f1: 0.9669 - precision: 0.9706 - recall: 0.9632 - val_loss: 2.3599 - val_accuracy: 0.6125 - val_f1: 0.6102 - val_precision: 0.6156 - val_recall: 0.6050\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.0997 - accuracy: 0.9637 - f1: 0.9630 - precision: 0.9669 - recall: 0.9592 - val_loss: 2.4451 - val_accuracy: 0.5867 - val_f1: 0.5910 - val_precision: 0.5990 - val_recall: 0.5833\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.0812 - accuracy: 0.9653 - f1: 0.9662 - precision: 0.9705 - recall: 0.9621 - val_loss: 2.5598 - val_accuracy: 0.6017 - val_f1: 0.5977 - val_precision: 0.6039 - val_recall: 0.5917\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0749 - accuracy: 0.9681 - f1: 0.9684 - precision: 0.9718 - recall: 0.9652 - val_loss: 2.5084 - val_accuracy: 0.5983 - val_f1: 0.5987 - val_precision: 0.6033 - val_recall: 0.5942\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.0809 - accuracy: 0.9690 - f1: 0.9695 - precision: 0.9720 - recall: 0.9671 - val_loss: 2.5566 - val_accuracy: 0.6000 - val_f1: 0.5989 - val_precision: 0.6047 - val_recall: 0.5933\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 128ms/step - loss: 0.0754 - accuracy: 0.9669 - f1: 0.9664 - precision: 0.9681 - recall: 0.9648 - val_loss: 2.6456 - val_accuracy: 0.6050 - val_f1: 0.6035 - val_precision: 0.6097 - val_recall: 0.5975\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.0841 - accuracy: 0.9661 - f1: 0.9665 - precision: 0.9698 - recall: 0.9632 - val_loss: 2.5296 - val_accuracy: 0.6008 - val_f1: 0.6004 - val_precision: 0.6059 - val_recall: 0.5950\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.0875 - accuracy: 0.9679 - f1: 0.9680 - precision: 0.9715 - recall: 0.9646 - val_loss: 2.5480 - val_accuracy: 0.6033 - val_f1: 0.5987 - val_precision: 0.6042 - val_recall: 0.5933\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.0722 - accuracy: 0.9701 - f1: 0.9692 - precision: 0.9723 - recall: 0.9661 - val_loss: 2.5472 - val_accuracy: 0.6017 - val_f1: 0.6028 - val_precision: 0.6099 - val_recall: 0.5958\n",
            "14/14 [==============================] - 1s 92ms/step - loss: 2.4490 - accuracy: 0.6143 - f1: 0.6161 - precision: 0.6247 - recall: 0.6079\n",
            "[2.448981523513794, 0.6142857074737549, 0.6161419749259949, 0.6247459650039673, 0.6078571081161499]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opfhhYPH1VoL",
        "outputId": "bb7339db-b869-4874-8402-97e7ea299e27"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(sg_train_gen,validation_data=sg_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 9s 165ms/step - loss: 1.3877 - accuracy: 0.2759 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3701 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 7s 164ms/step - loss: 1.3807 - accuracy: 0.3036 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3631 - val_accuracy: 0.3417 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.3752 - accuracy: 0.3064 - f1: 7.2363e-05 - precision: 9.4072e-04 - recall: 3.7629e-05 - val_loss: 1.3608 - val_accuracy: 0.3400 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.3693 - accuracy: 0.3263 - f1: 0.0178 - precision: 0.1920 - recall: 0.0097 - val_loss: 1.3637 - val_accuracy: 0.3367 - val_f1: 0.0586 - val_precision: 0.3989 - val_recall: 0.0317\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 1.3281 - accuracy: 0.3619 - f1: 0.0700 - precision: 0.4628 - recall: 0.0398 - val_loss: 1.3812 - val_accuracy: 0.3242 - val_f1: 0.0955 - val_precision: 0.4823 - val_recall: 0.0533\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.3104 - accuracy: 0.3878 - f1: 0.0961 - precision: 0.4935 - recall: 0.0546 - val_loss: 1.3652 - val_accuracy: 0.3475 - val_f1: 0.1432 - val_precision: 0.4623 - val_recall: 0.0850\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.3189 - accuracy: 0.3783 - f1: 0.1020 - precision: 0.5715 - recall: 0.0591 - val_loss: 1.4250 - val_accuracy: 0.3125 - val_f1: 0.0992 - val_precision: 0.4100 - val_recall: 0.0567\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.3455 - accuracy: 0.3576 - f1: 0.1007 - precision: 0.4270 - recall: 0.0577 - val_loss: 1.3941 - val_accuracy: 0.2975 - val_f1: 0.0033 - val_precision: 0.1667 - val_recall: 0.0017\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.3560 - accuracy: 0.3323 - f1: 0.0224 - precision: 0.3997 - recall: 0.0117 - val_loss: 1.3788 - val_accuracy: 0.2983 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.3442 - accuracy: 0.3642 - f1: 0.0096 - precision: 0.2529 - recall: 0.0049 - val_loss: 1.3881 - val_accuracy: 0.2983 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.3038 - accuracy: 0.3935 - f1: 0.0518 - precision: 0.5734 - recall: 0.0277 - val_loss: 1.4110 - val_accuracy: 0.3025 - val_f1: 0.0651 - val_precision: 0.3896 - val_recall: 0.0358\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.2704 - accuracy: 0.4234 - f1: 0.1517 - precision: 0.5638 - recall: 0.0885 - val_loss: 1.4169 - val_accuracy: 0.2958 - val_f1: 0.1079 - val_precision: 0.4123 - val_recall: 0.0625\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.2669 - accuracy: 0.4223 - f1: 0.1831 - precision: 0.5623 - recall: 0.1111 - val_loss: 1.3880 - val_accuracy: 0.3150 - val_f1: 0.0848 - val_precision: 0.5007 - val_recall: 0.0467\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.2715 - accuracy: 0.4262 - f1: 0.1819 - precision: 0.5955 - recall: 0.1092 - val_loss: 1.4369 - val_accuracy: 0.3150 - val_f1: 0.1022 - val_precision: 0.3506 - val_recall: 0.0600\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.2988 - accuracy: 0.4039 - f1: 0.1651 - precision: 0.5538 - recall: 0.0978 - val_loss: 1.3978 - val_accuracy: 0.3167 - val_f1: 0.0953 - val_precision: 0.4595 - val_recall: 0.0533\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.2963 - accuracy: 0.4114 - f1: 0.1330 - precision: 0.5337 - recall: 0.0776 - val_loss: 1.3848 - val_accuracy: 0.3208 - val_f1: 0.0920 - val_precision: 0.4511 - val_recall: 0.0517\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.2648 - accuracy: 0.4176 - f1: 0.1960 - precision: 0.5914 - recall: 0.1200 - val_loss: 1.4027 - val_accuracy: 0.3292 - val_f1: 0.1355 - val_precision: 0.4754 - val_recall: 0.0792\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 1.2001 - accuracy: 0.4807 - f1: 0.2727 - precision: 0.6443 - recall: 0.1760 - val_loss: 1.4414 - val_accuracy: 0.3183 - val_f1: 0.1956 - val_precision: 0.3988 - val_recall: 0.1300\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.2460 - accuracy: 0.4264 - f1: 0.2725 - precision: 0.5864 - recall: 0.1784 - val_loss: 1.4035 - val_accuracy: 0.3358 - val_f1: 0.1813 - val_precision: 0.4013 - val_recall: 0.1175\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.1878 - accuracy: 0.4712 - f1: 0.3119 - precision: 0.6503 - recall: 0.2069 - val_loss: 1.4536 - val_accuracy: 0.3158 - val_f1: 0.1974 - val_precision: 0.3847 - val_recall: 0.1333\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.1417 - accuracy: 0.5077 - f1: 0.3672 - precision: 0.6605 - recall: 0.2563 - val_loss: 1.4808 - val_accuracy: 0.3192 - val_f1: 0.2078 - val_precision: 0.3599 - val_recall: 0.1467\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.1415 - accuracy: 0.5138 - f1: 0.3954 - precision: 0.6343 - recall: 0.2883 - val_loss: 1.5411 - val_accuracy: 0.2867 - val_f1: 0.1816 - val_precision: 0.3140 - val_recall: 0.1283\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.1644 - accuracy: 0.4996 - f1: 0.3806 - precision: 0.6015 - recall: 0.2796 - val_loss: 1.5324 - val_accuracy: 0.2925 - val_f1: 0.1862 - val_precision: 0.3403 - val_recall: 0.1283\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 1.1718 - accuracy: 0.5053 - f1: 0.3623 - precision: 0.6111 - recall: 0.2586 - val_loss: 1.4598 - val_accuracy: 0.3558 - val_f1: 0.2197 - val_precision: 0.4067 - val_recall: 0.1508\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 1.0648 - accuracy: 0.5609 - f1: 0.4534 - precision: 0.6843 - recall: 0.3401 - val_loss: 1.5932 - val_accuracy: 0.2933 - val_f1: 0.2222 - val_precision: 0.3345 - val_recall: 0.1667\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 1.0353 - accuracy: 0.5697 - f1: 0.4815 - precision: 0.6792 - recall: 0.3749 - val_loss: 1.6000 - val_accuracy: 0.3308 - val_f1: 0.2596 - val_precision: 0.3635 - val_recall: 0.2025\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.9852 - accuracy: 0.6058 - f1: 0.5364 - precision: 0.7130 - recall: 0.4310 - val_loss: 1.6598 - val_accuracy: 0.3142 - val_f1: 0.2604 - val_precision: 0.3577 - val_recall: 0.2050\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 0.9813 - accuracy: 0.5993 - f1: 0.5415 - precision: 0.7001 - recall: 0.4424 - val_loss: 1.7097 - val_accuracy: 0.3000 - val_f1: 0.2455 - val_precision: 0.3248 - val_recall: 0.1975\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.0972 - accuracy: 0.5314 - f1: 0.4709 - precision: 0.6326 - recall: 0.3766 - val_loss: 1.5479 - val_accuracy: 0.3000 - val_f1: 0.2046 - val_precision: 0.3138 - val_recall: 0.1525\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.1529 - accuracy: 0.5019 - f1: 0.4011 - precision: 0.6143 - recall: 0.2994 - val_loss: 1.5697 - val_accuracy: 0.2900 - val_f1: 0.2064 - val_precision: 0.3213 - val_recall: 0.1525\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.0768 - accuracy: 0.5370 - f1: 0.4390 - precision: 0.6651 - recall: 0.3296 - val_loss: 1.6041 - val_accuracy: 0.3142 - val_f1: 0.2287 - val_precision: 0.3352 - val_recall: 0.1742\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.9993 - accuracy: 0.5837 - f1: 0.5207 - precision: 0.7063 - recall: 0.4134 - val_loss: 1.6184 - val_accuracy: 0.3192 - val_f1: 0.2413 - val_precision: 0.3370 - val_recall: 0.1883\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 0.9799 - accuracy: 0.5965 - f1: 0.5289 - precision: 0.7035 - recall: 0.4255 - val_loss: 1.6146 - val_accuracy: 0.3192 - val_f1: 0.2501 - val_precision: 0.3588 - val_recall: 0.1925\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.0242 - accuracy: 0.5757 - f1: 0.5145 - precision: 0.6824 - recall: 0.4138 - val_loss: 1.6825 - val_accuracy: 0.3192 - val_f1: 0.2717 - val_precision: 0.3521 - val_recall: 0.2217\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.9336 - accuracy: 0.6261 - f1: 0.5658 - precision: 0.7183 - recall: 0.4676 - val_loss: 1.6279 - val_accuracy: 0.3300 - val_f1: 0.2666 - val_precision: 0.3636 - val_recall: 0.2108\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.9377 - accuracy: 0.6131 - f1: 0.5581 - precision: 0.7092 - recall: 0.4615 - val_loss: 1.7218 - val_accuracy: 0.3258 - val_f1: 0.2726 - val_precision: 0.3485 - val_recall: 0.2242\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.9242 - accuracy: 0.6298 - f1: 0.5779 - precision: 0.7086 - recall: 0.4893 - val_loss: 1.7816 - val_accuracy: 0.3050 - val_f1: 0.2557 - val_precision: 0.3219 - val_recall: 0.2125\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.0298 - accuracy: 0.5799 - f1: 0.5223 - precision: 0.6688 - recall: 0.4307 - val_loss: 1.7279 - val_accuracy: 0.3008 - val_f1: 0.2574 - val_precision: 0.3350 - val_recall: 0.2092\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.9573 - accuracy: 0.6122 - f1: 0.5606 - precision: 0.7018 - recall: 0.4673 - val_loss: 1.7381 - val_accuracy: 0.3092 - val_f1: 0.2650 - val_precision: 0.3404 - val_recall: 0.2175\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.0746 - accuracy: 0.5529 - f1: 0.4962 - precision: 0.6368 - recall: 0.4076 - val_loss: 1.6828 - val_accuracy: 0.3083 - val_f1: 0.2651 - val_precision: 0.3445 - val_recall: 0.2158\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 0.9581 - accuracy: 0.6076 - f1: 0.5421 - precision: 0.7011 - recall: 0.4433 - val_loss: 1.6290 - val_accuracy: 0.3342 - val_f1: 0.2633 - val_precision: 0.3662 - val_recall: 0.2058\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.9870 - accuracy: 0.5919 - f1: 0.5320 - precision: 0.6923 - recall: 0.4331 - val_loss: 1.7159 - val_accuracy: 0.3025 - val_f1: 0.2561 - val_precision: 0.3297 - val_recall: 0.2100\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.8975 - accuracy: 0.6388 - f1: 0.5885 - precision: 0.7239 - recall: 0.4966 - val_loss: 1.8139 - val_accuracy: 0.3000 - val_f1: 0.2610 - val_precision: 0.3299 - val_recall: 0.2167\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.8971 - accuracy: 0.6375 - f1: 0.5910 - precision: 0.7210 - recall: 0.5020 - val_loss: 1.8438 - val_accuracy: 0.3075 - val_f1: 0.2588 - val_precision: 0.3241 - val_recall: 0.2158\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 0.8834 - accuracy: 0.6452 - f1: 0.6082 - precision: 0.7287 - recall: 0.5231 - val_loss: 1.8920 - val_accuracy: 0.3067 - val_f1: 0.2713 - val_precision: 0.3330 - val_recall: 0.2292\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 0.9117 - accuracy: 0.6304 - f1: 0.5974 - precision: 0.7018 - recall: 0.5208 - val_loss: 1.8704 - val_accuracy: 0.3183 - val_f1: 0.2691 - val_precision: 0.3317 - val_recall: 0.2267\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.8893 - accuracy: 0.6309 - f1: 0.6086 - precision: 0.7222 - recall: 0.5266 - val_loss: 1.9546 - val_accuracy: 0.2950 - val_f1: 0.2508 - val_precision: 0.3015 - val_recall: 0.2150\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.9085 - accuracy: 0.6248 - f1: 0.5845 - precision: 0.6889 - recall: 0.5084 - val_loss: 1.9171 - val_accuracy: 0.2992 - val_f1: 0.2514 - val_precision: 0.3082 - val_recall: 0.2125\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.8672 - accuracy: 0.6492 - f1: 0.6161 - precision: 0.7209 - recall: 0.5385 - val_loss: 1.9376 - val_accuracy: 0.3192 - val_f1: 0.2859 - val_precision: 0.3404 - val_recall: 0.2467\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.8288 - accuracy: 0.6616 - f1: 0.6291 - precision: 0.7357 - recall: 0.5503 - val_loss: 1.9577 - val_accuracy: 0.3150 - val_f1: 0.2870 - val_precision: 0.3425 - val_recall: 0.2475\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 7s 161ms/step - loss: 0.8597 - accuracy: 0.6463 - f1: 0.6249 - precision: 0.7271 - recall: 0.5486 - val_loss: 1.8946 - val_accuracy: 0.3025 - val_f1: 0.2661 - val_precision: 0.3244 - val_recall: 0.2258\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.9149 - accuracy: 0.6282 - f1: 0.5900 - precision: 0.6954 - recall: 0.5132 - val_loss: 1.8041 - val_accuracy: 0.3017 - val_f1: 0.2599 - val_precision: 0.3336 - val_recall: 0.2133\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 0.9502 - accuracy: 0.6066 - f1: 0.5646 - precision: 0.6814 - recall: 0.4830 - val_loss: 1.6585 - val_accuracy: 0.3292 - val_f1: 0.2749 - val_precision: 0.3671 - val_recall: 0.2200\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.8239 - accuracy: 0.6697 - f1: 0.6258 - precision: 0.7505 - recall: 0.5377 - val_loss: 1.7682 - val_accuracy: 0.3350 - val_f1: 0.2847 - val_precision: 0.3486 - val_recall: 0.2408\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.7370 - accuracy: 0.7177 - f1: 0.6924 - precision: 0.7839 - recall: 0.6211 - val_loss: 1.9289 - val_accuracy: 0.3317 - val_f1: 0.2991 - val_precision: 0.3528 - val_recall: 0.2600\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.6724 - accuracy: 0.7476 - f1: 0.7271 - precision: 0.8000 - recall: 0.6671 - val_loss: 1.9960 - val_accuracy: 0.3508 - val_f1: 0.3105 - val_precision: 0.3555 - val_recall: 0.2767\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.7745 - accuracy: 0.7067 - f1: 0.6863 - precision: 0.7604 - recall: 0.6259 - val_loss: 2.0181 - val_accuracy: 0.3358 - val_f1: 0.3097 - val_precision: 0.3523 - val_recall: 0.2767\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.7639 - accuracy: 0.7037 - f1: 0.6903 - precision: 0.7559 - recall: 0.6357 - val_loss: 2.0440 - val_accuracy: 0.3258 - val_f1: 0.3002 - val_precision: 0.3410 - val_recall: 0.2683\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.7639 - accuracy: 0.7059 - f1: 0.6841 - precision: 0.7570 - recall: 0.6249 - val_loss: 2.0896 - val_accuracy: 0.3117 - val_f1: 0.2753 - val_precision: 0.3117 - val_recall: 0.2467\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.8055 - accuracy: 0.6800 - f1: 0.6565 - precision: 0.7337 - recall: 0.5946 - val_loss: 1.9442 - val_accuracy: 0.3150 - val_f1: 0.2843 - val_precision: 0.3300 - val_recall: 0.2500\n",
            "14/14 [==============================] - 1s 89ms/step - loss: 2.0043 - accuracy: 0.2979 - f1: 0.2643 - precision: 0.3086 - recall: 0.2314\n",
            "[2.004330635070801, 0.2978571355342865, 0.26430872082710266, 0.30856674909591675, 0.2314285933971405]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YumD1M9Y1Xxr",
        "outputId": "cc178091-2c1d-4691-e6aa-d466cc679b47"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(64))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "cbow_train_gen=vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "cbow_val_gen=vec_gen(w2v_cbow,vocab_cbow,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 8s 133ms/step - loss: 1.3825 - accuracy: 0.2964 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3734 - val_accuracy: 0.3308 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3758 - accuracy: 0.3067 - f1: 0.0082 - precision: 0.0497 - recall: 0.0048 - val_loss: 1.3373 - val_accuracy: 0.3617 - val_f1: 0.0464 - val_precision: 0.6379 - val_recall: 0.0242\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.3336 - accuracy: 0.3497 - f1: 0.1107 - precision: 0.6338 - recall: 0.0636 - val_loss: 1.3317 - val_accuracy: 0.3600 - val_f1: 0.2415 - val_precision: 0.5526 - val_recall: 0.1550\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2873 - accuracy: 0.3838 - f1: 0.2080 - precision: 0.6945 - recall: 0.1265 - val_loss: 1.2957 - val_accuracy: 0.3800 - val_f1: 0.1836 - val_precision: 0.6849 - val_recall: 0.1067\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2454 - accuracy: 0.3952 - f1: 0.2400 - precision: 0.7649 - recall: 0.1443 - val_loss: 1.2846 - val_accuracy: 0.3925 - val_f1: 0.2753 - val_precision: 0.6520 - val_recall: 0.1750\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.2324 - accuracy: 0.4021 - f1: 0.2638 - precision: 0.7757 - recall: 0.1625 - val_loss: 1.2731 - val_accuracy: 0.4058 - val_f1: 0.2747 - val_precision: 0.6805 - val_recall: 0.1725\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.2081 - accuracy: 0.4152 - f1: 0.2779 - precision: 0.8032 - recall: 0.1711 - val_loss: 1.2690 - val_accuracy: 0.3925 - val_f1: 0.2521 - val_precision: 0.7025 - val_recall: 0.1542\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.1817 - accuracy: 0.4241 - f1: 0.2966 - precision: 0.8152 - recall: 0.1835 - val_loss: 1.2636 - val_accuracy: 0.4125 - val_f1: 0.2690 - val_precision: 0.7388 - val_recall: 0.1650\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.1671 - accuracy: 0.4375 - f1: 0.3088 - precision: 0.8389 - recall: 0.1914 - val_loss: 1.2767 - val_accuracy: 0.4142 - val_f1: 0.2782 - val_precision: 0.6741 - val_recall: 0.1758\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 1.1756 - accuracy: 0.4362 - f1: 0.2899 - precision: 0.8493 - recall: 0.1788 - val_loss: 1.3017 - val_accuracy: 0.3908 - val_f1: 0.2770 - val_precision: 0.6415 - val_recall: 0.1775\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.1588 - accuracy: 0.4487 - f1: 0.3304 - precision: 0.7958 - recall: 0.2107 - val_loss: 1.3221 - val_accuracy: 0.3992 - val_f1: 0.3060 - val_precision: 0.6274 - val_recall: 0.2025\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.1218 - accuracy: 0.4633 - f1: 0.3413 - precision: 0.8380 - recall: 0.2162 - val_loss: 1.3534 - val_accuracy: 0.4392 - val_f1: 0.3220 - val_precision: 0.6230 - val_recall: 0.2175\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.1158 - accuracy: 0.4730 - f1: 0.3446 - precision: 0.8441 - recall: 0.2185 - val_loss: 1.3055 - val_accuracy: 0.4275 - val_f1: 0.3274 - val_precision: 0.6356 - val_recall: 0.2208\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.0869 - accuracy: 0.4893 - f1: 0.3726 - precision: 0.8536 - recall: 0.2405 - val_loss: 1.2920 - val_accuracy: 0.4325 - val_f1: 0.3287 - val_precision: 0.6477 - val_recall: 0.2208\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 1.0407 - accuracy: 0.5051 - f1: 0.3964 - precision: 0.8543 - recall: 0.2609 - val_loss: 1.3016 - val_accuracy: 0.4525 - val_f1: 0.3487 - val_precision: 0.6301 - val_recall: 0.2417\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 1.0619 - accuracy: 0.5100 - f1: 0.3992 - precision: 0.8099 - recall: 0.2668 - val_loss: 1.2069 - val_accuracy: 0.4733 - val_f1: 0.3703 - val_precision: 0.6981 - val_recall: 0.2525\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.9816 - accuracy: 0.5587 - f1: 0.4476 - precision: 0.8395 - recall: 0.3092 - val_loss: 1.1890 - val_accuracy: 0.4708 - val_f1: 0.3050 - val_precision: 0.7594 - val_recall: 0.1917\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.9389 - accuracy: 0.5709 - f1: 0.4670 - precision: 0.8630 - recall: 0.3249 - val_loss: 1.1973 - val_accuracy: 0.4992 - val_f1: 0.3813 - val_precision: 0.6897 - val_recall: 0.2642\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.9031 - accuracy: 0.5960 - f1: 0.5074 - precision: 0.8591 - recall: 0.3631 - val_loss: 1.1880 - val_accuracy: 0.5158 - val_f1: 0.4176 - val_precision: 0.6981 - val_recall: 0.2983\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.8633 - accuracy: 0.6213 - f1: 0.5270 - precision: 0.8698 - recall: 0.3820 - val_loss: 1.1814 - val_accuracy: 0.5033 - val_f1: 0.3904 - val_precision: 0.7217 - val_recall: 0.2683\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.8512 - accuracy: 0.6240 - f1: 0.5280 - precision: 0.8931 - recall: 0.3788 - val_loss: 1.1946 - val_accuracy: 0.5058 - val_f1: 0.4190 - val_precision: 0.7099 - val_recall: 0.2983\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.8298 - accuracy: 0.6463 - f1: 0.5496 - precision: 0.8833 - recall: 0.4034 - val_loss: 1.1946 - val_accuracy: 0.5158 - val_f1: 0.4259 - val_precision: 0.7215 - val_recall: 0.3033\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.8445 - accuracy: 0.6273 - f1: 0.5413 - precision: 0.8790 - recall: 0.3955 - val_loss: 1.1532 - val_accuracy: 0.5200 - val_f1: 0.4248 - val_precision: 0.7273 - val_recall: 0.3008\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.7967 - accuracy: 0.6411 - f1: 0.5611 - precision: 0.8909 - recall: 0.4125 - val_loss: 1.1765 - val_accuracy: 0.5175 - val_f1: 0.4320 - val_precision: 0.7646 - val_recall: 0.3025\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.7385 - accuracy: 0.6910 - f1: 0.5966 - precision: 0.9014 - recall: 0.4480 - val_loss: 1.2073 - val_accuracy: 0.5367 - val_f1: 0.4676 - val_precision: 0.6732 - val_recall: 0.3592\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.6990 - accuracy: 0.7224 - f1: 0.6474 - precision: 0.8793 - recall: 0.5156 - val_loss: 1.2121 - val_accuracy: 0.5375 - val_f1: 0.5063 - val_precision: 0.6147 - val_recall: 0.4317\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.7039 - accuracy: 0.7200 - f1: 0.6611 - precision: 0.8303 - recall: 0.5515 - val_loss: 1.2015 - val_accuracy: 0.5183 - val_f1: 0.4933 - val_precision: 0.6002 - val_recall: 0.4200\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.6776 - accuracy: 0.7244 - f1: 0.6823 - precision: 0.8220 - recall: 0.5850 - val_loss: 1.2294 - val_accuracy: 0.5292 - val_f1: 0.4917 - val_precision: 0.5887 - val_recall: 0.4225\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.6687 - accuracy: 0.7271 - f1: 0.6921 - precision: 0.7995 - recall: 0.6123 - val_loss: 1.2621 - val_accuracy: 0.4833 - val_f1: 0.4347 - val_precision: 0.5164 - val_recall: 0.3758\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.7203 - accuracy: 0.7113 - f1: 0.6811 - precision: 0.7772 - recall: 0.6074 - val_loss: 1.2335 - val_accuracy: 0.5158 - val_f1: 0.4913 - val_precision: 0.5721 - val_recall: 0.4308\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.7399 - accuracy: 0.6741 - f1: 0.6454 - precision: 0.7354 - recall: 0.5777 - val_loss: 1.3340 - val_accuracy: 0.4533 - val_f1: 0.4522 - val_precision: 0.5614 - val_recall: 0.3792\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.7615 - accuracy: 0.6700 - f1: 0.6205 - precision: 0.7809 - recall: 0.5204 - val_loss: 1.3369 - val_accuracy: 0.4742 - val_f1: 0.4604 - val_precision: 0.5867 - val_recall: 0.3792\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.7015 - accuracy: 0.6972 - f1: 0.6507 - precision: 0.7811 - recall: 0.5607 - val_loss: 1.3392 - val_accuracy: 0.5008 - val_f1: 0.5007 - val_precision: 0.5451 - val_recall: 0.4633\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.6137 - accuracy: 0.7310 - f1: 0.7063 - precision: 0.7929 - recall: 0.6391 - val_loss: 1.3469 - val_accuracy: 0.4933 - val_f1: 0.4854 - val_precision: 0.5240 - val_recall: 0.4525\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.5631 - accuracy: 0.7684 - f1: 0.7502 - precision: 0.8127 - recall: 0.6977 - val_loss: 1.3464 - val_accuracy: 0.5275 - val_f1: 0.5218 - val_precision: 0.5492 - val_recall: 0.4975\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.5215 - accuracy: 0.8105 - f1: 0.7990 - precision: 0.8428 - recall: 0.7601 - val_loss: 1.3908 - val_accuracy: 0.5200 - val_f1: 0.5088 - val_precision: 0.5323 - val_recall: 0.4875\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.5173 - accuracy: 0.8077 - f1: 0.7977 - precision: 0.8291 - recall: 0.7691 - val_loss: 1.3511 - val_accuracy: 0.5342 - val_f1: 0.5205 - val_precision: 0.5532 - val_recall: 0.4917\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.5126 - accuracy: 0.8161 - f1: 0.8082 - precision: 0.8387 - recall: 0.7804 - val_loss: 1.5066 - val_accuracy: 0.5167 - val_f1: 0.5108 - val_precision: 0.5231 - val_recall: 0.4992\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.4785 - accuracy: 0.8349 - f1: 0.8339 - precision: 0.8467 - recall: 0.8215 - val_loss: 1.4286 - val_accuracy: 0.5158 - val_f1: 0.5119 - val_precision: 0.5254 - val_recall: 0.4992\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.4674 - accuracy: 0.8476 - f1: 0.8450 - precision: 0.8612 - recall: 0.8296 - val_loss: 1.5347 - val_accuracy: 0.5117 - val_f1: 0.5104 - val_precision: 0.5204 - val_recall: 0.5008\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.4385 - accuracy: 0.8549 - f1: 0.8547 - precision: 0.8650 - recall: 0.8447 - val_loss: 1.6517 - val_accuracy: 0.4783 - val_f1: 0.4757 - val_precision: 0.4833 - val_recall: 0.4683\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.5476 - accuracy: 0.8008 - f1: 0.7987 - precision: 0.8139 - recall: 0.7843 - val_loss: 1.7582 - val_accuracy: 0.4917 - val_f1: 0.4930 - val_precision: 0.5004 - val_recall: 0.4858\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 5s 116ms/step - loss: 0.5266 - accuracy: 0.8071 - f1: 0.8019 - precision: 0.8141 - recall: 0.7902 - val_loss: 1.5019 - val_accuracy: 0.5333 - val_f1: 0.5303 - val_precision: 0.5431 - val_recall: 0.5183\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.5222 - accuracy: 0.8192 - f1: 0.8184 - precision: 0.8294 - recall: 0.8078 - val_loss: 1.6006 - val_accuracy: 0.5250 - val_f1: 0.5201 - val_precision: 0.5289 - val_recall: 0.5117\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.6102 - accuracy: 0.7815 - f1: 0.7772 - precision: 0.7906 - recall: 0.7645 - val_loss: 1.5548 - val_accuracy: 0.5392 - val_f1: 0.5387 - val_precision: 0.5435 - val_recall: 0.5342\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.6478 - accuracy: 0.7856 - f1: 0.7825 - precision: 0.7972 - recall: 0.7685 - val_loss: 1.3821 - val_accuracy: 0.5617 - val_f1: 0.5618 - val_precision: 0.5707 - val_recall: 0.5533\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.4979 - accuracy: 0.8401 - f1: 0.8407 - precision: 0.8526 - recall: 0.8292 - val_loss: 1.4127 - val_accuracy: 0.5617 - val_f1: 0.5617 - val_precision: 0.5731 - val_recall: 0.5508\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.4458 - accuracy: 0.8671 - f1: 0.8660 - precision: 0.8760 - recall: 0.8563 - val_loss: 1.4149 - val_accuracy: 0.5775 - val_f1: 0.5751 - val_precision: 0.5812 - val_recall: 0.5692\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.4661 - accuracy: 0.8495 - f1: 0.8496 - precision: 0.8584 - recall: 0.8411 - val_loss: 1.3362 - val_accuracy: 0.5742 - val_f1: 0.5688 - val_precision: 0.5779 - val_recall: 0.5600\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.4028 - accuracy: 0.8739 - f1: 0.8746 - precision: 0.8850 - recall: 0.8645 - val_loss: 1.2723 - val_accuracy: 0.6175 - val_f1: 0.6178 - val_precision: 0.6267 - val_recall: 0.6092\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.4094 - accuracy: 0.8747 - f1: 0.8742 - precision: 0.8830 - recall: 0.8657 - val_loss: 1.2927 - val_accuracy: 0.5975 - val_f1: 0.5938 - val_precision: 0.6021 - val_recall: 0.5858\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.4441 - accuracy: 0.8522 - f1: 0.8501 - precision: 0.8608 - recall: 0.8398 - val_loss: 1.3003 - val_accuracy: 0.5867 - val_f1: 0.5881 - val_precision: 0.5948 - val_recall: 0.5817\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.3468 - accuracy: 0.8966 - f1: 0.8976 - precision: 0.9043 - recall: 0.8910 - val_loss: 1.2991 - val_accuracy: 0.6017 - val_f1: 0.6026 - val_precision: 0.6122 - val_recall: 0.5933\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.3240 - accuracy: 0.9101 - f1: 0.9095 - precision: 0.9145 - recall: 0.9046 - val_loss: 1.3543 - val_accuracy: 0.5875 - val_f1: 0.5883 - val_precision: 0.5978 - val_recall: 0.5792\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.3002 - accuracy: 0.9191 - f1: 0.9193 - precision: 0.9232 - recall: 0.9156 - val_loss: 1.4225 - val_accuracy: 0.5958 - val_f1: 0.5949 - val_precision: 0.5990 - val_recall: 0.5908\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 5s 118ms/step - loss: 0.2851 - accuracy: 0.9270 - f1: 0.9272 - precision: 0.9309 - recall: 0.9237 - val_loss: 1.5242 - val_accuracy: 0.5825 - val_f1: 0.5876 - val_precision: 0.5955 - val_recall: 0.5800\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 5s 119ms/step - loss: 0.2980 - accuracy: 0.9120 - f1: 0.9132 - precision: 0.9183 - recall: 0.9083 - val_loss: 1.4242 - val_accuracy: 0.5767 - val_f1: 0.5783 - val_precision: 0.5878 - val_recall: 0.5692\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.3249 - accuracy: 0.9052 - f1: 0.9053 - precision: 0.9121 - recall: 0.8986 - val_loss: 1.4263 - val_accuracy: 0.5725 - val_f1: 0.5724 - val_precision: 0.5810 - val_recall: 0.5642\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.4066 - accuracy: 0.8745 - f1: 0.8753 - precision: 0.8848 - recall: 0.8660 - val_loss: 1.3023 - val_accuracy: 0.5708 - val_f1: 0.5676 - val_precision: 0.5790 - val_recall: 0.5567\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 5s 117ms/step - loss: 0.3514 - accuracy: 0.8925 - f1: 0.8902 - precision: 0.8983 - recall: 0.8823 - val_loss: 1.4440 - val_accuracy: 0.5942 - val_f1: 0.5963 - val_precision: 0.6064 - val_recall: 0.5867\n",
            "14/14 [==============================] - 1s 87ms/step - loss: 1.5382 - accuracy: 0.5700 - f1: 0.5651 - precision: 0.5750 - recall: 0.5557\n",
            "[1.5381627082824707, 0.5699999928474426, 0.5651162266731262, 0.5749503374099731, 0.5557142496109009]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01yVMiF41ZtT",
        "outputId": "0ad6ca98-f50a-471a-ea78-35b1d1852b97"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 10s 145ms/step - loss: 1.3830 - accuracy: 0.2768 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3615 - val_accuracy: 0.3483 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 1.3434 - accuracy: 0.3524 - f1: 0.0046 - precision: 0.1196 - recall: 0.0024 - val_loss: 1.3157 - val_accuracy: 0.3900 - val_f1: 0.0526 - val_precision: 0.6917 - val_recall: 0.0275\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 1.2474 - accuracy: 0.4400 - f1: 0.1695 - precision: 0.7100 - recall: 0.0998 - val_loss: 1.2385 - val_accuracy: 0.4375 - val_f1: 0.2743 - val_precision: 0.5665 - val_recall: 0.1817\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 1.1136 - accuracy: 0.5286 - f1: 0.3922 - precision: 0.7217 - recall: 0.2711 - val_loss: 1.1597 - val_accuracy: 0.5008 - val_f1: 0.3910 - val_precision: 0.6299 - val_recall: 0.2842\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.9976 - accuracy: 0.5982 - f1: 0.5165 - precision: 0.7513 - recall: 0.3954 - val_loss: 1.1483 - val_accuracy: 0.5425 - val_f1: 0.4712 - val_precision: 0.6297 - val_recall: 0.3775\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.8873 - accuracy: 0.6500 - f1: 0.5947 - precision: 0.7819 - recall: 0.4819 - val_loss: 1.1246 - val_accuracy: 0.5558 - val_f1: 0.5077 - val_precision: 0.6522 - val_recall: 0.4167\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.7945 - accuracy: 0.7041 - f1: 0.6583 - precision: 0.8107 - recall: 0.5550 - val_loss: 1.1256 - val_accuracy: 0.5817 - val_f1: 0.5419 - val_precision: 0.6696 - val_recall: 0.4558\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.7069 - accuracy: 0.7365 - f1: 0.7059 - precision: 0.8368 - recall: 0.6117 - val_loss: 1.1109 - val_accuracy: 0.5967 - val_f1: 0.5712 - val_precision: 0.6732 - val_recall: 0.4967\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.6215 - accuracy: 0.7693 - f1: 0.7502 - precision: 0.8529 - recall: 0.6704 - val_loss: 1.1436 - val_accuracy: 0.5908 - val_f1: 0.5719 - val_precision: 0.6654 - val_recall: 0.5017\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.5755 - accuracy: 0.7896 - f1: 0.7829 - precision: 0.8679 - recall: 0.7136 - val_loss: 1.1751 - val_accuracy: 0.5975 - val_f1: 0.5870 - val_precision: 0.6470 - val_recall: 0.5375\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.5188 - accuracy: 0.8196 - f1: 0.8043 - precision: 0.8833 - recall: 0.7392 - val_loss: 1.2154 - val_accuracy: 0.6142 - val_f1: 0.6034 - val_precision: 0.6615 - val_recall: 0.5550\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.4695 - accuracy: 0.8358 - f1: 0.8295 - precision: 0.8807 - recall: 0.7842 - val_loss: 1.2135 - val_accuracy: 0.6150 - val_f1: 0.6035 - val_precision: 0.6559 - val_recall: 0.5592\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.4344 - accuracy: 0.8434 - f1: 0.8383 - precision: 0.8924 - recall: 0.7910 - val_loss: 1.2280 - val_accuracy: 0.6167 - val_f1: 0.6151 - val_precision: 0.6638 - val_recall: 0.5733\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3808 - accuracy: 0.8662 - f1: 0.8693 - precision: 0.9121 - recall: 0.8307 - val_loss: 1.2850 - val_accuracy: 0.6258 - val_f1: 0.6184 - val_precision: 0.6617 - val_recall: 0.5808\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 6s 126ms/step - loss: 0.3452 - accuracy: 0.8786 - f1: 0.8816 - precision: 0.9211 - recall: 0.8458 - val_loss: 1.2381 - val_accuracy: 0.6342 - val_f1: 0.6325 - val_precision: 0.6763 - val_recall: 0.5942\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.3049 - accuracy: 0.8927 - f1: 0.8904 - precision: 0.9167 - recall: 0.8657 - val_loss: 1.3270 - val_accuracy: 0.6150 - val_f1: 0.6072 - val_precision: 0.6445 - val_recall: 0.5742\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2760 - accuracy: 0.9047 - f1: 0.9047 - precision: 0.9319 - recall: 0.8792 - val_loss: 1.3437 - val_accuracy: 0.6375 - val_f1: 0.6376 - val_precision: 0.6671 - val_recall: 0.6108\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2742 - accuracy: 0.9025 - f1: 0.8996 - precision: 0.9271 - recall: 0.8739 - val_loss: 1.3101 - val_accuracy: 0.6325 - val_f1: 0.6289 - val_precision: 0.6550 - val_recall: 0.6050\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 6s 125ms/step - loss: 0.2426 - accuracy: 0.9167 - f1: 0.9157 - precision: 0.9373 - recall: 0.8952 - val_loss: 1.4033 - val_accuracy: 0.6350 - val_f1: 0.6333 - val_precision: 0.6646 - val_recall: 0.6050\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.2208 - accuracy: 0.9216 - f1: 0.9230 - precision: 0.9412 - recall: 0.9055 - val_loss: 1.5134 - val_accuracy: 0.6242 - val_f1: 0.6236 - val_precision: 0.6455 - val_recall: 0.6033\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2436 - accuracy: 0.9099 - f1: 0.9126 - precision: 0.9325 - recall: 0.8938 - val_loss: 1.4894 - val_accuracy: 0.6300 - val_f1: 0.6298 - val_precision: 0.6533 - val_recall: 0.6083\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.2186 - accuracy: 0.9221 - f1: 0.9214 - precision: 0.9406 - recall: 0.9032 - val_loss: 1.6141 - val_accuracy: 0.6442 - val_f1: 0.6443 - val_precision: 0.6630 - val_recall: 0.6267\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1985 - accuracy: 0.9292 - f1: 0.9302 - precision: 0.9440 - recall: 0.9170 - val_loss: 1.6135 - val_accuracy: 0.6275 - val_f1: 0.6304 - val_precision: 0.6478 - val_recall: 0.6142\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1696 - accuracy: 0.9410 - f1: 0.9411 - precision: 0.9552 - recall: 0.9275 - val_loss: 1.7082 - val_accuracy: 0.6375 - val_f1: 0.6363 - val_precision: 0.6499 - val_recall: 0.6233\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1447 - accuracy: 0.9472 - f1: 0.9481 - precision: 0.9591 - recall: 0.9375 - val_loss: 1.8090 - val_accuracy: 0.6317 - val_f1: 0.6310 - val_precision: 0.6435 - val_recall: 0.6192\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1369 - accuracy: 0.9499 - f1: 0.9498 - precision: 0.9600 - recall: 0.9400 - val_loss: 1.7930 - val_accuracy: 0.6392 - val_f1: 0.6430 - val_precision: 0.6577 - val_recall: 0.6292\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1203 - accuracy: 0.9578 - f1: 0.9576 - precision: 0.9650 - recall: 0.9505 - val_loss: 1.8425 - val_accuracy: 0.6342 - val_f1: 0.6343 - val_precision: 0.6477 - val_recall: 0.6217\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1199 - accuracy: 0.9589 - f1: 0.9586 - precision: 0.9675 - recall: 0.9500 - val_loss: 1.9372 - val_accuracy: 0.6417 - val_f1: 0.6411 - val_precision: 0.6536 - val_recall: 0.6292\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1379 - accuracy: 0.9532 - f1: 0.9522 - precision: 0.9591 - recall: 0.9456 - val_loss: 1.7766 - val_accuracy: 0.6275 - val_f1: 0.6258 - val_precision: 0.6381 - val_recall: 0.6142\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1626 - accuracy: 0.9474 - f1: 0.9462 - precision: 0.9567 - recall: 0.9360 - val_loss: 1.7375 - val_accuracy: 0.6292 - val_f1: 0.6314 - val_precision: 0.6455 - val_recall: 0.6183\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.1380 - accuracy: 0.9508 - f1: 0.9513 - precision: 0.9610 - recall: 0.9419 - val_loss: 1.9231 - val_accuracy: 0.6200 - val_f1: 0.6208 - val_precision: 0.6349 - val_recall: 0.6075\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1035 - accuracy: 0.9611 - f1: 0.9617 - precision: 0.9698 - recall: 0.9539 - val_loss: 2.0065 - val_accuracy: 0.6217 - val_f1: 0.6247 - val_precision: 0.6356 - val_recall: 0.6142\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1120 - accuracy: 0.9610 - f1: 0.9623 - precision: 0.9683 - recall: 0.9565 - val_loss: 1.9045 - val_accuracy: 0.6358 - val_f1: 0.6401 - val_precision: 0.6496 - val_recall: 0.6308\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.1011 - accuracy: 0.9609 - f1: 0.9605 - precision: 0.9647 - recall: 0.9564 - val_loss: 2.1792 - val_accuracy: 0.6217 - val_f1: 0.6221 - val_precision: 0.6278 - val_recall: 0.6167\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1075 - accuracy: 0.9600 - f1: 0.9593 - precision: 0.9642 - recall: 0.9545 - val_loss: 2.1107 - val_accuracy: 0.6217 - val_f1: 0.6231 - val_precision: 0.6307 - val_recall: 0.6158\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0960 - accuracy: 0.9635 - f1: 0.9639 - precision: 0.9690 - recall: 0.9589 - val_loss: 2.1971 - val_accuracy: 0.6267 - val_f1: 0.6266 - val_precision: 0.6379 - val_recall: 0.6158\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0858 - accuracy: 0.9653 - f1: 0.9664 - precision: 0.9705 - recall: 0.9624 - val_loss: 2.0763 - val_accuracy: 0.6350 - val_f1: 0.6333 - val_precision: 0.6410 - val_recall: 0.6258\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0881 - accuracy: 0.9692 - f1: 0.9701 - precision: 0.9741 - recall: 0.9662 - val_loss: 2.2691 - val_accuracy: 0.6242 - val_f1: 0.6280 - val_precision: 0.6344 - val_recall: 0.6217\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0838 - accuracy: 0.9687 - f1: 0.9678 - precision: 0.9718 - recall: 0.9639 - val_loss: 2.2517 - val_accuracy: 0.6250 - val_f1: 0.6260 - val_precision: 0.6323 - val_recall: 0.6200\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0897 - accuracy: 0.9692 - f1: 0.9695 - precision: 0.9742 - recall: 0.9648 - val_loss: 2.2194 - val_accuracy: 0.6367 - val_f1: 0.6361 - val_precision: 0.6443 - val_recall: 0.6283\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1054 - accuracy: 0.9620 - f1: 0.9628 - precision: 0.9669 - recall: 0.9588 - val_loss: 2.1185 - val_accuracy: 0.6417 - val_f1: 0.6424 - val_precision: 0.6518 - val_recall: 0.6333\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1389 - accuracy: 0.9513 - f1: 0.9516 - precision: 0.9570 - recall: 0.9462 - val_loss: 2.1841 - val_accuracy: 0.6267 - val_f1: 0.6279 - val_precision: 0.6370 - val_recall: 0.6192\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.1574 - accuracy: 0.9494 - f1: 0.9479 - precision: 0.9550 - recall: 0.9410 - val_loss: 2.1567 - val_accuracy: 0.6258 - val_f1: 0.6264 - val_precision: 0.6330 - val_recall: 0.6200\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.1236 - accuracy: 0.9554 - f1: 0.9551 - precision: 0.9601 - recall: 0.9502 - val_loss: 1.8828 - val_accuracy: 0.6333 - val_f1: 0.6375 - val_precision: 0.6470 - val_recall: 0.6283\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0907 - accuracy: 0.9684 - f1: 0.9675 - precision: 0.9717 - recall: 0.9633 - val_loss: 2.1148 - val_accuracy: 0.6400 - val_f1: 0.6417 - val_precision: 0.6487 - val_recall: 0.6350\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 6s 124ms/step - loss: 0.0815 - accuracy: 0.9662 - f1: 0.9671 - precision: 0.9701 - recall: 0.9641 - val_loss: 2.0253 - val_accuracy: 0.6467 - val_f1: 0.6508 - val_precision: 0.6593 - val_recall: 0.6425\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0684 - accuracy: 0.9704 - f1: 0.9699 - precision: 0.9721 - recall: 0.9677 - val_loss: 2.1592 - val_accuracy: 0.6350 - val_f1: 0.6385 - val_precision: 0.6481 - val_recall: 0.6292\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0688 - accuracy: 0.9683 - f1: 0.9681 - precision: 0.9725 - recall: 0.9638 - val_loss: 2.2387 - val_accuracy: 0.6292 - val_f1: 0.6309 - val_precision: 0.6387 - val_recall: 0.6233\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.0809 - accuracy: 0.9701 - f1: 0.9701 - precision: 0.9735 - recall: 0.9669 - val_loss: 2.2728 - val_accuracy: 0.6200 - val_f1: 0.6203 - val_precision: 0.6275 - val_recall: 0.6133\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.0998 - accuracy: 0.9590 - f1: 0.9592 - precision: 0.9647 - recall: 0.9539 - val_loss: 2.1740 - val_accuracy: 0.6408 - val_f1: 0.6436 - val_precision: 0.6533 - val_recall: 0.6342\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 5s 120ms/step - loss: 0.0814 - accuracy: 0.9648 - f1: 0.9660 - precision: 0.9688 - recall: 0.9632 - val_loss: 2.1834 - val_accuracy: 0.6442 - val_f1: 0.6438 - val_precision: 0.6528 - val_recall: 0.6350\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 6s 123ms/step - loss: 0.0902 - accuracy: 0.9667 - f1: 0.9658 - precision: 0.9678 - recall: 0.9637 - val_loss: 2.0898 - val_accuracy: 0.6483 - val_f1: 0.6509 - val_precision: 0.6587 - val_recall: 0.6433\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0762 - accuracy: 0.9706 - f1: 0.9709 - precision: 0.9728 - recall: 0.9690 - val_loss: 2.2303 - val_accuracy: 0.6275 - val_f1: 0.6310 - val_precision: 0.6406 - val_recall: 0.6217\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0771 - accuracy: 0.9714 - f1: 0.9716 - precision: 0.9732 - recall: 0.9699 - val_loss: 2.3016 - val_accuracy: 0.6325 - val_f1: 0.6339 - val_precision: 0.6396 - val_recall: 0.6283\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0726 - accuracy: 0.9696 - f1: 0.9687 - precision: 0.9712 - recall: 0.9663 - val_loss: 2.2300 - val_accuracy: 0.6425 - val_f1: 0.6377 - val_precision: 0.6457 - val_recall: 0.6300\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0630 - accuracy: 0.9725 - f1: 0.9725 - precision: 0.9742 - recall: 0.9708 - val_loss: 2.2649 - val_accuracy: 0.6400 - val_f1: 0.6417 - val_precision: 0.6477 - val_recall: 0.6358\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0592 - accuracy: 0.9725 - f1: 0.9731 - precision: 0.9752 - recall: 0.9710 - val_loss: 2.3474 - val_accuracy: 0.6333 - val_f1: 0.6369 - val_precision: 0.6457 - val_recall: 0.6283\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0540 - accuracy: 0.9757 - f1: 0.9748 - precision: 0.9770 - recall: 0.9725 - val_loss: 2.3788 - val_accuracy: 0.6350 - val_f1: 0.6351 - val_precision: 0.6394 - val_recall: 0.6308\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 6s 122ms/step - loss: 0.0509 - accuracy: 0.9779 - f1: 0.9776 - precision: 0.9793 - recall: 0.9760 - val_loss: 2.3154 - val_accuracy: 0.6367 - val_f1: 0.6381 - val_precision: 0.6464 - val_recall: 0.6300\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 6s 121ms/step - loss: 0.0567 - accuracy: 0.9735 - f1: 0.9735 - precision: 0.9762 - recall: 0.9708 - val_loss: 2.5093 - val_accuracy: 0.6392 - val_f1: 0.6392 - val_precision: 0.6452 - val_recall: 0.6333\n",
            "14/14 [==============================] - 1s 86ms/step - loss: 2.5528 - accuracy: 0.6314 - f1: 0.6300 - precision: 0.6381 - recall: 0.6221\n",
            "[2.5527875423431396, 0.631428599357605, 0.6299979090690613, 0.6381165385246277, 0.6221428513526917]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_G3Ff5r1bSD",
        "outputId": "0fa5c19c-95f8-47fc-bff5-0d27a159f372"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(cbow_train_gen,validation_data=cbow_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 9s 161ms/step - loss: 1.3910 - accuracy: 0.2500 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3728 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3800 - accuracy: 0.2954 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3707 - val_accuracy: 0.3300 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3779 - accuracy: 0.2977 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3695 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3724 - accuracy: 0.3098 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3715 - val_accuracy: 0.3242 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3727 - accuracy: 0.3067 - f1: 1.8683e-04 - precision: 0.0094 - recall: 9.4349e-05 - val_loss: 1.3677 - val_accuracy: 0.3167 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3718 - accuracy: 0.3278 - f1: 0.0067 - precision: 0.1438 - recall: 0.0035 - val_loss: 1.3740 - val_accuracy: 0.3167 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3643 - accuracy: 0.3163 - f1: 0.0047 - precision: 0.1103 - recall: 0.0024 - val_loss: 1.3721 - val_accuracy: 0.3258 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.3708 - accuracy: 0.3086 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3748 - val_accuracy: 0.3058 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.3616 - accuracy: 0.3303 - f1: 0.0016 - precision: 0.0457 - recall: 8.3590e-04 - val_loss: 1.3673 - val_accuracy: 0.3225 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3676 - accuracy: 0.3373 - f1: 0.0020 - precision: 0.0599 - recall: 0.0010 - val_loss: 1.3699 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.3666 - accuracy: 0.3165 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3764 - val_accuracy: 0.2942 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.3562 - accuracy: 0.3355 - f1: 0.0035 - precision: 0.0972 - recall: 0.0018 - val_loss: 1.4027 - val_accuracy: 0.2975 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.3554 - accuracy: 0.3379 - f1: 0.0046 - precision: 0.1585 - recall: 0.0023 - val_loss: 1.3667 - val_accuracy: 0.3058 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3456 - accuracy: 0.3453 - f1: 0.0168 - precision: 0.1526 - recall: 0.0092 - val_loss: 1.3660 - val_accuracy: 0.3267 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.3297 - accuracy: 0.3865 - f1: 0.0246 - precision: 0.2462 - recall: 0.0134 - val_loss: 1.3764 - val_accuracy: 0.3167 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3815 - accuracy: 0.2969 - f1: 1.2288e-04 - precision: 0.0062 - recall: 6.2052e-05 - val_loss: 1.3703 - val_accuracy: 0.3258 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 1.3263 - accuracy: 0.3735 - f1: 0.0165 - precision: 0.1653 - recall: 0.0088 - val_loss: 1.3451 - val_accuracy: 0.3500 - val_f1: 0.0017 - val_precision: 0.0833 - val_recall: 8.3333e-04\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 1.2701 - accuracy: 0.4153 - f1: 0.0582 - precision: 0.6104 - recall: 0.0312 - val_loss: 1.3806 - val_accuracy: 0.3292 - val_f1: 0.0946 - val_precision: 0.3816 - val_recall: 0.0542\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.2839 - accuracy: 0.3972 - f1: 0.1496 - precision: 0.5053 - recall: 0.0901 - val_loss: 1.3664 - val_accuracy: 0.3258 - val_f1: 0.0032 - val_precision: 0.0556 - val_recall: 0.0017\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.2866 - accuracy: 0.3975 - f1: 0.0702 - precision: 0.4716 - recall: 0.0387 - val_loss: 1.3553 - val_accuracy: 0.3325 - val_f1: 0.0746 - val_precision: 0.4009 - val_recall: 0.0417\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 1.2721 - accuracy: 0.4141 - f1: 0.1144 - precision: 0.5353 - recall: 0.0649 - val_loss: 1.3938 - val_accuracy: 0.3158 - val_f1: 0.0512 - val_precision: 0.3641 - val_recall: 0.0283\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.3516 - accuracy: 0.3372 - f1: 0.0365 - precision: 0.3256 - recall: 0.0199 - val_loss: 1.3838 - val_accuracy: 0.3225 - val_f1: 0.0800 - val_precision: 0.3676 - val_recall: 0.0450\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.2568 - accuracy: 0.4206 - f1: 0.1498 - precision: 0.5774 - recall: 0.0871 - val_loss: 1.4119 - val_accuracy: 0.2925 - val_f1: 0.1291 - val_precision: 0.3577 - val_recall: 0.0792\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.2516 - accuracy: 0.4261 - f1: 0.1869 - precision: 0.5500 - recall: 0.1141 - val_loss: 1.4042 - val_accuracy: 0.3283 - val_f1: 0.1285 - val_precision: 0.3866 - val_recall: 0.0775\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 7s 162ms/step - loss: 1.1782 - accuracy: 0.4884 - f1: 0.2303 - precision: 0.6319 - recall: 0.1418 - val_loss: 1.4316 - val_accuracy: 0.3342 - val_f1: 0.2117 - val_precision: 0.3792 - val_recall: 0.1475\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.1420 - accuracy: 0.4994 - f1: 0.3458 - precision: 0.6227 - recall: 0.2408 - val_loss: 1.4711 - val_accuracy: 0.3383 - val_f1: 0.2264 - val_precision: 0.3762 - val_recall: 0.1625\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.1649 - accuracy: 0.4924 - f1: 0.3417 - precision: 0.5827 - recall: 0.2443 - val_loss: 1.4484 - val_accuracy: 0.3258 - val_f1: 0.2383 - val_precision: 0.3649 - val_recall: 0.1775\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.1992 - accuracy: 0.4710 - f1: 0.3351 - precision: 0.5658 - recall: 0.2415 - val_loss: 1.4366 - val_accuracy: 0.3283 - val_f1: 0.1907 - val_precision: 0.3810 - val_recall: 0.1275\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.1524 - accuracy: 0.4884 - f1: 0.3391 - precision: 0.6295 - recall: 0.2344 - val_loss: 1.4891 - val_accuracy: 0.3308 - val_f1: 0.2274 - val_precision: 0.3773 - val_recall: 0.1633\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.1814 - accuracy: 0.4672 - f1: 0.3296 - precision: 0.5966 - recall: 0.2318 - val_loss: 1.5065 - val_accuracy: 0.3000 - val_f1: 0.1531 - val_precision: 0.3325 - val_recall: 0.1000\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.2254 - accuracy: 0.4475 - f1: 0.2948 - precision: 0.5740 - recall: 0.2009 - val_loss: 1.4495 - val_accuracy: 0.3058 - val_f1: 0.1276 - val_precision: 0.3329 - val_recall: 0.0792\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 1.2059 - accuracy: 0.4733 - f1: 0.2758 - precision: 0.5913 - recall: 0.1825 - val_loss: 1.4789 - val_accuracy: 0.3033 - val_f1: 0.1691 - val_precision: 0.3499 - val_recall: 0.1117\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 1.1267 - accuracy: 0.5234 - f1: 0.3736 - precision: 0.6474 - recall: 0.2650 - val_loss: 1.4831 - val_accuracy: 0.3375 - val_f1: 0.2569 - val_precision: 0.3692 - val_recall: 0.1975\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.0947 - accuracy: 0.5367 - f1: 0.4231 - precision: 0.6209 - recall: 0.3220 - val_loss: 1.4602 - val_accuracy: 0.3592 - val_f1: 0.2596 - val_precision: 0.4006 - val_recall: 0.1942\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.0545 - accuracy: 0.5514 - f1: 0.4646 - precision: 0.6553 - recall: 0.3611 - val_loss: 1.4917 - val_accuracy: 0.3058 - val_f1: 0.1552 - val_precision: 0.3254 - val_recall: 0.1025\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.2641 - accuracy: 0.4300 - f1: 0.2754 - precision: 0.5596 - recall: 0.1834 - val_loss: 1.4503 - val_accuracy: 0.3150 - val_f1: 0.1618 - val_precision: 0.3476 - val_recall: 0.1058\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.1986 - accuracy: 0.4680 - f1: 0.2818 - precision: 0.6300 - recall: 0.1837 - val_loss: 1.4509 - val_accuracy: 0.3433 - val_f1: 0.1844 - val_precision: 0.3970 - val_recall: 0.1208\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.1078 - accuracy: 0.5235 - f1: 0.3947 - precision: 0.6649 - recall: 0.2832 - val_loss: 1.4792 - val_accuracy: 0.3617 - val_f1: 0.2544 - val_precision: 0.4062 - val_recall: 0.1858\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.1447 - accuracy: 0.5083 - f1: 0.3802 - precision: 0.6323 - recall: 0.2737 - val_loss: 1.4681 - val_accuracy: 0.3733 - val_f1: 0.2229 - val_precision: 0.3704 - val_recall: 0.1600\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.0745 - accuracy: 0.5522 - f1: 0.4387 - precision: 0.6721 - recall: 0.3272 - val_loss: 1.4739 - val_accuracy: 0.3358 - val_f1: 0.2263 - val_precision: 0.3645 - val_recall: 0.1650\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 1.0111 - accuracy: 0.5771 - f1: 0.4849 - precision: 0.7008 - recall: 0.3731 - val_loss: 1.5269 - val_accuracy: 0.3475 - val_f1: 0.2676 - val_precision: 0.3652 - val_recall: 0.2117\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.0360 - accuracy: 0.5712 - f1: 0.4905 - precision: 0.6752 - recall: 0.3867 - val_loss: 1.4883 - val_accuracy: 0.3392 - val_f1: 0.2394 - val_precision: 0.3551 - val_recall: 0.1817\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.0040 - accuracy: 0.5715 - f1: 0.5010 - precision: 0.6926 - recall: 0.3937 - val_loss: 1.5323 - val_accuracy: 0.3625 - val_f1: 0.2843 - val_precision: 0.3821 - val_recall: 0.2267\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.0069 - accuracy: 0.5830 - f1: 0.5068 - precision: 0.6797 - recall: 0.4055 - val_loss: 1.5263 - val_accuracy: 0.3342 - val_f1: 0.2677 - val_precision: 0.3900 - val_recall: 0.2042\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9126 - accuracy: 0.6305 - f1: 0.5730 - precision: 0.7301 - recall: 0.4737 - val_loss: 1.6098 - val_accuracy: 0.3433 - val_f1: 0.2911 - val_precision: 0.3754 - val_recall: 0.2383\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.8856 - accuracy: 0.6444 - f1: 0.6109 - precision: 0.7482 - recall: 0.5175 - val_loss: 1.6803 - val_accuracy: 0.3425 - val_f1: 0.3000 - val_precision: 0.3664 - val_recall: 0.2542\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9131 - accuracy: 0.6349 - f1: 0.6019 - precision: 0.7159 - recall: 0.5202 - val_loss: 1.6448 - val_accuracy: 0.3350 - val_f1: 0.2709 - val_precision: 0.3540 - val_recall: 0.2200\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 1.0257 - accuracy: 0.5685 - f1: 0.5171 - precision: 0.6615 - recall: 0.4256 - val_loss: 1.5375 - val_accuracy: 0.3333 - val_f1: 0.2441 - val_precision: 0.3543 - val_recall: 0.1867\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 1.0162 - accuracy: 0.5786 - f1: 0.5023 - precision: 0.6913 - recall: 0.3954 - val_loss: 1.5803 - val_accuracy: 0.3375 - val_f1: 0.2553 - val_precision: 0.3540 - val_recall: 0.2000\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 1.0606 - accuracy: 0.5541 - f1: 0.4874 - precision: 0.6632 - recall: 0.3870 - val_loss: 1.5430 - val_accuracy: 0.3217 - val_f1: 0.2451 - val_precision: 0.3553 - val_recall: 0.1875\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.9742 - accuracy: 0.6030 - f1: 0.5354 - precision: 0.7094 - recall: 0.4307 - val_loss: 1.5842 - val_accuracy: 0.3075 - val_f1: 0.2605 - val_precision: 0.3569 - val_recall: 0.2058\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9762 - accuracy: 0.6133 - f1: 0.5645 - precision: 0.7157 - recall: 0.4668 - val_loss: 1.5952 - val_accuracy: 0.3142 - val_f1: 0.2742 - val_precision: 0.3622 - val_recall: 0.2208\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 1.0078 - accuracy: 0.5840 - f1: 0.5269 - precision: 0.6711 - recall: 0.4345 - val_loss: 1.5200 - val_accuracy: 0.3467 - val_f1: 0.2775 - val_precision: 0.3819 - val_recall: 0.2183\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9281 - accuracy: 0.6243 - f1: 0.5854 - precision: 0.7306 - recall: 0.4899 - val_loss: 1.5410 - val_accuracy: 0.3442 - val_f1: 0.2646 - val_precision: 0.3568 - val_recall: 0.2108\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.8957 - accuracy: 0.6457 - f1: 0.5938 - precision: 0.7382 - recall: 0.4986 - val_loss: 1.5854 - val_accuracy: 0.3383 - val_f1: 0.2761 - val_precision: 0.3620 - val_recall: 0.2233\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 0.8789 - accuracy: 0.6462 - f1: 0.6018 - precision: 0.7284 - recall: 0.5138 - val_loss: 1.6198 - val_accuracy: 0.3550 - val_f1: 0.2943 - val_precision: 0.3705 - val_recall: 0.2442\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 0.9006 - accuracy: 0.6343 - f1: 0.5991 - precision: 0.7171 - recall: 0.5153 - val_loss: 1.5779 - val_accuracy: 0.3458 - val_f1: 0.2647 - val_precision: 0.3662 - val_recall: 0.2075\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 7s 156ms/step - loss: 0.9736 - accuracy: 0.6065 - f1: 0.5487 - precision: 0.6887 - recall: 0.4570 - val_loss: 1.5408 - val_accuracy: 0.3675 - val_f1: 0.2857 - val_precision: 0.3831 - val_recall: 0.2283\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9742 - accuracy: 0.6194 - f1: 0.5483 - precision: 0.7053 - recall: 0.4498 - val_loss: 1.5496 - val_accuracy: 0.3475 - val_f1: 0.2717 - val_precision: 0.3781 - val_recall: 0.2133\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9721 - accuracy: 0.6094 - f1: 0.5420 - precision: 0.7176 - recall: 0.4368 - val_loss: 1.5773 - val_accuracy: 0.3400 - val_f1: 0.2806 - val_precision: 0.3735 - val_recall: 0.2250\n",
            "14/14 [==============================] - 1s 85ms/step - loss: 1.5882 - accuracy: 0.3379 - f1: 0.2769 - precision: 0.3728 - recall: 0.2207\n",
            "[1.588218331336975, 0.3378571569919586, 0.2769269645214081, 0.37279874086380005, 0.22071430087089539]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czoFmEwK1dWT",
        "outputId": "99aa004a-4b5f-4276-d4b2-dbac609e4787"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(64))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "glove_train_gen=glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size)\r\n",
        "glove_val_gen=glove_gen(vocab_glove,X_val,y_val,batch_size,Max_input_size,Embedding_size)\r\n",
        "history_glove_lstm = model_glove.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 4s 43ms/step - loss: 1.3756 - accuracy: 0.3153 - f1: 0.0148 - precision: 0.0646 - recall: 0.0088 - val_loss: 1.2618 - val_accuracy: 0.4292 - val_f1: 0.2256 - val_precision: 0.7417 - val_recall: 0.1333\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 1.2312 - accuracy: 0.4300 - f1: 0.2654 - precision: 0.6724 - recall: 0.1684 - val_loss: 1.1901 - val_accuracy: 0.4733 - val_f1: 0.4556 - val_precision: 0.6147 - val_recall: 0.3625\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 1.1158 - accuracy: 0.4837 - f1: 0.3970 - precision: 0.7034 - recall: 0.2906 - val_loss: 1.0966 - val_accuracy: 0.4942 - val_f1: 0.4712 - val_precision: 0.6431 - val_recall: 0.3725\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.9549 - accuracy: 0.5660 - f1: 0.4838 - precision: 0.7637 - recall: 0.3597 - val_loss: 1.1100 - val_accuracy: 0.5017 - val_f1: 0.4384 - val_precision: 0.6702 - val_recall: 0.3267\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.8744 - accuracy: 0.6423 - f1: 0.5157 - precision: 0.8222 - recall: 0.3803 - val_loss: 1.0005 - val_accuracy: 0.5592 - val_f1: 0.4266 - val_precision: 0.8120 - val_recall: 0.2900\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.8475 - accuracy: 0.6325 - f1: 0.5236 - precision: 0.7852 - recall: 0.3985 - val_loss: 1.1314 - val_accuracy: 0.6125 - val_f1: 0.5979 - val_precision: 0.6693 - val_recall: 0.5408\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.7262 - accuracy: 0.7128 - f1: 0.6721 - precision: 0.7823 - recall: 0.5914 - val_loss: 1.1879 - val_accuracy: 0.6083 - val_f1: 0.6032 - val_precision: 0.6430 - val_recall: 0.5683\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.6727 - accuracy: 0.7407 - f1: 0.7175 - precision: 0.7714 - recall: 0.6712 - val_loss: 1.1423 - val_accuracy: 0.6042 - val_f1: 0.5999 - val_precision: 0.6262 - val_recall: 0.5758\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.6478 - accuracy: 0.7587 - f1: 0.7368 - precision: 0.7988 - recall: 0.6849 - val_loss: 1.2096 - val_accuracy: 0.6133 - val_f1: 0.6148 - val_precision: 0.6260 - val_recall: 0.6042\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.5923 - accuracy: 0.7739 - f1: 0.7691 - precision: 0.7935 - recall: 0.7465 - val_loss: 1.3657 - val_accuracy: 0.6092 - val_f1: 0.6050 - val_precision: 0.6119 - val_recall: 0.5983\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.6239 - accuracy: 0.7677 - f1: 0.7604 - precision: 0.7763 - recall: 0.7454 - val_loss: 1.2674 - val_accuracy: 0.5550 - val_f1: 0.5547 - val_precision: 0.5712 - val_recall: 0.5392\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.5769 - accuracy: 0.7884 - f1: 0.7883 - precision: 0.8052 - recall: 0.7723 - val_loss: 1.1572 - val_accuracy: 0.6000 - val_f1: 0.5986 - val_precision: 0.6066 - val_recall: 0.5908\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.4981 - accuracy: 0.8351 - f1: 0.8338 - precision: 0.8411 - recall: 0.8267 - val_loss: 1.0994 - val_accuracy: 0.6675 - val_f1: 0.6689 - val_precision: 0.6745 - val_recall: 0.6633\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.4009 - accuracy: 0.8745 - f1: 0.8734 - precision: 0.8793 - recall: 0.8676 - val_loss: 1.2389 - val_accuracy: 0.6500 - val_f1: 0.6521 - val_precision: 0.6567 - val_recall: 0.6475\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.4398 - accuracy: 0.8586 - f1: 0.8571 - precision: 0.8668 - recall: 0.8478 - val_loss: 1.3626 - val_accuracy: 0.6392 - val_f1: 0.6390 - val_precision: 0.6457 - val_recall: 0.6325\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.4546 - accuracy: 0.8567 - f1: 0.8566 - precision: 0.8650 - recall: 0.8484 - val_loss: 1.1451 - val_accuracy: 0.6458 - val_f1: 0.6531 - val_precision: 0.6707 - val_recall: 0.6367\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.4961 - accuracy: 0.8396 - f1: 0.8406 - precision: 0.8566 - recall: 0.8254 - val_loss: 0.8820 - val_accuracy: 0.7308 - val_f1: 0.7323 - val_precision: 0.7591 - val_recall: 0.7075\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.3465 - accuracy: 0.8961 - f1: 0.8937 - precision: 0.9056 - recall: 0.8822 - val_loss: 0.9408 - val_accuracy: 0.7250 - val_f1: 0.7258 - val_precision: 0.7417 - val_recall: 0.7108\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2789 - accuracy: 0.9241 - f1: 0.9211 - precision: 0.9276 - recall: 0.9146 - val_loss: 0.9649 - val_accuracy: 0.7317 - val_f1: 0.7346 - val_precision: 0.7437 - val_recall: 0.7258\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2409 - accuracy: 0.9345 - f1: 0.9346 - precision: 0.9369 - recall: 0.9323 - val_loss: 1.0421 - val_accuracy: 0.7250 - val_f1: 0.7273 - val_precision: 0.7384 - val_recall: 0.7167\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 28ms/step - loss: 0.2284 - accuracy: 0.9401 - f1: 0.9390 - precision: 0.9434 - recall: 0.9347 - val_loss: 1.0943 - val_accuracy: 0.7142 - val_f1: 0.7118 - val_precision: 0.7205 - val_recall: 0.7033\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2276 - accuracy: 0.9393 - f1: 0.9393 - precision: 0.9438 - recall: 0.9348 - val_loss: 1.0879 - val_accuracy: 0.7333 - val_f1: 0.7297 - val_precision: 0.7407 - val_recall: 0.7192\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2092 - accuracy: 0.9436 - f1: 0.9444 - precision: 0.9474 - recall: 0.9415 - val_loss: 1.1323 - val_accuracy: 0.7325 - val_f1: 0.7330 - val_precision: 0.7420 - val_recall: 0.7242\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2023 - accuracy: 0.9500 - f1: 0.9488 - precision: 0.9518 - recall: 0.9459 - val_loss: 1.2075 - val_accuracy: 0.7317 - val_f1: 0.7334 - val_precision: 0.7403 - val_recall: 0.7267\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1952 - accuracy: 0.9538 - f1: 0.9535 - precision: 0.9548 - recall: 0.9522 - val_loss: 1.3658 - val_accuracy: 0.7025 - val_f1: 0.7039 - val_precision: 0.7104 - val_recall: 0.6975\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.2194 - accuracy: 0.9477 - f1: 0.9474 - precision: 0.9503 - recall: 0.9445 - val_loss: 1.1396 - val_accuracy: 0.7283 - val_f1: 0.7298 - val_precision: 0.7418 - val_recall: 0.7183\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2760 - accuracy: 0.9233 - f1: 0.9226 - precision: 0.9308 - recall: 0.9146 - val_loss: 1.1954 - val_accuracy: 0.7333 - val_f1: 0.7362 - val_precision: 0.7445 - val_recall: 0.7283\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2187 - accuracy: 0.9419 - f1: 0.9417 - precision: 0.9460 - recall: 0.9374 - val_loss: 1.0748 - val_accuracy: 0.7342 - val_f1: 0.7363 - val_precision: 0.7515 - val_recall: 0.7217\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.2289 - accuracy: 0.9444 - f1: 0.9425 - precision: 0.9472 - recall: 0.9379 - val_loss: 1.0594 - val_accuracy: 0.7242 - val_f1: 0.7312 - val_precision: 0.7465 - val_recall: 0.7167\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.2037 - accuracy: 0.9472 - f1: 0.9470 - precision: 0.9516 - recall: 0.9424 - val_loss: 1.1266 - val_accuracy: 0.7283 - val_f1: 0.7341 - val_precision: 0.7463 - val_recall: 0.7225\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1968 - accuracy: 0.9531 - f1: 0.9531 - precision: 0.9573 - recall: 0.9490 - val_loss: 1.0825 - val_accuracy: 0.7258 - val_f1: 0.7294 - val_precision: 0.7375 - val_recall: 0.7217\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1795 - accuracy: 0.9529 - f1: 0.9516 - precision: 0.9558 - recall: 0.9475 - val_loss: 1.0908 - val_accuracy: 0.7317 - val_f1: 0.7352 - val_precision: 0.7457 - val_recall: 0.7250\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1647 - accuracy: 0.9554 - f1: 0.9564 - precision: 0.9587 - recall: 0.9540 - val_loss: 1.2011 - val_accuracy: 0.7467 - val_f1: 0.7468 - val_precision: 0.7574 - val_recall: 0.7367\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1427 - accuracy: 0.9658 - f1: 0.9658 - precision: 0.9679 - recall: 0.9636 - val_loss: 1.1760 - val_accuracy: 0.7450 - val_f1: 0.7457 - val_precision: 0.7594 - val_recall: 0.7325\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1365 - accuracy: 0.9681 - f1: 0.9669 - precision: 0.9703 - recall: 0.9635 - val_loss: 1.2158 - val_accuracy: 0.7425 - val_f1: 0.7434 - val_precision: 0.7539 - val_recall: 0.7333\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1379 - accuracy: 0.9669 - f1: 0.9671 - precision: 0.9720 - recall: 0.9624 - val_loss: 1.1747 - val_accuracy: 0.7317 - val_f1: 0.7376 - val_precision: 0.7519 - val_recall: 0.7242\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1314 - accuracy: 0.9675 - f1: 0.9665 - precision: 0.9685 - recall: 0.9644 - val_loss: 1.2043 - val_accuracy: 0.7308 - val_f1: 0.7300 - val_precision: 0.7412 - val_recall: 0.7192\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1455 - accuracy: 0.9642 - f1: 0.9636 - precision: 0.9681 - recall: 0.9592 - val_loss: 1.1860 - val_accuracy: 0.7375 - val_f1: 0.7373 - val_precision: 0.7485 - val_recall: 0.7267\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1365 - accuracy: 0.9679 - f1: 0.9674 - precision: 0.9707 - recall: 0.9642 - val_loss: 1.2446 - val_accuracy: 0.7425 - val_f1: 0.7449 - val_precision: 0.7607 - val_recall: 0.7300\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1227 - accuracy: 0.9706 - f1: 0.9693 - precision: 0.9739 - recall: 0.9648 - val_loss: 1.3534 - val_accuracy: 0.7333 - val_f1: 0.7370 - val_precision: 0.7540 - val_recall: 0.7208\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1635 - accuracy: 0.9561 - f1: 0.9565 - precision: 0.9646 - recall: 0.9487 - val_loss: 1.2212 - val_accuracy: 0.7392 - val_f1: 0.7393 - val_precision: 0.7572 - val_recall: 0.7225\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1118 - accuracy: 0.9713 - f1: 0.9723 - precision: 0.9777 - recall: 0.9670 - val_loss: 1.3135 - val_accuracy: 0.7333 - val_f1: 0.7320 - val_precision: 0.7491 - val_recall: 0.7158\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1063 - accuracy: 0.9723 - f1: 0.9719 - precision: 0.9768 - recall: 0.9671 - val_loss: 1.5264 - val_accuracy: 0.7267 - val_f1: 0.7259 - val_precision: 0.7346 - val_recall: 0.7175\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1373 - accuracy: 0.9696 - f1: 0.9696 - precision: 0.9734 - recall: 0.9658 - val_loss: 1.3213 - val_accuracy: 0.7008 - val_f1: 0.7067 - val_precision: 0.7273 - val_recall: 0.6875\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 32ms/step - loss: 0.1259 - accuracy: 0.9730 - f1: 0.9722 - precision: 0.9748 - recall: 0.9696 - val_loss: 1.3317 - val_accuracy: 0.7358 - val_f1: 0.7377 - val_precision: 0.7493 - val_recall: 0.7267\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1201 - accuracy: 0.9685 - f1: 0.9692 - precision: 0.9735 - recall: 0.9650 - val_loss: 1.3427 - val_accuracy: 0.7133 - val_f1: 0.7169 - val_precision: 0.7375 - val_recall: 0.6975\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1421 - accuracy: 0.9626 - f1: 0.9657 - precision: 0.9725 - recall: 0.9591 - val_loss: 1.4538 - val_accuracy: 0.7133 - val_f1: 0.7133 - val_precision: 0.7209 - val_recall: 0.7058\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1775 - accuracy: 0.9560 - f1: 0.9570 - precision: 0.9630 - recall: 0.9511 - val_loss: 1.2606 - val_accuracy: 0.7317 - val_f1: 0.7367 - val_precision: 0.7497 - val_recall: 0.7242\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1453 - accuracy: 0.9622 - f1: 0.9621 - precision: 0.9673 - recall: 0.9569 - val_loss: 1.2428 - val_accuracy: 0.7292 - val_f1: 0.7333 - val_precision: 0.7556 - val_recall: 0.7125\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1125 - accuracy: 0.9691 - f1: 0.9693 - precision: 0.9764 - recall: 0.9625 - val_loss: 1.3158 - val_accuracy: 0.7458 - val_f1: 0.7447 - val_precision: 0.7611 - val_recall: 0.7292\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1405 - accuracy: 0.9633 - f1: 0.9646 - precision: 0.9717 - recall: 0.9577 - val_loss: 1.4118 - val_accuracy: 0.7200 - val_f1: 0.7197 - val_precision: 0.7334 - val_recall: 0.7067\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1264 - accuracy: 0.9674 - f1: 0.9684 - precision: 0.9738 - recall: 0.9631 - val_loss: 1.1251 - val_accuracy: 0.7500 - val_f1: 0.7482 - val_precision: 0.7676 - val_recall: 0.7300\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1529 - accuracy: 0.9609 - f1: 0.9624 - precision: 0.9724 - recall: 0.9528 - val_loss: 1.1106 - val_accuracy: 0.7242 - val_f1: 0.7265 - val_precision: 0.7459 - val_recall: 0.7083\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1997 - accuracy: 0.9454 - f1: 0.9455 - precision: 0.9560 - recall: 0.9353 - val_loss: 1.1389 - val_accuracy: 0.7400 - val_f1: 0.7422 - val_precision: 0.7621 - val_recall: 0.7233\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1250 - accuracy: 0.9657 - f1: 0.9649 - precision: 0.9743 - recall: 0.9558 - val_loss: 1.2840 - val_accuracy: 0.7408 - val_f1: 0.7435 - val_precision: 0.7605 - val_recall: 0.7275\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1419 - accuracy: 0.9622 - f1: 0.9630 - precision: 0.9713 - recall: 0.9550 - val_loss: 1.0514 - val_accuracy: 0.7417 - val_f1: 0.7437 - val_precision: 0.7702 - val_recall: 0.7192\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 29ms/step - loss: 0.1354 - accuracy: 0.9652 - f1: 0.9645 - precision: 0.9729 - recall: 0.9563 - val_loss: 1.0004 - val_accuracy: 0.7475 - val_f1: 0.7563 - val_precision: 0.7907 - val_recall: 0.7250\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 31ms/step - loss: 0.1014 - accuracy: 0.9704 - f1: 0.9729 - precision: 0.9820 - recall: 0.9640 - val_loss: 1.1710 - val_accuracy: 0.7433 - val_f1: 0.7420 - val_precision: 0.7689 - val_recall: 0.7175\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1121 - accuracy: 0.9688 - f1: 0.9702 - precision: 0.9804 - recall: 0.9603 - val_loss: 1.3288 - val_accuracy: 0.7450 - val_f1: 0.7496 - val_precision: 0.7596 - val_recall: 0.7400\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 30ms/step - loss: 0.1059 - accuracy: 0.9705 - f1: 0.9724 - precision: 0.9787 - recall: 0.9663 - val_loss: 1.3089 - val_accuracy: 0.7333 - val_f1: 0.7416 - val_precision: 0.7583 - val_recall: 0.7258\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 1.4572 - accuracy: 0.7257 - f1: 0.7273 - precision: 0.7433 - recall: 0.7121\n",
            "[1.4571775197982788, 0.7257142663002014, 0.7273151278495789, 0.743291974067688, 0.712142825126648]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKM9PEwm1fkb",
        "outputId": "f49f416c-ee44-4f6f-cba5-e458ee0e5f30"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "46/46 [==============================] - 6s 64ms/step - loss: 1.3740 - accuracy: 0.3174 - f1: 0.0014 - precision: 0.0465 - recall: 7.3168e-04 - val_loss: 1.2914 - val_accuracy: 0.4075 - val_f1: 0.0357 - val_precision: 0.8611 - val_recall: 0.0183\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 1.2099 - accuracy: 0.4714 - f1: 0.1487 - precision: 0.6206 - recall: 0.0915 - val_loss: 1.0432 - val_accuracy: 0.5567 - val_f1: 0.4596 - val_precision: 0.6752 - val_recall: 0.3492\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 2s 43ms/step - loss: 0.9594 - accuracy: 0.5987 - f1: 0.5160 - precision: 0.7322 - recall: 0.4008 - val_loss: 0.8475 - val_accuracy: 0.6442 - val_f1: 0.6191 - val_precision: 0.7391 - val_recall: 0.5333\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.7085 - accuracy: 0.7284 - f1: 0.7119 - precision: 0.8197 - recall: 0.6306 - val_loss: 0.7976 - val_accuracy: 0.7008 - val_f1: 0.6952 - val_precision: 0.7380 - val_recall: 0.6575\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.5247 - accuracy: 0.8112 - f1: 0.8031 - precision: 0.8521 - recall: 0.7599 - val_loss: 0.8166 - val_accuracy: 0.7150 - val_f1: 0.7127 - val_precision: 0.7363 - val_recall: 0.6908\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.3900 - accuracy: 0.8610 - f1: 0.8585 - precision: 0.8847 - recall: 0.8342 - val_loss: 0.8183 - val_accuracy: 0.7242 - val_f1: 0.7173 - val_precision: 0.7441 - val_recall: 0.6925\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.3182 - accuracy: 0.8958 - f1: 0.8934 - precision: 0.9129 - recall: 0.8748 - val_loss: 0.7786 - val_accuracy: 0.7417 - val_f1: 0.7313 - val_precision: 0.7541 - val_recall: 0.7100\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.2505 - accuracy: 0.9157 - f1: 0.9143 - precision: 0.9294 - recall: 0.9000 - val_loss: 0.8640 - val_accuracy: 0.7292 - val_f1: 0.7280 - val_precision: 0.7484 - val_recall: 0.7092\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.2234 - accuracy: 0.9237 - f1: 0.9215 - precision: 0.9336 - recall: 0.9097 - val_loss: 0.8601 - val_accuracy: 0.7333 - val_f1: 0.7350 - val_precision: 0.7482 - val_recall: 0.7225\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.1603 - accuracy: 0.9492 - f1: 0.9490 - precision: 0.9557 - recall: 0.9425 - val_loss: 0.9148 - val_accuracy: 0.7483 - val_f1: 0.7505 - val_precision: 0.7586 - val_recall: 0.7425\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.1512 - accuracy: 0.9494 - f1: 0.9468 - precision: 0.9528 - recall: 0.9408 - val_loss: 1.0979 - val_accuracy: 0.7150 - val_f1: 0.7153 - val_precision: 0.7215 - val_recall: 0.7092\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.1531 - accuracy: 0.9473 - f1: 0.9472 - precision: 0.9512 - recall: 0.9432 - val_loss: 0.9201 - val_accuracy: 0.7408 - val_f1: 0.7442 - val_precision: 0.7518 - val_recall: 0.7367\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.1217 - accuracy: 0.9596 - f1: 0.9592 - precision: 0.9615 - recall: 0.9570 - val_loss: 1.0523 - val_accuracy: 0.7458 - val_f1: 0.7463 - val_precision: 0.7527 - val_recall: 0.7400\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.1007 - accuracy: 0.9644 - f1: 0.9638 - precision: 0.9653 - recall: 0.9623 - val_loss: 1.1300 - val_accuracy: 0.7292 - val_f1: 0.7325 - val_precision: 0.7402 - val_recall: 0.7250\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.1174 - accuracy: 0.9582 - f1: 0.9581 - precision: 0.9617 - recall: 0.9545 - val_loss: 1.0621 - val_accuracy: 0.7408 - val_f1: 0.7430 - val_precision: 0.7503 - val_recall: 0.7358\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0921 - accuracy: 0.9672 - f1: 0.9681 - precision: 0.9705 - recall: 0.9656 - val_loss: 1.0653 - val_accuracy: 0.7467 - val_f1: 0.7486 - val_precision: 0.7549 - val_recall: 0.7425\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0968 - accuracy: 0.9666 - f1: 0.9661 - precision: 0.9678 - recall: 0.9644 - val_loss: 1.0848 - val_accuracy: 0.7458 - val_f1: 0.7471 - val_precision: 0.7544 - val_recall: 0.7400\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0778 - accuracy: 0.9716 - f1: 0.9713 - precision: 0.9729 - recall: 0.9698 - val_loss: 1.1060 - val_accuracy: 0.7533 - val_f1: 0.7508 - val_precision: 0.7568 - val_recall: 0.7450\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0642 - accuracy: 0.9760 - f1: 0.9758 - precision: 0.9771 - recall: 0.9745 - val_loss: 1.1001 - val_accuracy: 0.7575 - val_f1: 0.7582 - val_precision: 0.7641 - val_recall: 0.7525\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0574 - accuracy: 0.9790 - f1: 0.9796 - precision: 0.9803 - recall: 0.9789 - val_loss: 1.1298 - val_accuracy: 0.7475 - val_f1: 0.7508 - val_precision: 0.7577 - val_recall: 0.7442\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0620 - accuracy: 0.9744 - f1: 0.9745 - precision: 0.9748 - recall: 0.9743 - val_loss: 1.2009 - val_accuracy: 0.7483 - val_f1: 0.7476 - val_precision: 0.7564 - val_recall: 0.7392\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0591 - accuracy: 0.9752 - f1: 0.9753 - precision: 0.9760 - recall: 0.9747 - val_loss: 1.0993 - val_accuracy: 0.7433 - val_f1: 0.7460 - val_precision: 0.7539 - val_recall: 0.7383\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0568 - accuracy: 0.9773 - f1: 0.9776 - precision: 0.9783 - recall: 0.9770 - val_loss: 1.2017 - val_accuracy: 0.7492 - val_f1: 0.7474 - val_precision: 0.7551 - val_recall: 0.7400\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0577 - accuracy: 0.9763 - f1: 0.9758 - precision: 0.9766 - recall: 0.9750 - val_loss: 1.2332 - val_accuracy: 0.7417 - val_f1: 0.7428 - val_precision: 0.7490 - val_recall: 0.7367\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0585 - accuracy: 0.9792 - f1: 0.9789 - precision: 0.9798 - recall: 0.9781 - val_loss: 1.1746 - val_accuracy: 0.7492 - val_f1: 0.7498 - val_precision: 0.7538 - val_recall: 0.7458\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0529 - accuracy: 0.9776 - f1: 0.9772 - precision: 0.9779 - recall: 0.9764 - val_loss: 1.2906 - val_accuracy: 0.7308 - val_f1: 0.7331 - val_precision: 0.7363 - val_recall: 0.7300\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0522 - accuracy: 0.9780 - f1: 0.9780 - precision: 0.9787 - recall: 0.9773 - val_loss: 1.2066 - val_accuracy: 0.7367 - val_f1: 0.7361 - val_precision: 0.7399 - val_recall: 0.7325\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0505 - accuracy: 0.9761 - f1: 0.9758 - precision: 0.9764 - recall: 0.9752 - val_loss: 1.3158 - val_accuracy: 0.7183 - val_f1: 0.7184 - val_precision: 0.7244 - val_recall: 0.7125\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0542 - accuracy: 0.9764 - f1: 0.9769 - precision: 0.9775 - recall: 0.9762 - val_loss: 1.2551 - val_accuracy: 0.7425 - val_f1: 0.7422 - val_precision: 0.7461 - val_recall: 0.7383\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0591 - accuracy: 0.9758 - f1: 0.9758 - precision: 0.9762 - recall: 0.9753 - val_loss: 1.2817 - val_accuracy: 0.7400 - val_f1: 0.7403 - val_precision: 0.7432 - val_recall: 0.7375\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0689 - accuracy: 0.9695 - f1: 0.9702 - precision: 0.9710 - recall: 0.9695 - val_loss: 1.3965 - val_accuracy: 0.7175 - val_f1: 0.7179 - val_precision: 0.7209 - val_recall: 0.7150\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0535 - accuracy: 0.9737 - f1: 0.9745 - precision: 0.9759 - recall: 0.9731 - val_loss: 1.2396 - val_accuracy: 0.7283 - val_f1: 0.7283 - val_precision: 0.7352 - val_recall: 0.7217\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0589 - accuracy: 0.9745 - f1: 0.9748 - precision: 0.9753 - recall: 0.9744 - val_loss: 1.3484 - val_accuracy: 0.7175 - val_f1: 0.7189 - val_precision: 0.7263 - val_recall: 0.7117\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0584 - accuracy: 0.9750 - f1: 0.9745 - precision: 0.9753 - recall: 0.9737 - val_loss: 1.2989 - val_accuracy: 0.7408 - val_f1: 0.7423 - val_precision: 0.7464 - val_recall: 0.7383\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0571 - accuracy: 0.9727 - f1: 0.9730 - precision: 0.9735 - recall: 0.9724 - val_loss: 1.4226 - val_accuracy: 0.7250 - val_f1: 0.7266 - val_precision: 0.7300 - val_recall: 0.7233\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0499 - accuracy: 0.9765 - f1: 0.9762 - precision: 0.9775 - recall: 0.9748 - val_loss: 1.3124 - val_accuracy: 0.7367 - val_f1: 0.7376 - val_precision: 0.7420 - val_recall: 0.7333\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0465 - accuracy: 0.9785 - f1: 0.9783 - precision: 0.9790 - recall: 0.9777 - val_loss: 1.3237 - val_accuracy: 0.7458 - val_f1: 0.7469 - val_precision: 0.7513 - val_recall: 0.7425\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0486 - accuracy: 0.9751 - f1: 0.9749 - precision: 0.9763 - recall: 0.9736 - val_loss: 1.3838 - val_accuracy: 0.7317 - val_f1: 0.7329 - val_precision: 0.7367 - val_recall: 0.7292\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0463 - accuracy: 0.9771 - f1: 0.9768 - precision: 0.9770 - recall: 0.9765 - val_loss: 1.4149 - val_accuracy: 0.7350 - val_f1: 0.7370 - val_precision: 0.7399 - val_recall: 0.7342\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0451 - accuracy: 0.9739 - f1: 0.9743 - precision: 0.9751 - recall: 0.9736 - val_loss: 1.4386 - val_accuracy: 0.7300 - val_f1: 0.7300 - val_precision: 0.7333 - val_recall: 0.7267\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0561 - accuracy: 0.9734 - f1: 0.9738 - precision: 0.9751 - recall: 0.9726 - val_loss: 1.2833 - val_accuracy: 0.7400 - val_f1: 0.7399 - val_precision: 0.7424 - val_recall: 0.7375\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0744 - accuracy: 0.9691 - f1: 0.9689 - precision: 0.9697 - recall: 0.9681 - val_loss: 1.4315 - val_accuracy: 0.7200 - val_f1: 0.7188 - val_precision: 0.7210 - val_recall: 0.7167\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0567 - accuracy: 0.9724 - f1: 0.9732 - precision: 0.9745 - recall: 0.9720 - val_loss: 1.2705 - val_accuracy: 0.7350 - val_f1: 0.7366 - val_precision: 0.7416 - val_recall: 0.7317\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0419 - accuracy: 0.9782 - f1: 0.9781 - precision: 0.9791 - recall: 0.9771 - val_loss: 1.3991 - val_accuracy: 0.7408 - val_f1: 0.7411 - val_precision: 0.7458 - val_recall: 0.7367\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0437 - accuracy: 0.9791 - f1: 0.9788 - precision: 0.9795 - recall: 0.9782 - val_loss: 1.4142 - val_accuracy: 0.7358 - val_f1: 0.7360 - val_precision: 0.7387 - val_recall: 0.7333\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0442 - accuracy: 0.9773 - f1: 0.9774 - precision: 0.9786 - recall: 0.9761 - val_loss: 1.3434 - val_accuracy: 0.7483 - val_f1: 0.7468 - val_precision: 0.7512 - val_recall: 0.7425\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 2s 42ms/step - loss: 0.0455 - accuracy: 0.9761 - f1: 0.9754 - precision: 0.9766 - recall: 0.9743 - val_loss: 1.4392 - val_accuracy: 0.7550 - val_f1: 0.7552 - val_precision: 0.7571 - val_recall: 0.7533\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0404 - accuracy: 0.9770 - f1: 0.9774 - precision: 0.9781 - recall: 0.9767 - val_loss: 1.5136 - val_accuracy: 0.7500 - val_f1: 0.7491 - val_precision: 0.7507 - val_recall: 0.7475\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0540 - accuracy: 0.9720 - f1: 0.9722 - precision: 0.9727 - recall: 0.9718 - val_loss: 1.3595 - val_accuracy: 0.7517 - val_f1: 0.7531 - val_precision: 0.7553 - val_recall: 0.7508\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0427 - accuracy: 0.9754 - f1: 0.9753 - precision: 0.9766 - recall: 0.9741 - val_loss: 1.3945 - val_accuracy: 0.7500 - val_f1: 0.7481 - val_precision: 0.7496 - val_recall: 0.7467\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0473 - accuracy: 0.9739 - f1: 0.9742 - precision: 0.9756 - recall: 0.9727 - val_loss: 1.3465 - val_accuracy: 0.7608 - val_f1: 0.7632 - val_precision: 0.7665 - val_recall: 0.7600\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0425 - accuracy: 0.9767 - f1: 0.9767 - precision: 0.9770 - recall: 0.9763 - val_loss: 1.4663 - val_accuracy: 0.7392 - val_f1: 0.7389 - val_precision: 0.7411 - val_recall: 0.7367\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0511 - accuracy: 0.9724 - f1: 0.9723 - precision: 0.9727 - recall: 0.9720 - val_loss: 1.5748 - val_accuracy: 0.7375 - val_f1: 0.7374 - val_precision: 0.7389 - val_recall: 0.7358\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0379 - accuracy: 0.9779 - f1: 0.9781 - precision: 0.9785 - recall: 0.9777 - val_loss: 1.5446 - val_accuracy: 0.7550 - val_f1: 0.7559 - val_precision: 0.7585 - val_recall: 0.7533\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0401 - accuracy: 0.9788 - f1: 0.9786 - precision: 0.9789 - recall: 0.9783 - val_loss: 1.5765 - val_accuracy: 0.7375 - val_f1: 0.7381 - val_precision: 0.7387 - val_recall: 0.7375\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0353 - accuracy: 0.9812 - f1: 0.9812 - precision: 0.9817 - recall: 0.9807 - val_loss: 1.7303 - val_accuracy: 0.7375 - val_f1: 0.7354 - val_precision: 0.7376 - val_recall: 0.7333\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0391 - accuracy: 0.9796 - f1: 0.9794 - precision: 0.9808 - recall: 0.9781 - val_loss: 1.6232 - val_accuracy: 0.7458 - val_f1: 0.7469 - val_precision: 0.7489 - val_recall: 0.7450\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 2s 40ms/step - loss: 0.0479 - accuracy: 0.9776 - f1: 0.9779 - precision: 0.9799 - recall: 0.9760 - val_loss: 1.6934 - val_accuracy: 0.7325 - val_f1: 0.7324 - val_precision: 0.7339 - val_recall: 0.7308\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0447 - accuracy: 0.9751 - f1: 0.9747 - precision: 0.9759 - recall: 0.9735 - val_loss: 1.7432 - val_accuracy: 0.7333 - val_f1: 0.7310 - val_precision: 0.7329 - val_recall: 0.7292\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 2s 41ms/step - loss: 0.0389 - accuracy: 0.9795 - f1: 0.9792 - precision: 0.9807 - recall: 0.9777 - val_loss: 1.7964 - val_accuracy: 0.7317 - val_f1: 0.7325 - val_precision: 0.7360 - val_recall: 0.7292\n",
            "14/14 [==============================] - 0s 22ms/step - loss: 1.5401 - accuracy: 0.7600 - f1: 0.7617 - precision: 0.7642 - recall: 0.7593\n",
            "[1.5401203632354736, 0.7599999904632568, 0.7617068886756897, 0.7641639709472656, 0.7592858076095581]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQBa5s4s1hNU",
        "outputId": "3cf4e39c-e9d8-4449-dcea-fbc9781c0357"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(64))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy',f1,precision,recall])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(glove_train_gen,validation_data=glove_val_gen,validation_steps=int((len(X_val)/batch_size)+1),steps_per_epoch=int((len(X_train)/batch_size)+1),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 5s 89ms/step - loss: 1.3896 - accuracy: 0.2616 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3708 - val_accuracy: 0.3317 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 1.3803 - accuracy: 0.3064 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3712 - val_accuracy: 0.3283 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 4s 85ms/step - loss: 1.3746 - accuracy: 0.3113 - f1: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.3583 - val_accuracy: 0.3367 - val_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 1.3474 - accuracy: 0.3532 - f1: 0.0159 - precision: 0.1600 - recall: 0.0088 - val_loss: 1.3090 - val_accuracy: 0.3733 - val_f1: 0.1187 - val_precision: 0.5727 - val_recall: 0.0667\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 1.2953 - accuracy: 0.3964 - f1: 0.1414 - precision: 0.5904 - recall: 0.0832 - val_loss: 1.3872 - val_accuracy: 0.3442 - val_f1: 0.1445 - val_precision: 0.3355 - val_recall: 0.0925\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 1.3095 - accuracy: 0.3916 - f1: 0.0984 - precision: 0.4844 - recall: 0.0590 - val_loss: 1.3702 - val_accuracy: 0.3967 - val_f1: 0.1964 - val_precision: 0.4312 - val_recall: 0.1275\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 1.2732 - accuracy: 0.4270 - f1: 0.1939 - precision: 0.5894 - recall: 0.1213 - val_loss: 1.3509 - val_accuracy: 0.3825 - val_f1: 0.1545 - val_precision: 0.4611 - val_recall: 0.0933\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 1.2268 - accuracy: 0.4332 - f1: 0.2357 - precision: 0.6547 - recall: 0.1445 - val_loss: 1.4642 - val_accuracy: 0.3525 - val_f1: 0.1724 - val_precision: 0.3921 - val_recall: 0.1108\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 1.1797 - accuracy: 0.4667 - f1: 0.2808 - precision: 0.6636 - recall: 0.1806 - val_loss: 1.3426 - val_accuracy: 0.3850 - val_f1: 0.2061 - val_precision: 0.4425 - val_recall: 0.1350\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 1.1455 - accuracy: 0.4766 - f1: 0.3171 - precision: 0.6727 - recall: 0.2091 - val_loss: 1.3002 - val_accuracy: 0.4125 - val_f1: 0.2272 - val_precision: 0.5048 - val_recall: 0.1475\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 1.1268 - accuracy: 0.5028 - f1: 0.3359 - precision: 0.6856 - recall: 0.2245 - val_loss: 1.3282 - val_accuracy: 0.4125 - val_f1: 0.2543 - val_precision: 0.4936 - val_recall: 0.1733\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 1.0943 - accuracy: 0.5184 - f1: 0.3905 - precision: 0.7013 - recall: 0.2719 - val_loss: 1.3627 - val_accuracy: 0.3892 - val_f1: 0.2718 - val_precision: 0.4591 - val_recall: 0.1942\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 1.0785 - accuracy: 0.5300 - f1: 0.4096 - precision: 0.6903 - recall: 0.2927 - val_loss: 1.3909 - val_accuracy: 0.3942 - val_f1: 0.2794 - val_precision: 0.4698 - val_recall: 0.1992\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 1.1022 - accuracy: 0.4994 - f1: 0.3942 - precision: 0.6852 - recall: 0.2787 - val_loss: 1.4312 - val_accuracy: 0.3950 - val_f1: 0.3099 - val_precision: 0.4666 - val_recall: 0.2325\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 1.1315 - accuracy: 0.4962 - f1: 0.3563 - precision: 0.6505 - recall: 0.2478 - val_loss: 1.4679 - val_accuracy: 0.3583 - val_f1: 0.2481 - val_precision: 0.3826 - val_recall: 0.1842\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 1.1260 - accuracy: 0.5051 - f1: 0.3720 - precision: 0.6555 - recall: 0.2614 - val_loss: 1.4674 - val_accuracy: 0.3833 - val_f1: 0.2620 - val_precision: 0.4354 - val_recall: 0.1883\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 1.0986 - accuracy: 0.5252 - f1: 0.3849 - precision: 0.7062 - recall: 0.2655 - val_loss: 1.4501 - val_accuracy: 0.3667 - val_f1: 0.2524 - val_precision: 0.3770 - val_recall: 0.1900\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 1.1222 - accuracy: 0.5055 - f1: 0.4072 - precision: 0.6380 - recall: 0.3006 - val_loss: 1.5168 - val_accuracy: 0.3342 - val_f1: 0.2243 - val_precision: 0.3469 - val_recall: 0.1658\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 1.0755 - accuracy: 0.5371 - f1: 0.4350 - precision: 0.6699 - recall: 0.3233 - val_loss: 1.4273 - val_accuracy: 0.3933 - val_f1: 0.2948 - val_precision: 0.4657 - val_recall: 0.2167\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.9995 - accuracy: 0.5895 - f1: 0.4823 - precision: 0.7124 - recall: 0.3653 - val_loss: 1.4496 - val_accuracy: 0.3458 - val_f1: 0.2665 - val_precision: 0.3903 - val_recall: 0.2025\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.9486 - accuracy: 0.6070 - f1: 0.5320 - precision: 0.7311 - recall: 0.4195 - val_loss: 1.4916 - val_accuracy: 0.3717 - val_f1: 0.3024 - val_precision: 0.3991 - val_recall: 0.2442\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.9795 - accuracy: 0.5899 - f1: 0.5301 - precision: 0.6898 - recall: 0.4314 - val_loss: 1.5385 - val_accuracy: 0.3458 - val_f1: 0.2842 - val_precision: 0.3821 - val_recall: 0.2267\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 4s 78ms/step - loss: 0.9168 - accuracy: 0.6411 - f1: 0.5776 - precision: 0.7292 - recall: 0.4791 - val_loss: 1.6267 - val_accuracy: 0.3383 - val_f1: 0.2882 - val_precision: 0.3618 - val_recall: 0.2400\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.9301 - accuracy: 0.6207 - f1: 0.5711 - precision: 0.7089 - recall: 0.4794 - val_loss: 1.6190 - val_accuracy: 0.3500 - val_f1: 0.3040 - val_precision: 0.3786 - val_recall: 0.2542\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 4s 86ms/step - loss: 0.9190 - accuracy: 0.6283 - f1: 0.5829 - precision: 0.7224 - recall: 0.4892 - val_loss: 1.5390 - val_accuracy: 0.3817 - val_f1: 0.3351 - val_precision: 0.4257 - val_recall: 0.2767\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.9250 - accuracy: 0.6221 - f1: 0.5847 - precision: 0.7171 - recall: 0.4942 - val_loss: 1.5616 - val_accuracy: 0.3617 - val_f1: 0.3096 - val_precision: 0.4010 - val_recall: 0.2525\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.9103 - accuracy: 0.6281 - f1: 0.5770 - precision: 0.7195 - recall: 0.4828 - val_loss: 1.5552 - val_accuracy: 0.3408 - val_f1: 0.2885 - val_precision: 0.3788 - val_recall: 0.2333\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.9460 - accuracy: 0.6001 - f1: 0.5644 - precision: 0.7026 - recall: 0.4727 - val_loss: 1.5364 - val_accuracy: 0.3492 - val_f1: 0.2944 - val_precision: 0.3934 - val_recall: 0.2358\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.9921 - accuracy: 0.5957 - f1: 0.5434 - precision: 0.7039 - recall: 0.4441 - val_loss: 1.5589 - val_accuracy: 0.3650 - val_f1: 0.3224 - val_precision: 0.3994 - val_recall: 0.2708\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.9178 - accuracy: 0.6415 - f1: 0.5892 - precision: 0.7286 - recall: 0.4956 - val_loss: 1.6030 - val_accuracy: 0.3642 - val_f1: 0.3188 - val_precision: 0.3915 - val_recall: 0.2692\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8152 - accuracy: 0.6770 - f1: 0.6471 - precision: 0.7572 - recall: 0.5654 - val_loss: 1.6419 - val_accuracy: 0.3650 - val_f1: 0.3099 - val_precision: 0.3791 - val_recall: 0.2625\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.8208 - accuracy: 0.6797 - f1: 0.6513 - precision: 0.7521 - recall: 0.5753 - val_loss: 1.6326 - val_accuracy: 0.3658 - val_f1: 0.3168 - val_precision: 0.3837 - val_recall: 0.2700\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8232 - accuracy: 0.6835 - f1: 0.6590 - precision: 0.7604 - recall: 0.5822 - val_loss: 1.7455 - val_accuracy: 0.3592 - val_f1: 0.3278 - val_precision: 0.3718 - val_recall: 0.2933\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.8389 - accuracy: 0.6696 - f1: 0.6445 - precision: 0.7353 - recall: 0.5743 - val_loss: 1.7062 - val_accuracy: 0.3508 - val_f1: 0.3080 - val_precision: 0.3603 - val_recall: 0.2692\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.8358 - accuracy: 0.6865 - f1: 0.6434 - precision: 0.7435 - recall: 0.5678 - val_loss: 1.7511 - val_accuracy: 0.3383 - val_f1: 0.2890 - val_precision: 0.3526 - val_recall: 0.2450\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.8124 - accuracy: 0.6854 - f1: 0.6611 - precision: 0.7448 - recall: 0.5951 - val_loss: 1.7587 - val_accuracy: 0.3508 - val_f1: 0.3138 - val_precision: 0.3645 - val_recall: 0.2758\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.7668 - accuracy: 0.7001 - f1: 0.6789 - precision: 0.7617 - recall: 0.6129 - val_loss: 1.8650 - val_accuracy: 0.3425 - val_f1: 0.3105 - val_precision: 0.3544 - val_recall: 0.2767\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.8072 - accuracy: 0.6909 - f1: 0.6642 - precision: 0.7514 - recall: 0.5960 - val_loss: 1.7695 - val_accuracy: 0.3467 - val_f1: 0.3180 - val_precision: 0.3700 - val_recall: 0.2792\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.7745 - accuracy: 0.7066 - f1: 0.6857 - precision: 0.7713 - recall: 0.6179 - val_loss: 1.8226 - val_accuracy: 0.3467 - val_f1: 0.3138 - val_precision: 0.3681 - val_recall: 0.2742\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.8080 - accuracy: 0.6848 - f1: 0.6695 - precision: 0.7492 - recall: 0.6057 - val_loss: 1.7687 - val_accuracy: 0.3500 - val_f1: 0.3220 - val_precision: 0.3653 - val_recall: 0.2883\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.7904 - accuracy: 0.6947 - f1: 0.6752 - precision: 0.7556 - recall: 0.6108 - val_loss: 1.8024 - val_accuracy: 0.3583 - val_f1: 0.3303 - val_precision: 0.3772 - val_recall: 0.2942\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.7094 - accuracy: 0.7359 - f1: 0.7202 - precision: 0.7979 - recall: 0.6569 - val_loss: 1.8138 - val_accuracy: 0.3458 - val_f1: 0.3298 - val_precision: 0.3759 - val_recall: 0.2942\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.7686 - accuracy: 0.6992 - f1: 0.6878 - precision: 0.7631 - recall: 0.6268 - val_loss: 1.8065 - val_accuracy: 0.3617 - val_f1: 0.3443 - val_precision: 0.3904 - val_recall: 0.3083\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.9397 - accuracy: 0.6276 - f1: 0.6049 - precision: 0.6881 - recall: 0.5422 - val_loss: 1.5515 - val_accuracy: 0.3542 - val_f1: 0.3200 - val_precision: 0.3908 - val_recall: 0.2717\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.8546 - accuracy: 0.6731 - f1: 0.6315 - precision: 0.7417 - recall: 0.5507 - val_loss: 1.6786 - val_accuracy: 0.3567 - val_f1: 0.3323 - val_precision: 0.3836 - val_recall: 0.2933\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.8055 - accuracy: 0.7002 - f1: 0.6661 - precision: 0.7638 - recall: 0.5919 - val_loss: 1.7116 - val_accuracy: 0.3742 - val_f1: 0.3494 - val_precision: 0.3993 - val_recall: 0.3108\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.7191 - accuracy: 0.7294 - f1: 0.7152 - precision: 0.7958 - recall: 0.6501 - val_loss: 1.7995 - val_accuracy: 0.3825 - val_f1: 0.3532 - val_precision: 0.3944 - val_recall: 0.3200\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.7022 - accuracy: 0.7393 - f1: 0.7250 - precision: 0.7989 - recall: 0.6642 - val_loss: 1.8689 - val_accuracy: 0.3550 - val_f1: 0.3357 - val_precision: 0.3788 - val_recall: 0.3017\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.6914 - accuracy: 0.7413 - f1: 0.7267 - precision: 0.7936 - recall: 0.6706 - val_loss: 1.8991 - val_accuracy: 0.3567 - val_f1: 0.3348 - val_precision: 0.3714 - val_recall: 0.3050\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.7201 - accuracy: 0.7274 - f1: 0.7085 - precision: 0.7714 - recall: 0.6555 - val_loss: 1.9283 - val_accuracy: 0.3600 - val_f1: 0.3363 - val_precision: 0.3689 - val_recall: 0.3092\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 4s 79ms/step - loss: 0.6465 - accuracy: 0.7618 - f1: 0.7519 - precision: 0.8114 - recall: 0.7010 - val_loss: 1.9615 - val_accuracy: 0.3692 - val_f1: 0.3466 - val_precision: 0.3729 - val_recall: 0.3242\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 0.6498 - accuracy: 0.7554 - f1: 0.7450 - precision: 0.8010 - recall: 0.6966 - val_loss: 1.9514 - val_accuracy: 0.3683 - val_f1: 0.3502 - val_precision: 0.3846 - val_recall: 0.3217\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.6567 - accuracy: 0.7538 - f1: 0.7472 - precision: 0.8028 - recall: 0.6993 - val_loss: 1.9496 - val_accuracy: 0.3775 - val_f1: 0.3535 - val_precision: 0.3878 - val_recall: 0.3250\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.6351 - accuracy: 0.7638 - f1: 0.7581 - precision: 0.8140 - recall: 0.7102 - val_loss: 1.9316 - val_accuracy: 0.3725 - val_f1: 0.3667 - val_precision: 0.3981 - val_recall: 0.3400\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 4s 84ms/step - loss: 0.7561 - accuracy: 0.7161 - f1: 0.7033 - precision: 0.7585 - recall: 0.6562 - val_loss: 1.8444 - val_accuracy: 0.3525 - val_f1: 0.3261 - val_precision: 0.3662 - val_recall: 0.2942\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.7338 - accuracy: 0.7277 - f1: 0.7178 - precision: 0.7816 - recall: 0.6642 - val_loss: 1.8399 - val_accuracy: 0.3708 - val_f1: 0.3455 - val_precision: 0.3866 - val_recall: 0.3125\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 4s 82ms/step - loss: 0.7015 - accuracy: 0.7391 - f1: 0.7280 - precision: 0.7907 - recall: 0.6750 - val_loss: 1.8725 - val_accuracy: 0.3700 - val_f1: 0.3577 - val_precision: 0.3886 - val_recall: 0.3317\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.7005 - accuracy: 0.7344 - f1: 0.7236 - precision: 0.7778 - recall: 0.6769 - val_loss: 1.7573 - val_accuracy: 0.4050 - val_f1: 0.3841 - val_precision: 0.4221 - val_recall: 0.3525\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 4s 81ms/step - loss: 0.7494 - accuracy: 0.7167 - f1: 0.7072 - precision: 0.7733 - recall: 0.6520 - val_loss: 1.7844 - val_accuracy: 0.3592 - val_f1: 0.3379 - val_precision: 0.3715 - val_recall: 0.3100\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.7570 - accuracy: 0.7167 - f1: 0.7011 - precision: 0.7700 - recall: 0.6439 - val_loss: 1.8374 - val_accuracy: 0.3650 - val_f1: 0.3442 - val_precision: 0.3751 - val_recall: 0.3183\n",
            "14/14 [==============================] - 0s 24ms/step - loss: 1.8115 - accuracy: 0.3864 - f1: 0.3655 - precision: 0.4004 - recall: 0.3364\n",
            "[1.8115077018737793, 0.386428564786911, 0.3655298352241516, 0.40039023756980896, 0.33642858266830444]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZqN_SaM3hs9",
        "outputId": "89b5770d-d020-4e9c-c567-4173702df0b3"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "# print(history_sg_bi.history)\r\n",
        "# plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()\r\n",
        "# plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "# plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "# plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "# plt.xlabel('epochs')\r\n",
        "# plt.legend()\r\n",
        "# # plt.yticks()\r\n",
        "# plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.6000000238418579, 0.6142857074737549, 0.2978571355342865]\n",
            "cbow [0.5699999928474426, 0.631428599357605, 0.3378571569919586]\n",
            "glove [0.7257142663002014, 0.7599999904632568, 0.386428564786911]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}