{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewDataPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIKg74VXk/Y6BaWIWlyaRU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/NewDataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFeegM0M-HLw"
      },
      "source": [
        "!pip3 install ekphrasis\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, sys, os, csv\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AALj7ATBABq",
        "outputId": "3efe06b0-9f9e-4826-b1ec-04a76227a8f7"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, sys, os, csv\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import itertools\n",
        "!pip3 install emoji\n",
        "import emoji\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "#from nltk.corpus import stopwords\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "import pickle\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "SEED = 42\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras import utils as np_utils\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.preprocessing import text,sequence\n",
        "from keras.layers import Input\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Lambda, BatchNormalization, Concatenate\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import keras.layers as layers\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.engine import Layer\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Activation, Flatten\n",
        "\n",
        "sess = tf.Session()\n",
        "# K.set_session(sess)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwZNqzLsEotf",
        "outputId": "dccc357a-e432-4790-b8d9-6b0e252ef13f"
      },
      "source": [
        "from google.colab import  drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjP3_106BZmV"
      },
      "source": [
        "def load_dict_smileys():\n",
        "    \n",
        "    return {\n",
        "        \":‑)\":\"smiley\",\n",
        "        \":-]\":\"smiley\",\n",
        "        \":-3\":\"smiley\",\n",
        "        \":->\":\"smiley\",\n",
        "        \"8-)\":\"smiley\",\n",
        "        \":-}\":\"smiley\",\n",
        "        \":)\":\"smiley\",\n",
        "        \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\",\n",
        "        \":>\":\"smiley\",\n",
        "        \"8)\":\"smiley\",\n",
        "        \":}\":\"smiley\",\n",
        "        \":o)\":\"smiley\",\n",
        "        \":c)\":\"smiley\",\n",
        "        \":^)\":\"smiley\",\n",
        "        \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\",\n",
        "        \":-))\":\"smiley\",\n",
        "        \":‑D\":\"smiley\",\n",
        "        \"8‑D\":\"smiley\",\n",
        "        \"x‑D\":\"smiley\",\n",
        "        \"X‑D\":\"smiley\",\n",
        "        \":D\":\"smiley\",\n",
        "        \"8D\":\"smiley\",\n",
        "        \"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\",\n",
        "        \":‑(\":\"sad\",\n",
        "        \":‑c\":\"sad\",\n",
        "        \":‑<\":\"sad\",\n",
        "        \":‑[\":\"sad\",\n",
        "        \":(\":\"sad\",\n",
        "        \":c\":\"sad\",\n",
        "        \":<\":\"sad\",\n",
        "        \":[\":\"sad\",\n",
        "        \":-||\":\"sad\",\n",
        "        \">:[\":\"sad\",\n",
        "        \":{\":\"sad\",\n",
        "        \":@\":\"sad\",\n",
        "        \">:(\":\"sad\",\n",
        "        \":'‑(\":\"sad\",\n",
        "        \":'(\":\"sad\",\n",
        "        \":‑P\":\"playful\",\n",
        "        \"X‑P\":\"playful\",\n",
        "        \"x‑p\":\"playful\",\n",
        "        \":‑p\":\"playful\",\n",
        "        \":‑Þ\":\"playful\",\n",
        "        \":‑þ\":\"playful\",\n",
        "        \":‑b\":\"playful\",\n",
        "        \":P\":\"playful\",\n",
        "        \"XP\":\"playful\",\n",
        "        \"xp\":\"playful\",\n",
        "        \":p\":\"playful\",\n",
        "        \":Þ\":\"playful\",\n",
        "        \":þ\":\"playful\",\n",
        "        \":b\":\"playful\",\n",
        "        \"<3\":\"love\"\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOztndGzBo5S"
      },
      "source": [
        "\n",
        "def load_dict_contractions():\n",
        "    \n",
        "    return {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "\t\"cant\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "\t\"didnt\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "\t\"doesnt\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "\t\"dont\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "\t\"hadnt\":\"had not\",\n",
        "        \"hasnt\":\"has not\",\n",
        "        \"havent\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"I'd\":\"I would\",\n",
        "        \"I'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\n",
        "\t\"Im\":\"I am\",\n",
        "\t\"im\":\"I am\",\n",
        "        \"I'm'a\":\"I am about to\",\n",
        "        \"I'm'o\":\"I am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"I've\":\"I have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "\t\"mightnt\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "\t\"shouldnt\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "\t\"wasnt\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "\t\"werent\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "\t\"whos\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "\t\"wouldnt\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"Whatcha\":\"What are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\"\n",
        "        }\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vScQPJaWBs43"
      },
      "source": [
        "def strip_accents(text):\n",
        "    if 'ø' in text or  'Ø' in text:\n",
        "        #Do nothing when finding ø \n",
        "        return text   \n",
        "    text = text.encode('ascii', 'ignore')\n",
        "    text = text.decode(\"utf-8\")\n",
        "    return str(text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMUnoKH5_hVO",
        "outputId": "a13327f6-e3f6-40b2-97c9-f764fcbce720"
      },
      "source": [
        "def remove_numbers(x):\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "\n",
        "def preprocess_word(word):\n",
        "    # Convert more than 2 letter repetitions to 2 letter\n",
        "    # funnnnny --> funny\n",
        "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
        "    # Remove - & '\n",
        "    word = re.sub(r'(-|\\')', '', word)\n",
        "    return word\n",
        "\n",
        "\n",
        "def is_valid_word(word):\n",
        "    # Check if word begins with an alphabet\n",
        "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
        "\n",
        "\n",
        "def cleanText(text): \n",
        "    processed_tweet = []\n",
        "    porter_stemmer = PorterStemmer()\t\n",
        "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "    #Special case not handled previously.\n",
        "    text = text.replace('\\x92',\"'\")\n",
        "    #Removal of address\n",
        "    text = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", text).split())\t\n",
        "    #text = replace_contractions(text)\n",
        "    #text = replace_links(text, \"link\")\n",
        "    text = remove_numbers(text)\n",
        "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{]',\"\",text)\n",
        "    text = text.lower()\n",
        "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "    CONTRACTIONS = load_dict_contractions()\n",
        "    text = text.replace(\"’\",\"'\")\n",
        "    words = text.split()\n",
        "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
        "    text = \" \".join(reformed)\n",
        "    # Standardizing words\n",
        "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        "    #Deal with smileys\n",
        "    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
        "    SMILEY = load_dict_smileys()  \n",
        "    words = text.split()\n",
        "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
        "    text = \" \".join(reformed)\n",
        "    #Deal with emojis\n",
        "    text = emoji.demojize(text)\n",
        "    #Strip accents\n",
        "    text= strip_accents(text)\n",
        "    text = text.replace(\":\",\" \")\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        word = preprocess_word(word)\n",
        "        if is_valid_word(word):\n",
        "             word = str(porter_stemmer.stem(word))\n",
        "        processed_tweet.append(word)    \n",
        "    return ' '.join(processed_tweet)\n",
        "    \n",
        "'''def tweet_label(label):\n",
        "    if label == \"empty\":return 1 # neutral\n",
        "    elif label == \"sadness\":return 2 # sad\n",
        "    elif label == \"enthusiasm\":return 3 # happy\n",
        "    elif label == \"neutral\":return 0 # neutral\n",
        "    elif label == \"worry\":return 4 # sad\n",
        "    elif label == \"surprise\":return 5 # happy\n",
        "    elif label == \"love\":return 6 # happy\n",
        "    elif label == \"fun\":return 7 # happy\n",
        "    elif label == \"hate\":return 8\n",
        "    elif label == \"happiness\":return 9 # happy\n",
        "    elif label == \"boredom\":return 10 # neutral\n",
        "    elif label == \"relief\":return 11 # happy\n",
        "    elif label == \"anger\":return 12'''\n",
        "\n",
        "def tweet_label(label):\n",
        "\tif (label == \"enthusiasm\" or label==\"surprise\" or label == \"love\" or label == \"fun\" or label == \"happiness\" or label == \"relief\"):return 1 # happy\n",
        "\telif (label == \"neutral\" or label == \"empty\" or  label == \"boredom\"):return 0 #neutral \n",
        "\telif (label == \"hate\" or label == \"sadness\" or label == \"worry\"):return 2 # hate\n",
        "\telif (label == \"anger\"):return 3 # anger\n",
        "\n",
        "train_clean_tweet_texts = []\n",
        "train_clean_tweet_emotion=[]\n",
        "test_clean_tweet_texts=[]\n",
        "#test_clean_tweet_emotion=[]\n",
        "\n",
        "for i in range(len(train_data)):                                               \n",
        "\ttrain_clean_tweet_texts.append(cleanText(train_data['tweets'][i]))\n",
        "\ttrain_clean_tweet_emotion.append(tweet_label(train_data['emotion'][i]))\n",
        "for i in range(len(test_data)):\n",
        "\ttest_clean_tweet_texts.append(cleanText(test_data['tweets'][i]))\t\n",
        "\t\n",
        "\n",
        "\n",
        "time_steps=100\n",
        "\n",
        "#Saving cleaned data as csv\n",
        "clean_train_df = pd.DataFrame(train_clean_tweet_texts,columns=['tweets'])\n",
        "clean_train_df['emotion'] = pd.DataFrame(train_clean_tweet_emotion,columns=['emotion'])\n",
        "\n",
        "\n",
        "clean_test_df = pd.DataFrame(test_clean_tweet_texts,columns=['tweets'])\n",
        "\n",
        "print(clean_train_df.head())\n",
        "print(clean_test_df.head())\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  emotion\n",
            "0  tiffanylu i know i wa listenin to bad habit ea...        0\n",
            "1  layin n bed with a headach ughhwaitin on your ...        2\n",
            "2                        funer ceremonygloomi friday        2\n",
            "3                  want to hang out with friend soon        1\n",
            "4  dannycastillo we want to trade with someon who...        0\n",
            "                                              tweets\n",
            "0   is hangin with the love of my life tessa mccravi\n",
            "1  ive got an urg to make music like massiv I am ...\n",
            "2                          lacrossehawti rofl uh huh\n",
            "3  fankri haha thank tiff it went well but they w...\n",
            "4             alyssaisntcool hahah i love him though\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QfLI8QVACBf"
      },
      "source": [
        "clean_test_df.to_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/test_clean_tweetmerge.csv',index=False)\n",
        "clean_train_df.to_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/train_clean_tweetmerge.csv',index=False)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoD9QYo-AHx3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}