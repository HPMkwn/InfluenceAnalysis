{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEmointMod.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEmointMod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e26c4ac-23e9-472f-f6f7-5f64ce7dba72"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ab5390-26fd-44b6-af17-e0b29a02aadd"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 'thought', 'it', 'was', 'because', 'she', 'once', 'sunk', 'her', 'teeth', 'into', 'something', 'that', 'is', 'meant', '2b', 'soked'], ['answers', 'i', 'have', 'contacted', 'answers', 'service', 'whathappenedtocustomerservice'], ['who', 'knew', 'softballs', 'could', 'sting', 'so', 'bad', 'jimmy', 'fallon', 'has', 'been', 'under', 'a', 'lot', 'of', 'heat', 'for', 'his', 'andldquo', 'softballandrdquo', 'donald', 'trump', 'interview'], ['resent'], ['so', 'going', 'to', 'local', 'news', 'immediately', 'after', 'designated', 'survivor', 'turns', 'out', 'to', 'be', 'a', 'smooth', 'transition', 'chaos', 'a', 'raging', 'fire', 'media']]\n",
            "[[0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor=1.5"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xYyXPmkpgpP"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.30, random_state=42)\r\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\r\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='NN' or tweet[i][1][:2]=='RB' or tweet[i][1][:2]=='VB':\r\n",
        "            tweet[i]=((tweet[i][0])*mul_factor,tweet[i][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ' or tweet[i][1][:2]=='NN' or tweet[i][1][:2]=='RB' or tweet[i][1][:2]=='VB':\r\n",
        "            tweet[i]=((tweet[i][0])*mul_factor,tweet[i][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 50"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "outputId": "ee946726-91a0-4c8d-b668-34431d6d8851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history_sg_lstm = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-32-f0e292bfb240>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.3765 - accuracy: 0.3190\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.3735 - accuracy: 0.3216\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3734 - accuracy: 0.3216\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.3732 - accuracy: 0.3216\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3639 - accuracy: 0.3363\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3427 - accuracy: 0.3637\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3345 - accuracy: 0.3704\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.3204 - accuracy: 0.3859\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.3160 - accuracy: 0.3914\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.3114 - accuracy: 0.3967\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3043 - accuracy: 0.4039\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2993 - accuracy: 0.4092\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.2946 - accuracy: 0.4088\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2892 - accuracy: 0.4124\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2867 - accuracy: 0.4129\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.2805 - accuracy: 0.4161\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2767 - accuracy: 0.4231\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2731 - accuracy: 0.4259\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2669 - accuracy: 0.4284\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2619 - accuracy: 0.4329\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2470 - accuracy: 0.4427\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2336 - accuracy: 0.4488\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2255 - accuracy: 0.4529\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.1779 - accuracy: 0.4680\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.1477 - accuracy: 0.4786\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.1041 - accuracy: 0.4980\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.0522 - accuracy: 0.5104\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.0105 - accuracy: 0.5314\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.9791 - accuracy: 0.5516\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.9806 - accuracy: 0.5559\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 0.9398 - accuracy: 0.5802\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.8968 - accuracy: 0.6027\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.8737 - accuracy: 0.6165\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.8691 - accuracy: 0.6114\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.8355 - accuracy: 0.6363\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.8347 - accuracy: 0.6367\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.8043 - accuracy: 0.6486\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.7830 - accuracy: 0.6627\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.7693 - accuracy: 0.6627\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.7452 - accuracy: 0.6796\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.7975 - accuracy: 0.6565\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.7995 - accuracy: 0.6424\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.7344 - accuracy: 0.6867\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.6856 - accuracy: 0.7184\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6514 - accuracy: 0.7384\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.6350 - accuracy: 0.7486\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6076 - accuracy: 0.7639\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.5956 - accuracy: 0.7759\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.6248 - accuracy: 0.7494\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.6590 - accuracy: 0.7292\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 1.2378 - accuracy: 0.4848\n",
            "[1.2378042936325073, 0.48476189374923706]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "outputId": "077b239b-6bd5-44d5-a7bf-b52890cb05d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 3s 71ms/step - loss: 1.3721 - accuracy: 0.3194\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 1.3492 - accuracy: 0.3384\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.3116 - accuracy: 0.3835\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.2712 - accuracy: 0.4078\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.2422 - accuracy: 0.4380\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.2138 - accuracy: 0.4563\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.1847 - accuracy: 0.4788\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.1565 - accuracy: 0.4986\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.1335 - accuracy: 0.5155\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.1039 - accuracy: 0.5296\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.0752 - accuracy: 0.5439\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.0465 - accuracy: 0.5535\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 1.0197 - accuracy: 0.5759\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.9863 - accuracy: 0.5902\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.9486 - accuracy: 0.6053\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.9131 - accuracy: 0.6329\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.8863 - accuracy: 0.6469\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.8322 - accuracy: 0.6710\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.7899 - accuracy: 0.6924\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.7553 - accuracy: 0.7043\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.6982 - accuracy: 0.7298\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.7095 - accuracy: 0.7259\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.6490 - accuracy: 0.7535\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.6933 - accuracy: 0.7412\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.6250 - accuracy: 0.7653\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.5646 - accuracy: 0.7884\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.5290 - accuracy: 0.8053\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.5130 - accuracy: 0.8086\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.4796 - accuracy: 0.8249\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.4743 - accuracy: 0.8212\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.4633 - accuracy: 0.8361\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.4542 - accuracy: 0.8369\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.4264 - accuracy: 0.8443\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.3822 - accuracy: 0.8633\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3736 - accuracy: 0.8669\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.3538 - accuracy: 0.8767\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.3436 - accuracy: 0.8794\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3870 - accuracy: 0.8610\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.3708 - accuracy: 0.8694\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3245 - accuracy: 0.8878\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.4599 - accuracy: 0.8392\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.3180 - accuracy: 0.8929\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 0.2804 - accuracy: 0.9063\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.2651 - accuracy: 0.9129\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.2511 - accuracy: 0.9169\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.2468 - accuracy: 0.9167\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.2426 - accuracy: 0.9182\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.2461 - accuracy: 0.9163\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.2520 - accuracy: 0.9157\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.2286 - accuracy: 0.9271\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 1.6224 - accuracy: 0.6162\n",
            "[1.6223911046981812, 0.616190493106842]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "outputId": "74e907f7-a8b3-49b1-d94d-fab8c50c5cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3792 - accuracy: 0.3067\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3713 - accuracy: 0.3182\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3627 - accuracy: 0.3320\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3500 - accuracy: 0.3437\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3349 - accuracy: 0.3606\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3127 - accuracy: 0.3820\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3344 - accuracy: 0.3712\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3328 - accuracy: 0.3594\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3173 - accuracy: 0.3765\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3221 - accuracy: 0.3751\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3017 - accuracy: 0.3933\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.2826 - accuracy: 0.4073\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.2634 - accuracy: 0.4231\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.2398 - accuracy: 0.4400\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.2447 - accuracy: 0.4388\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.2330 - accuracy: 0.4412\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.2328 - accuracy: 0.4437\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.2486 - accuracy: 0.4308\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.2609 - accuracy: 0.4182\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1908 - accuracy: 0.4678\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1618 - accuracy: 0.4869\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.1763 - accuracy: 0.4739\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1844 - accuracy: 0.4676\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.1618 - accuracy: 0.4824\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 4s 90ms/step - loss: 1.1540 - accuracy: 0.5018\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1613 - accuracy: 0.4841\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1988 - accuracy: 0.4610\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 4s 90ms/step - loss: 1.1889 - accuracy: 0.4696\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 4s 90ms/step - loss: 1.2087 - accuracy: 0.4631\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 4s 91ms/step - loss: 1.1885 - accuracy: 0.4743\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.1447 - accuracy: 0.4945\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 4s 91ms/step - loss: 1.1780 - accuracy: 0.4792\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 4s 90ms/step - loss: 1.1364 - accuracy: 0.5069\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.1112 - accuracy: 0.5151\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.0925 - accuracy: 0.5273\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.1279 - accuracy: 0.5084\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1202 - accuracy: 0.5153\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1419 - accuracy: 0.4976\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0960 - accuracy: 0.5278\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0755 - accuracy: 0.5382\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0444 - accuracy: 0.5545\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0261 - accuracy: 0.5680\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0943 - accuracy: 0.5324\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.0987 - accuracy: 0.5202\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.1177 - accuracy: 0.5178\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0884 - accuracy: 0.5239\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.1006 - accuracy: 0.5214\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1499 - accuracy: 0.4978\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0582 - accuracy: 0.5463\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 0.9997 - accuracy: 0.5778\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 1.6051 - accuracy: 0.3071\n",
            "[1.6050890684127808, 0.30714285373687744]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "outputId": "135fee8f-9012-484a-e1ee-be7a02e3994e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3765 - accuracy: 0.3190\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3736 - accuracy: 0.3216\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 1.3735 - accuracy: 0.3216\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3734 - accuracy: 0.3216\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3733 - accuracy: 0.3216\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3731 - accuracy: 0.3216\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3647 - accuracy: 0.3376\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 3s 62ms/step - loss: 1.3457 - accuracy: 0.3698\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.3332 - accuracy: 0.3816\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3163 - accuracy: 0.3949\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3071 - accuracy: 0.4016\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2967 - accuracy: 0.4051\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2877 - accuracy: 0.4102\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2781 - accuracy: 0.4137\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 1.2684 - accuracy: 0.4194\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 1.2527 - accuracy: 0.4304\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2357 - accuracy: 0.4380\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 1.2105 - accuracy: 0.4463\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.1889 - accuracy: 0.4569\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 1.1445 - accuracy: 0.4771\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.1005 - accuracy: 0.4973\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 3s 62ms/step - loss: 1.0566 - accuracy: 0.5202\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 1.0185 - accuracy: 0.5486\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.9900 - accuracy: 0.5584\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.9608 - accuracy: 0.5686\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.9679 - accuracy: 0.5722\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.9175 - accuracy: 0.6043\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.8659 - accuracy: 0.6269\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.8380 - accuracy: 0.6384\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.8238 - accuracy: 0.6504\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.8259 - accuracy: 0.6504\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.8256 - accuracy: 0.6412\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.8735 - accuracy: 0.6194\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.8036 - accuracy: 0.6576\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.7936 - accuracy: 0.6571\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.7681 - accuracy: 0.6733\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.7319 - accuracy: 0.6996\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 3s 62ms/step - loss: 0.7408 - accuracy: 0.6873\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 3s 62ms/step - loss: 0.6939 - accuracy: 0.7149\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6590 - accuracy: 0.7435\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6434 - accuracy: 0.7471\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6319 - accuracy: 0.7582\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6587 - accuracy: 0.7400\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6923 - accuracy: 0.7157\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.6803 - accuracy: 0.7227\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.6342 - accuracy: 0.7527\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6384 - accuracy: 0.7465\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.7330 - accuracy: 0.6947\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.6764 - accuracy: 0.7190\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.6277 - accuracy: 0.7504\n",
            " 2/21 [=>............................] - ETA: 0s - loss: 1.1685 - accuracy: 0.4950WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0194s vs `on_test_batch_end` time: 0.0511s). Check your callbacks.\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 1.2438 - accuracy: 0.4767\n",
            "[1.2438102960586548, 0.476666659116745]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "outputId": "73d4f583-a577-4d90-c2da-aa89f184b7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.3726 - accuracy: 0.3204\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3489 - accuracy: 0.3400\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.3074 - accuracy: 0.3790\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.2712 - accuracy: 0.4184\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2391 - accuracy: 0.4329\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.2105 - accuracy: 0.4576\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.1888 - accuracy: 0.4747\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.1562 - accuracy: 0.4939\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.1349 - accuracy: 0.5104\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.1076 - accuracy: 0.5273\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 1.0814 - accuracy: 0.5390\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 1.0495 - accuracy: 0.5545\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 1.0220 - accuracy: 0.5724\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 0.9839 - accuracy: 0.5949\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.9507 - accuracy: 0.6155\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.9274 - accuracy: 0.6237\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.8872 - accuracy: 0.6418\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.8408 - accuracy: 0.6688\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.8124 - accuracy: 0.6796\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 3s 70ms/step - loss: 0.7877 - accuracy: 0.6908\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 3s 70ms/step - loss: 0.7270 - accuracy: 0.7198\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 0.7338 - accuracy: 0.7139\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.6693 - accuracy: 0.7388\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.6349 - accuracy: 0.7555\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 0.5981 - accuracy: 0.7710\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.5733 - accuracy: 0.7763\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.6241 - accuracy: 0.7598\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.5682 - accuracy: 0.7869\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.5254 - accuracy: 0.8016\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.5160 - accuracy: 0.8055\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.4739 - accuracy: 0.8314\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.4431 - accuracy: 0.8376\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 3s 63ms/step - loss: 0.4116 - accuracy: 0.8498\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.4004 - accuracy: 0.8569\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.4054 - accuracy: 0.8553\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3874 - accuracy: 0.8620\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.3673 - accuracy: 0.8708\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3570 - accuracy: 0.8763\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.3644 - accuracy: 0.8714\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.3637 - accuracy: 0.8706\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3488 - accuracy: 0.8796\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3518 - accuracy: 0.8755\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 3s 65ms/step - loss: 0.3309 - accuracy: 0.8855\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.3004 - accuracy: 0.8971\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.2874 - accuracy: 0.9033\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.2900 - accuracy: 0.8992\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 3s 64ms/step - loss: 0.2842 - accuracy: 0.9004\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 3s 67ms/step - loss: 0.2643 - accuracy: 0.9116\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 3s 66ms/step - loss: 0.2549 - accuracy: 0.9131\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 3s 68ms/step - loss: 0.2638 - accuracy: 0.9090\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 1.7343 - accuracy: 0.6024\n",
            "[1.7343302965164185, 0.6023809313774109]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "outputId": "554a62bf-932c-4d6e-e00b-437722656683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.3779 - accuracy: 0.3112\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.3661 - accuracy: 0.3278\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.3499 - accuracy: 0.3473\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3296 - accuracy: 0.3557\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.3187 - accuracy: 0.3671\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.3123 - accuracy: 0.3708\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.2965 - accuracy: 0.3796\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.2888 - accuracy: 0.3892\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.2731 - accuracy: 0.4147\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.2598 - accuracy: 0.4253\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.2298 - accuracy: 0.4400\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.2112 - accuracy: 0.4567\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.2009 - accuracy: 0.4682\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.1732 - accuracy: 0.4853\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.1747 - accuracy: 0.4802\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1403 - accuracy: 0.5129\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1118 - accuracy: 0.5239\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1259 - accuracy: 0.5173\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.1550 - accuracy: 0.5049\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.0927 - accuracy: 0.5414\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.0985 - accuracy: 0.5406\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0731 - accuracy: 0.5555\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.0731 - accuracy: 0.5437\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0661 - accuracy: 0.5469\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 1.0517 - accuracy: 0.5647\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0899 - accuracy: 0.5429\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0668 - accuracy: 0.5541\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 0.9974 - accuracy: 0.5871\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9600 - accuracy: 0.6137\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9611 - accuracy: 0.6143\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 4s 86ms/step - loss: 1.0074 - accuracy: 0.5829\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0788 - accuracy: 0.5439\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0335 - accuracy: 0.5692\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9814 - accuracy: 0.5978\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9555 - accuracy: 0.6043\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0076 - accuracy: 0.5845\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 1.0020 - accuracy: 0.5884\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 0.9660 - accuracy: 0.6129\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 0.9448 - accuracy: 0.6173\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 0.9071 - accuracy: 0.6433\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 0.8614 - accuracy: 0.6537\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 0.8542 - accuracy: 0.6614\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 0.8764 - accuracy: 0.6390\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9195 - accuracy: 0.6202\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9245 - accuracy: 0.6139\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 0.9038 - accuracy: 0.6271\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 4s 87ms/step - loss: 0.9232 - accuracy: 0.6137\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 4s 90ms/step - loss: 0.9226 - accuracy: 0.6212\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 4s 88ms/step - loss: 0.9428 - accuracy: 0.6131\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 4s 89ms/step - loss: 1.0142 - accuracy: 0.5798\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 1.7462 - accuracy: 0.3067\n",
            "[1.746240258216858, 0.30666667222976685]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "outputId": "5febe7ab-17c0-4cf7-d723-d211b8912914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_lstm = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 1.3737 - accuracy: 0.3200\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.2772 - accuracy: 0.4216\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.2247 - accuracy: 0.4527\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.1993 - accuracy: 0.4602\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.1508 - accuracy: 0.4800\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.0881 - accuracy: 0.5047\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.0205 - accuracy: 0.5459\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.9461 - accuracy: 0.5914\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8952 - accuracy: 0.6171\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.8495 - accuracy: 0.6441\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.7486 - accuracy: 0.6920\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.7054 - accuracy: 0.7182\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6720 - accuracy: 0.7373\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6560 - accuracy: 0.7410\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6154 - accuracy: 0.7627\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6102 - accuracy: 0.7688\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6584 - accuracy: 0.7559\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.5816 - accuracy: 0.7929\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.5286 - accuracy: 0.8155\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.4867 - accuracy: 0.8363\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.4599 - accuracy: 0.8467\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.4448 - accuracy: 0.8565\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.4183 - accuracy: 0.8629\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3588 - accuracy: 0.8876\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3506 - accuracy: 0.8957\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.3388 - accuracy: 0.9018\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3267 - accuracy: 0.9041\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3111 - accuracy: 0.9120\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3370 - accuracy: 0.9012\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3290 - accuracy: 0.9041\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3342 - accuracy: 0.9002\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3445 - accuracy: 0.8965\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2996 - accuracy: 0.9173\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2638 - accuracy: 0.9312\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2514 - accuracy: 0.9329\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2646 - accuracy: 0.9284\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2821 - accuracy: 0.9214\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2658 - accuracy: 0.9282\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2504 - accuracy: 0.9357\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2251 - accuracy: 0.9414\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2488 - accuracy: 0.9324\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2394 - accuracy: 0.9357\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.3204 - accuracy: 0.9106\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2434 - accuracy: 0.9318\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2496 - accuracy: 0.9347\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2297 - accuracy: 0.9406\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2245 - accuracy: 0.9422\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2119 - accuracy: 0.9469\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2111 - accuracy: 0.9455\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2125 - accuracy: 0.9439\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 0.8873 - accuracy: 0.7638\n",
            "[0.8872970938682556, 0.7638095021247864]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "outputId": "406ca738-2698-4faf-ea7f-5875553d10cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.3668 - accuracy: 0.3271\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.2331 - accuracy: 0.4567\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.0598 - accuracy: 0.5594\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.9081 - accuracy: 0.6461\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.7857 - accuracy: 0.7141\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.6842 - accuracy: 0.7514\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.5976 - accuracy: 0.7855\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.5263 - accuracy: 0.8118\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.4654 - accuracy: 0.8386\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.4039 - accuracy: 0.8676\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3473 - accuracy: 0.8861\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3000 - accuracy: 0.9047\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2626 - accuracy: 0.9167\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2399 - accuracy: 0.9253\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2397 - accuracy: 0.9186\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2268 - accuracy: 0.9237\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2010 - accuracy: 0.9333\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2111 - accuracy: 0.9259\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2196 - accuracy: 0.9212\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1806 - accuracy: 0.9365\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1374 - accuracy: 0.9567\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1261 - accuracy: 0.9590\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1238 - accuracy: 0.9604\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1191 - accuracy: 0.9608\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1108 - accuracy: 0.9610\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1080 - accuracy: 0.9627\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1087 - accuracy: 0.9622\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.1028 - accuracy: 0.9657\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0988 - accuracy: 0.9665\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0931 - accuracy: 0.9673\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0773 - accuracy: 0.9706\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0680 - accuracy: 0.9716\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0657 - accuracy: 0.9692\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0633 - accuracy: 0.9716\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0602 - accuracy: 0.9720\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0593 - accuracy: 0.9733\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0685 - accuracy: 0.9704\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0630 - accuracy: 0.9710\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0566 - accuracy: 0.9739\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0554 - accuracy: 0.9722\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0583 - accuracy: 0.9727\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0566 - accuracy: 0.9731\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0536 - accuracy: 0.9733\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0492 - accuracy: 0.9749\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0527 - accuracy: 0.9743\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0513 - accuracy: 0.9741\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0487 - accuracy: 0.9735\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0507 - accuracy: 0.9739\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0630 - accuracy: 0.9716\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.0509 - accuracy: 0.9767\n",
            "21/21 [==============================] - 0s 13ms/step - loss: 1.1748 - accuracy: 0.7586\n",
            "[1.174773097038269, 0.758571445941925]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "outputId": "4b7fe7fb-9b8b-46c6-badf-d45626087873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 1.3793 - accuracy: 0.3020\n",
            "Epoch 2/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.3655 - accuracy: 0.3318\n",
            "Epoch 3/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.3349 - accuracy: 0.3686\n",
            "Epoch 4/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.3019 - accuracy: 0.3892\n",
            "Epoch 5/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 1.2876 - accuracy: 0.4051\n",
            "Epoch 6/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.2514 - accuracy: 0.4298\n",
            "Epoch 7/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 1.2304 - accuracy: 0.4457\n",
            "Epoch 8/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.2135 - accuracy: 0.4498\n",
            "Epoch 9/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.1859 - accuracy: 0.4627\n",
            "Epoch 10/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 1.1552 - accuracy: 0.4824\n",
            "Epoch 11/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.1182 - accuracy: 0.5055\n",
            "Epoch 12/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.0924 - accuracy: 0.5231\n",
            "Epoch 13/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.0663 - accuracy: 0.5359\n",
            "Epoch 14/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0679 - accuracy: 0.5322\n",
            "Epoch 15/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0784 - accuracy: 0.5222\n",
            "Epoch 16/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0754 - accuracy: 0.5231\n",
            "Epoch 17/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0283 - accuracy: 0.5559\n",
            "Epoch 18/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.9933 - accuracy: 0.5751\n",
            "Epoch 19/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.9732 - accuracy: 0.5880\n",
            "Epoch 20/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9869 - accuracy: 0.5798\n",
            "Epoch 21/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0666 - accuracy: 0.5384\n",
            "Epoch 22/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.0633 - accuracy: 0.5469\n",
            "Epoch 23/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9927 - accuracy: 0.5808\n",
            "Epoch 24/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9519 - accuracy: 0.5982\n",
            "Epoch 25/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.9343 - accuracy: 0.5976\n",
            "Epoch 26/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.9249 - accuracy: 0.6037\n",
            "Epoch 27/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.9184 - accuracy: 0.6153\n",
            "Epoch 28/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9380 - accuracy: 0.6014\n",
            "Epoch 29/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.8830 - accuracy: 0.6337\n",
            "Epoch 30/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.8443 - accuracy: 0.6571\n",
            "Epoch 31/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.8192 - accuracy: 0.6680\n",
            "Epoch 32/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.8356 - accuracy: 0.6614\n",
            "Epoch 33/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.8584 - accuracy: 0.6435\n",
            "Epoch 34/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9153 - accuracy: 0.6196\n",
            "Epoch 35/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.9438 - accuracy: 0.6043\n",
            "Epoch 36/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9226 - accuracy: 0.6088\n",
            "Epoch 37/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.8892 - accuracy: 0.6210\n",
            "Epoch 38/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.8491 - accuracy: 0.6422\n",
            "Epoch 39/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.8171 - accuracy: 0.6663\n",
            "Epoch 40/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.8266 - accuracy: 0.6569\n",
            "Epoch 41/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.8165 - accuracy: 0.6655\n",
            "Epoch 42/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.7910 - accuracy: 0.6718\n",
            "Epoch 43/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.7958 - accuracy: 0.6745\n",
            "Epoch 44/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.8181 - accuracy: 0.6637\n",
            "Epoch 45/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.9293 - accuracy: 0.6196\n",
            "Epoch 46/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0046 - accuracy: 0.5839\n",
            "Epoch 47/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.9218 - accuracy: 0.6106\n",
            "Epoch 48/50\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 0.8424 - accuracy: 0.6508\n",
            "Epoch 49/50\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.7697 - accuracy: 0.6959\n",
            "Epoch 50/50\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 0.7187 - accuracy: 0.7108\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.7332 - accuracy: 0.3824\n",
            "[1.733182430267334, 0.38238096237182617]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "outputId": "a803f945-066b-4de0-b54d-77b986747afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])\r\n",
        "print(history_sg_bi.history)\r\n",
        "plt.title(\"Skip-Gram + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()\r\n",
        "plt.title(\"GloVe + Bi-LSTM accuracy, loss vs epochs Graph\")\r\n",
        "plt.plot(history_glove_bi.history['loss'],c='b',label='loss')\r\n",
        "plt.plot(history_glove_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.48476189374923706, 0.616190493106842, 0.30714285373687744]\n",
            "cbow [0.476666659116745, 0.6023809313774109, 0.30666667222976685]\n",
            "glove [0.7638095021247864, 0.758571445941925, 0.38238096237182617]\n",
            "{'loss': [1.3721295595169067, 1.3492070436477661, 1.3116365671157837, 1.2711528539657593, 1.242195725440979, 1.213836669921875, 1.1846592426300049, 1.156532645225525, 1.1334924697875977, 1.1038926839828491, 1.075216293334961, 1.046500325202942, 1.019684076309204, 0.9863032102584839, 0.9486438632011414, 0.9131025671958923, 0.8862888813018799, 0.8321577310562134, 0.7898989319801331, 0.75534987449646, 0.6981725096702576, 0.7095126509666443, 0.6490120887756348, 0.6933291554450989, 0.6249576807022095, 0.5645757913589478, 0.528995156288147, 0.5130370259284973, 0.4796333909034729, 0.4743385910987854, 0.46333804726600647, 0.4541698098182678, 0.42636919021606445, 0.3821522295475006, 0.3736085891723633, 0.353844553232193, 0.34362635016441345, 0.3870014548301697, 0.37077343463897705, 0.3245290219783783, 0.45993906259536743, 0.3180096745491028, 0.28035515546798706, 0.26513922214508057, 0.2511211931705475, 0.24678601324558258, 0.24255099892616272, 0.24609659612178802, 0.25204071402549744, 0.22856736183166504], 'accuracy': [0.3193877637386322, 0.3383673429489136, 0.3834693729877472, 0.4077551066875458, 0.4379591941833496, 0.45632654428482056, 0.4787755012512207, 0.4985714256763458, 0.5155102014541626, 0.5295918583869934, 0.5438775420188904, 0.5534693598747253, 0.5759183764457703, 0.5902040600776672, 0.6053061485290527, 0.6328571438789368, 0.6469388008117676, 0.6710203886032104, 0.6924489736557007, 0.704285740852356, 0.7297959327697754, 0.7259183526039124, 0.7534694075584412, 0.741224467754364, 0.7653061151504517, 0.7883673310279846, 0.8053061366081238, 0.808571457862854, 0.8248979449272156, 0.8212245106697083, 0.836122453212738, 0.8369387984275818, 0.8442857265472412, 0.863265335559845, 0.8669387698173523, 0.8767346739768982, 0.8793877363204956, 0.8610203862190247, 0.8693877458572388, 0.8877550959587097, 0.8391836881637573, 0.8928571343421936, 0.9063265323638916, 0.9128571152687073, 0.9169387817382812, 0.9167346954345703, 0.9181632399559021, 0.9163265228271484, 0.9157142639160156, 0.927142858505249]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}