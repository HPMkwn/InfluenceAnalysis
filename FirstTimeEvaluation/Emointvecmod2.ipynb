{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emointvecmod2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/Emointvecmod2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74eb6957-aae0-41a7-8ed8-c7e14ea458b9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Data/Tweets_clean_data.csv')\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {\"anger\":[1,0,0,0],\"sadness\":[0,1,0,0],\"fear\":[0,0,1,0],\"joy\":[0,0,0,1]}\n",
        "data_tweet = [x.lower().split() for x in df['Tweet']]\n",
        "data_cat = np.array([category_dict[x] for x in df['Emotion']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PmA3_Ihp-Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0c14e6-ff74-46e2-c0ba-f63929fcb966"
      },
      "source": [
        "print(data_tweet[:5])\n",
        "print(data_cat[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['why', 'should', 'i', 'listen', 'to', 'someone', 'with', 'a', 'tie', 'like', 'that'], ['do', 'not', 'let', 'the', 'behavior', 'of', 'others', 'destroy', 'ur', 'inner', 'peace', 'dalai', 'lama', 'healing', 'depression', 'anxiety', 'intuition', 'meditation', 'bok'], ['waltz', 'with', 'bashir', 'was', 'incredible', 'tho', 'i', 'think', 'it', 'is', 'more', 'of', 'an', 'animated', 'film', 'than', 'documentary', 'about', 'lebanon', 'war'], ['michelle', 'who', 'did', 'nothing', 'is', 'hating', 'on', 'nicole', 's', 'game', 'hahaha', 'bitter', 'bb18'], ['just', 'joined', 'pottermore', 'and', 'was', 'sorted', 'into', 'hufflepuff', 'pouting', 'facepouting', 'facepouting', 'face']]\n",
            "[[0 0 1 0]\n",
            " [0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b95560-99a7-471e-c127-3b2762738ff0"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(475486, 595345)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 100\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor = 1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fue2npdFINi"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_tweet, data_cat, test_size=0.30, random_state=42)\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.get_vector(i[0]),i[1]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])      \r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8db7e09-5770-4466-c966-99b28c9add59"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-bddb9a303a31>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 1.3773 - accuracy: 0.3120\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3752 - accuracy: 0.3153\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3752 - accuracy: 0.3153\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3752 - accuracy: 0.3153\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3751 - accuracy: 0.3153\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3751 - accuracy: 0.3153\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3748 - accuracy: 0.3153\n",
            "21/21 [==============================] - 1s 54ms/step - loss: 1.3741 - accuracy: 0.3214\n",
            "[1.374060869216919, 0.3214285671710968]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHtKjzc1SA8",
        "outputId": "022a0d39-4a00-4c98-bbe0-14d056764b59"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3769 - accuracy: 0.3143\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3747 - accuracy: 0.3153\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3745 - accuracy: 0.3153\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3743 - accuracy: 0.3153\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3741 - accuracy: 0.3153\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3736 - accuracy: 0.3153\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3728 - accuracy: 0.3153\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3717 - accuracy: 0.3131\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3712 - accuracy: 0.3153\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3687 - accuracy: 0.3155\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.4095 - accuracy: 0.3192\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3720 - accuracy: 0.3153\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3694 - accuracy: 0.3153\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 3s 61ms/step - loss: 1.3687 - accuracy: 0.3153\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3679 - accuracy: 0.3153\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3670 - accuracy: 0.3153\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3658 - accuracy: 0.3153\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3643 - accuracy: 0.3176\n",
            "21/21 [==============================] - 1s 56ms/step - loss: 1.3688 - accuracy: 0.3224\n",
            "[1.3688297271728516, 0.3223809599876404]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opfhhYPH1VoL",
        "outputId": "4d5dd4c4-2bd4-4e89-ae3e-ea32b569ce51"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3766 - accuracy: 0.3118\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3716 - accuracy: 0.3206\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3686 - accuracy: 0.3192\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3714 - accuracy: 0.3182\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3675 - accuracy: 0.3245\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3747 - accuracy: 0.3145\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 4s 84ms/step - loss: 1.3693 - accuracy: 0.3180\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3693 - accuracy: 0.3184\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3654 - accuracy: 0.3261\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 1.3650 - accuracy: 0.3229\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3712 - accuracy: 0.3194\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3568 - accuracy: 0.3347\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3619 - accuracy: 0.3294\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3775 - accuracy: 0.3106\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3776 - accuracy: 0.3153\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3759 - accuracy: 0.3153\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 4s 83ms/step - loss: 1.3753 - accuracy: 0.3157\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3755 - accuracy: 0.3153\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3738 - accuracy: 0.3151\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3769 - accuracy: 0.3092\n",
            "21/21 [==============================] - 1s 55ms/step - loss: 1.3743 - accuracy: 0.3214\n",
            "[1.3743339776992798, 0.3214285671710968]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YumD1M9Y1Xxr",
        "outputId": "7e768caf-e05c-4ee5-da66-4813275829b0"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3770 - accuracy: 0.3131\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3753 - accuracy: 0.3153\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 3s 61ms/step - loss: 1.3751 - accuracy: 0.3153\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3752 - accuracy: 0.3153\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3751 - accuracy: 0.3153\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3751 - accuracy: 0.3153\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3751 - accuracy: 0.3153\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 3s 62ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 3s 56ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 3s 57ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3748 - accuracy: 0.3153\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3748 - accuracy: 0.3153\n",
            "21/21 [==============================] - 1s 55ms/step - loss: 1.3741 - accuracy: 0.3214\n",
            "[1.374089241027832, 0.3214285671710968]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01yVMiF41ZtT",
        "outputId": "191221bb-fc2f-4b76-b50c-53df52984fbb"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3770 - accuracy: 0.3124\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 3s 61ms/step - loss: 1.3750 - accuracy: 0.3153\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3749 - accuracy: 0.3153\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3748 - accuracy: 0.3153\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3746 - accuracy: 0.3153\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3744 - accuracy: 0.3153\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3742 - accuracy: 0.3153\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3739 - accuracy: 0.3153\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3734 - accuracy: 0.3153\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3730 - accuracy: 0.3145\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3718 - accuracy: 0.3153\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 3s 58ms/step - loss: 1.3699 - accuracy: 0.3153\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3659 - accuracy: 0.3192\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3674 - accuracy: 0.3153\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3638 - accuracy: 0.3171\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3592 - accuracy: 0.3284\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3666 - accuracy: 0.3153\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 3s 59ms/step - loss: 1.3650 - accuracy: 0.3165\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3613 - accuracy: 0.3227\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 1.3528 - accuracy: 0.3337\n",
            "21/21 [==============================] - 1s 54ms/step - loss: 1.3655 - accuracy: 0.3271\n",
            "[1.365541934967041, 0.3271428644657135]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_G3Ff5r1bSD",
        "outputId": "b5eb6421-ce49-4f15-b603-8d1491f2efb4"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3781 - accuracy: 0.3110\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3745 - accuracy: 0.3141\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3729 - accuracy: 0.3180\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3723 - accuracy: 0.3151\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3705 - accuracy: 0.3188\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3833 - accuracy: 0.3080\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3698 - accuracy: 0.3088\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3686 - accuracy: 0.3135\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3705 - accuracy: 0.3241\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3678 - accuracy: 0.3173\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3680 - accuracy: 0.3210\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3685 - accuracy: 0.3165\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3610 - accuracy: 0.3206\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 1.3618 - accuracy: 0.3220\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3656 - accuracy: 0.3182\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 4s 81ms/step - loss: 1.3633 - accuracy: 0.3257\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3587 - accuracy: 0.3237\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3574 - accuracy: 0.3312\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 4s 82ms/step - loss: 1.3536 - accuracy: 0.3304\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 4s 80ms/step - loss: 1.3509 - accuracy: 0.3418\n",
            "21/21 [==============================] - 1s 56ms/step - loss: 1.3774 - accuracy: 0.3238\n",
            "[1.3773947954177856, 0.32380953431129456]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgUwBHxCw-lB"
      },
      "source": [
        "epochs=25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czoFmEwK1dWT",
        "outputId": "e40a4c0d-7bdb-42b3-ddff-c060ad0fd606"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.3765 - accuracy: 0.3108\n",
            "Epoch 2/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.2852 - accuracy: 0.3941\n",
            "Epoch 3/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.2239 - accuracy: 0.4308\n",
            "Epoch 4/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.1756 - accuracy: 0.4522\n",
            "Epoch 5/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.1491 - accuracy: 0.4653\n",
            "Epoch 6/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.1156 - accuracy: 0.4798\n",
            "Epoch 7/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.0735 - accuracy: 0.5041\n",
            "Epoch 8/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.0304 - accuracy: 0.5378\n",
            "Epoch 9/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 1.0054 - accuracy: 0.5502\n",
            "Epoch 10/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.9895 - accuracy: 0.5651\n",
            "Epoch 11/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.9160 - accuracy: 0.6002\n",
            "Epoch 12/25\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.8931 - accuracy: 0.6073\n",
            "Epoch 13/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8989 - accuracy: 0.6049\n",
            "Epoch 14/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8995 - accuracy: 0.6073\n",
            "Epoch 15/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8260 - accuracy: 0.6484\n",
            "Epoch 16/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8131 - accuracy: 0.6502\n",
            "Epoch 17/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.7770 - accuracy: 0.6773\n",
            "Epoch 18/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8404 - accuracy: 0.6369\n",
            "Epoch 19/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8368 - accuracy: 0.6324\n",
            "Epoch 20/25\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.7900 - accuracy: 0.6643\n",
            "Epoch 21/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.7355 - accuracy: 0.6959\n",
            "Epoch 22/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.7048 - accuracy: 0.7124\n",
            "Epoch 23/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6830 - accuracy: 0.7186\n",
            "Epoch 24/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6657 - accuracy: 0.7308\n",
            "Epoch 25/25\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.6596 - accuracy: 0.7341\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 1.1940 - accuracy: 0.5805\n",
            "[1.1940250396728516, 0.5804761648178101]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKM9PEwm1fkb",
        "outputId": "c8db693a-73f8-404b-e577-50f48aada28f"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.3557 - accuracy: 0.3359\n",
            "Epoch 2/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.2300 - accuracy: 0.4582\n",
            "Epoch 3/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.1090 - accuracy: 0.5316\n",
            "Epoch 4/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 1.0158 - accuracy: 0.5839\n",
            "Epoch 5/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.9382 - accuracy: 0.6157\n",
            "Epoch 6/25\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.8700 - accuracy: 0.6551\n",
            "Epoch 7/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.8075 - accuracy: 0.6859\n",
            "Epoch 8/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.7505 - accuracy: 0.7116\n",
            "Epoch 9/25\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.7013 - accuracy: 0.7327\n",
            "Epoch 10/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.6561 - accuracy: 0.7514\n",
            "Epoch 11/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.6138 - accuracy: 0.7706\n",
            "Epoch 12/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.5728 - accuracy: 0.7859\n",
            "Epoch 13/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.5312 - accuracy: 0.8045\n",
            "Epoch 14/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.4985 - accuracy: 0.8173\n",
            "Epoch 15/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.4744 - accuracy: 0.8271\n",
            "Epoch 16/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.4798 - accuracy: 0.8243\n",
            "Epoch 17/25\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.4721 - accuracy: 0.8229\n",
            "Epoch 18/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.4079 - accuracy: 0.8512\n",
            "Epoch 19/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3808 - accuracy: 0.8592\n",
            "Epoch 20/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3729 - accuracy: 0.8639\n",
            "Epoch 21/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3744 - accuracy: 0.8592\n",
            "Epoch 22/25\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.3526 - accuracy: 0.8718\n",
            "Epoch 23/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3269 - accuracy: 0.8796\n",
            "Epoch 24/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.3041 - accuracy: 0.8920\n",
            "Epoch 25/25\n",
            "49/49 [==============================] - 1s 15ms/step - loss: 0.2874 - accuracy: 0.8976\n",
            "21/21 [==============================] - 0s 12ms/step - loss: 1.0139 - accuracy: 0.6905\n",
            "[1.0138580799102783, 0.6904761791229248]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQBa5s4s1hNU",
        "outputId": "17168e15-4110-4797-c6b1-8fd808537d19"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.3700 - accuracy: 0.3108\n",
            "Epoch 2/25\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.3131 - accuracy: 0.3784\n",
            "Epoch 3/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.2842 - accuracy: 0.3965\n",
            "Epoch 4/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.2550 - accuracy: 0.4222\n",
            "Epoch 5/25\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.2244 - accuracy: 0.4359\n",
            "Epoch 6/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.2021 - accuracy: 0.4557\n",
            "Epoch 7/25\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 1.1891 - accuracy: 0.4629\n",
            "Epoch 8/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.1747 - accuracy: 0.4831\n",
            "Epoch 9/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.1554 - accuracy: 0.4912\n",
            "Epoch 10/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.1398 - accuracy: 0.5035\n",
            "Epoch 11/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.1537 - accuracy: 0.5018\n",
            "Epoch 12/25\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 1.1763 - accuracy: 0.4927\n",
            "Epoch 13/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.1199 - accuracy: 0.5200\n",
            "Epoch 14/25\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 1.0894 - accuracy: 0.5418\n",
            "Epoch 15/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.0777 - accuracy: 0.5576\n",
            "Epoch 16/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.0507 - accuracy: 0.5633\n",
            "Epoch 17/25\n",
            "49/49 [==============================] - 2s 46ms/step - loss: 1.0692 - accuracy: 0.5586\n",
            "Epoch 18/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.0752 - accuracy: 0.5518\n",
            "Epoch 19/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.0598 - accuracy: 0.5584\n",
            "Epoch 20/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 1.0066 - accuracy: 0.5861\n",
            "Epoch 21/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.9780 - accuracy: 0.5947\n",
            "Epoch 22/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.9685 - accuracy: 0.6080\n",
            "Epoch 23/25\n",
            "49/49 [==============================] - 2s 44ms/step - loss: 0.9611 - accuracy: 0.6094\n",
            "Epoch 24/25\n",
            "49/49 [==============================] - 2s 45ms/step - loss: 1.0101 - accuracy: 0.5898\n",
            "Epoch 25/25\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 1.0111 - accuracy: 0.5816\n",
            "21/21 [==============================] - 0s 14ms/step - loss: 1.3662 - accuracy: 0.3986\n",
            "[1.3661569356918335, 0.3985714316368103]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZqN_SaM3hs9",
        "outputId": "99afdad3-bf00-46d2-c207-1bbfd89fdcc9"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.3214285671710968, 0.3223809599876404, 0.3214285671710968]\n",
            "cbow [0.3214285671710968, 0.3271428644657135, 0.32380953431129456]\n",
            "glove [0.5804761648178101, 0.6904761791229248, 0.3985714316368103]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}