{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEkphrasisvec4negpos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEkphrasisvec4negpos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49220b3-c33c-4f23-890c-b2976f26924d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/train_clean.csv',names=['Tweet','Emotion'])\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/test_clean.csv',names=['Tweet','Emotion'])\n",
        "df_train = df_train.sample(frac=1)\n",
        "df_test = df_train.sample(frac=1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zrtpGceAiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "c433c246-ffd5-4177-b61f-fb873aafd129"
      },
      "source": [
        "df_train.head(1000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5306</th>\n",
              "      <td>mikeabram link doe not work</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>thedailysurvey whi do we love those that we ca...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8880</th>\n",
              "      <td>pauline i know I am so excit we cannot go earl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6834</th>\n",
              "      <td>freesat coverag sky dish is realli realli bad ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4919</th>\n",
              "      <td>td_keepsitr haha awe well right now i could be...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9877</th>\n",
              "      <td>joeysmissmac blah got a shot at the hospit tak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5965</th>\n",
              "      <td>hate live down here in fl miss ga like crazyli...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>sophiaf3f3 word I am alway up</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1693</th>\n",
              "      <td>3wordsaftersex it hurt now</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8845</th>\n",
              "      <td>school over I am go to miss a bunch of peopl</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet Emotion\n",
              "5306                        mikeabram link doe not work       0\n",
              "752   thedailysurvey whi do we love those that we ca...       2\n",
              "8880  pauline i know I am so excit we cannot go earl...       1\n",
              "6834  freesat coverag sky dish is realli realli bad ...       2\n",
              "4919  td_keepsitr haha awe well right now i could be...       1\n",
              "...                                                 ...     ...\n",
              "9877  joeysmissmac blah got a shot at the hospit tak...       0\n",
              "5965  hate live down here in fl miss ga like crazyli...       2\n",
              "2492                      sophiaf3f3 word I am alway up       0\n",
              "1693                         3wordsaftersex it hurt now       2\n",
              "8845       school over I am go to miss a bunch of peopl       2\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K68sxL_LfLy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269f86de-742a-4619-e9eb-2e964074d63f"
      },
      "source": [
        "df_test.Emotion.unique()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['2', '0', '1', '3', 'emotion'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9KFGTW2iP8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f567a8-7377-41d4-c99b-853c3ca5567f"
      },
      "source": [
        "df_train.nunique()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tweet      12370\n",
              "Emotion        5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {'0':[1,0,0,0],'1':[0,1,0,0],'2':[0,0,1,0],'3':[0,0,0,1],'emotion':[1,0,0,0]}\n",
        "train_data_tweet = [x.lower().split() for x in df_train['Tweet']]\n",
        "train_data_cat = np.array([category_dict[x] for x in df_train['Emotion']])\n",
        "test_data_tweet = [x.lower().split() for x in df_test['Tweet']]\n",
        "test_data_cat = np.array([category_dict[x] for x in df_test['Emotion']])\n",
        "\n",
        "data_tweet = train_data_tweet + test_data_tweet"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ksBvx9Z_ws"
      },
      "source": [
        "def save_dict_to_file(dic):\r\n",
        "    f = open(\"/content/drive/My Drive/InfluenceAnalysis/vocab1.txt\",'w')\r\n",
        "    for i in dic:\r\n",
        "      f.write(str(i)+\"\\t\")\r\n",
        "      f.write(str(w2v_sg.wv.get_vector(i)))\r\n",
        "      f.write(\"\\n\")\r\n",
        "    f.close()\r\n",
        "save_dict_to_file(vocab_sg)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 500\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor=1.5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z65iXVhlpeDX"
      },
      "source": [
        "positive = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/positive-words.csv')\r\n",
        "positive = list(positive['0'][1:])\r\n",
        "negative = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/negative-words.csv')\r\n",
        "negative = list(negative['0'][2:])\r\n",
        "pn_dict = {}\r\n",
        "for i in range(len(positive)):\r\n",
        "  pn_dict[positive[i]] = 1\r\n",
        "\r\n",
        "for i in range(len(negative)):\r\n",
        "  pn_dict[negative[i]] = -1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqvzy05D2iim"
      },
      "source": [
        "X_train = train_data_tweet\r\n",
        "X_test = test_data_tweet\r\n",
        "y_train = train_data_cat\r\n",
        "y_test = test_data_cat \r\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\r\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\r\n",
        "X_train = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_train]\r\n",
        "X_test = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_test]\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIGpYOBRx-kf"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          tweet[i]=((tweet[i][0]*tweet[i][2]),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN' or tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          tweet[i]=((tweet[i][0]*tweet[i][2]),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN' or tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 40"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa52c16-55bd-4c25-a170-2ea7bbf6a74b"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-bddb9a303a31>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/40\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 1.2620 - accuracy: 0.5692\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 1.0067 - accuracy: 0.5869\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9836 - accuracy: 0.5869\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9824 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9803 - accuracy: 0.5869\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9765 - accuracy: 0.5869\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9725 - accuracy: 0.5869\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9678 - accuracy: 0.5866\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9649 - accuracy: 0.5866\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9611 - accuracy: 0.5867\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9566 - accuracy: 0.5854\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 261ms/step - loss: 0.9521 - accuracy: 0.5853\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 7s 272ms/step - loss: 0.9481 - accuracy: 0.5850\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9445 - accuracy: 0.5867\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9407 - accuracy: 0.5878\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9364 - accuracy: 0.5900\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9317 - accuracy: 0.5939\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9261 - accuracy: 0.5949\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9176 - accuracy: 0.5999\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9058 - accuracy: 0.6095\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.8970 - accuracy: 0.6155\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8888 - accuracy: 0.6178\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.8779 - accuracy: 0.6263\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.8694 - accuracy: 0.6324\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.8605 - accuracy: 0.6386\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8521 - accuracy: 0.6437\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.8435 - accuracy: 0.6476\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.8350 - accuracy: 0.6531\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.8259 - accuracy: 0.6608\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.8157 - accuracy: 0.6699\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.8053 - accuracy: 0.6794\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.7966 - accuracy: 0.6858\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.7894 - accuracy: 0.6896\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.7858 - accuracy: 0.6903\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.7971 - accuracy: 0.6800\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.7895 - accuracy: 0.6839\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.7907 - accuracy: 0.6817\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.7721 - accuracy: 0.6929\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.7650 - accuracy: 0.6983\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7489 - accuracy: 0.7088\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7574 - accuracy: 0.7016\n",
            "[0.7573731541633606, 0.7015833258628845]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb20bf90-3ba9-4973-d6f6-eae15c1bc4bb"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 1.2075 - accuracy: 0.5446\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9941 - accuracy: 0.5869\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9740 - accuracy: 0.5870\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.9681 - accuracy: 0.5870\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9611 - accuracy: 0.5872\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9549 - accuracy: 0.5869\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.9493 - accuracy: 0.5878\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9444 - accuracy: 0.5890\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9404 - accuracy: 0.5895\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9362 - accuracy: 0.5913\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9323 - accuracy: 0.5926\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9283 - accuracy: 0.5934\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9240 - accuracy: 0.5951\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9197 - accuracy: 0.5978\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9150 - accuracy: 0.5997\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9101 - accuracy: 0.6015\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9048 - accuracy: 0.6034\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8988 - accuracy: 0.6063\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 262ms/step - loss: 0.8921 - accuracy: 0.6093\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.8845 - accuracy: 0.6124\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.8769 - accuracy: 0.6170\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 7s 277ms/step - loss: 0.8683 - accuracy: 0.6203\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8587 - accuracy: 0.6255\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 260ms/step - loss: 0.8511 - accuracy: 0.6280\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.8480 - accuracy: 0.6305\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.8560 - accuracy: 0.6263\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.8342 - accuracy: 0.6357\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.8208 - accuracy: 0.6411\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.8082 - accuracy: 0.6482\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.7951 - accuracy: 0.6548\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.7836 - accuracy: 0.6618\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.7738 - accuracy: 0.6665\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.7658 - accuracy: 0.6697\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.7682 - accuracy: 0.6669\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.7583 - accuracy: 0.6741\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.7430 - accuracy: 0.6805\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.7404 - accuracy: 0.6808\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 259ms/step - loss: 0.7441 - accuracy: 0.6766\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.7532 - accuracy: 0.6773\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.7450 - accuracy: 0.6808\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.7550 - accuracy: 0.6775\n",
            "[0.7549876570701599, 0.6775000095367432]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be82bfb-cd39-4236-a7aa-a8cfd8486d75"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 6s 260ms/step - loss: 1.1100 - accuracy: 0.5458\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9925 - accuracy: 0.5863\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.9852 - accuracy: 0.5868\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 7s 273ms/step - loss: 0.9844 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9838 - accuracy: 0.5871\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9831 - accuracy: 0.5872\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9820 - accuracy: 0.5872\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 260ms/step - loss: 0.9799 - accuracy: 0.5874\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9758 - accuracy: 0.5879\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 262ms/step - loss: 0.9696 - accuracy: 0.5880\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.9645 - accuracy: 0.5886\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9592 - accuracy: 0.5903\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.9518 - accuracy: 0.5922\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.9456 - accuracy: 0.5934\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9423 - accuracy: 0.5953\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9388 - accuracy: 0.5972\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9257 - accuracy: 0.5995\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9208 - accuracy: 0.6041\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9089 - accuracy: 0.6058\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9042 - accuracy: 0.6108\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.8890 - accuracy: 0.6134\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 262ms/step - loss: 0.8831 - accuracy: 0.6192\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.8700 - accuracy: 0.6229\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.8615 - accuracy: 0.6265\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 262ms/step - loss: 0.8494 - accuracy: 0.6342\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.8384 - accuracy: 0.6370\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 270ms/step - loss: 0.8270 - accuracy: 0.6440\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 7s 271ms/step - loss: 0.8191 - accuracy: 0.6456\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 7s 285ms/step - loss: 0.8177 - accuracy: 0.6442\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 7s 273ms/step - loss: 0.8090 - accuracy: 0.6452\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 261ms/step - loss: 0.7914 - accuracy: 0.6578\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.7732 - accuracy: 0.6669\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.7616 - accuracy: 0.6740\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 7s 273ms/step - loss: 0.7556 - accuracy: 0.6743\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.7579 - accuracy: 0.6751\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.7715 - accuracy: 0.6631\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.8138 - accuracy: 0.6437\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.8233 - accuracy: 0.6389\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.8058 - accuracy: 0.6482\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.7673 - accuracy: 0.6705\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.7656 - accuracy: 0.6731\n",
            "[0.7655823826789856, 0.6730833053588867]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dbe516-8c68-4f2d-c8c6-1328e6ae6a87"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 1.2425 - accuracy: 0.5744\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 1.0018 - accuracy: 0.5869\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9854 - accuracy: 0.5869\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.9841 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9830 - accuracy: 0.5869\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9810 - accuracy: 0.5869\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9770 - accuracy: 0.5869\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9718 - accuracy: 0.5869\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9659 - accuracy: 0.5869\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.9602 - accuracy: 0.5867\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9547 - accuracy: 0.5862\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.9512 - accuracy: 0.5861\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9471 - accuracy: 0.5867\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9433 - accuracy: 0.5885\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9396 - accuracy: 0.5900\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9355 - accuracy: 0.5910\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9308 - accuracy: 0.5927\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9255 - accuracy: 0.5938\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9190 - accuracy: 0.5943\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.9097 - accuracy: 0.5996\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8971 - accuracy: 0.6092\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8881 - accuracy: 0.6148\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.8786 - accuracy: 0.6193\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.8663 - accuracy: 0.6256\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.8582 - accuracy: 0.6288\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.8455 - accuracy: 0.6381\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.8377 - accuracy: 0.6447\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.8271 - accuracy: 0.6500\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.8172 - accuracy: 0.6552\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.8069 - accuracy: 0.6626\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.7989 - accuracy: 0.6682\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7917 - accuracy: 0.6752\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.7825 - accuracy: 0.6755\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7758 - accuracy: 0.6803\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.7809 - accuracy: 0.6799\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.7850 - accuracy: 0.6762\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.7654 - accuracy: 0.6873\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.7602 - accuracy: 0.6948\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7502 - accuracy: 0.7008\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.7407 - accuracy: 0.7063\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 0.7700 - accuracy: 0.6918\n",
            "[0.7700018882751465, 0.6918333172798157]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71731c0c-36a4-4758-dc6d-ae460d890d74"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " 2/24 [=>............................] - ETA: 1s - loss: 1.3798 - accuracy: 0.3080WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0428s vs `on_train_batch_end` time: 0.0780s). Check your callbacks.\n",
            "24/24 [==============================] - 6s 252ms/step - loss: 1.2170 - accuracy: 0.5454\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.9915 - accuracy: 0.5870\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.9748 - accuracy: 0.5869\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.9698 - accuracy: 0.5870\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.9647 - accuracy: 0.5870\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.9586 - accuracy: 0.5872\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9525 - accuracy: 0.5876\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 261ms/step - loss: 0.9478 - accuracy: 0.5879\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.9434 - accuracy: 0.5888\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.9392 - accuracy: 0.5890\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.9353 - accuracy: 0.5903\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.9316 - accuracy: 0.5911\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.9276 - accuracy: 0.5934\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.9238 - accuracy: 0.5947\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9198 - accuracy: 0.5959\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9156 - accuracy: 0.5974\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 261ms/step - loss: 0.9113 - accuracy: 0.5992\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 262ms/step - loss: 0.9068 - accuracy: 0.6004\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9021 - accuracy: 0.6016\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.8972 - accuracy: 0.6034\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8928 - accuracy: 0.6063\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.8877 - accuracy: 0.6095\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8816 - accuracy: 0.6108\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8745 - accuracy: 0.6137\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.8674 - accuracy: 0.6184\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8582 - accuracy: 0.6216\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.8476 - accuracy: 0.6269\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.8411 - accuracy: 0.6300\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8326 - accuracy: 0.6334\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8275 - accuracy: 0.6363\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.8347 - accuracy: 0.6332\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8092 - accuracy: 0.6445\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 256ms/step - loss: 0.8057 - accuracy: 0.6486\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 257ms/step - loss: 0.8006 - accuracy: 0.6489\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.7911 - accuracy: 0.6564\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.7804 - accuracy: 0.6628\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.7657 - accuracy: 0.6708\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.7576 - accuracy: 0.6738\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.7480 - accuracy: 0.6798\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.7468 - accuracy: 0.6781\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.7634 - accuracy: 0.6695\n",
            "[0.7634326219558716, 0.6694999933242798]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e532eb3-5132-4398-a91b-3b72bcb31a18"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 1.1167 - accuracy: 0.5263\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9869 - accuracy: 0.5868\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9856 - accuracy: 0.5869\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 7s 288ms/step - loss: 0.9856 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 7s 281ms/step - loss: 0.9855 - accuracy: 0.5869\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9853 - accuracy: 0.5869\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9853 - accuracy: 0.5869\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 7s 272ms/step - loss: 0.9853 - accuracy: 0.5869\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9853 - accuracy: 0.5869\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 269ms/step - loss: 0.9852 - accuracy: 0.5869\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 260ms/step - loss: 0.9852 - accuracy: 0.5869\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.9851 - accuracy: 0.5869\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 262ms/step - loss: 0.9851 - accuracy: 0.5869\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.9850 - accuracy: 0.5869\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9850 - accuracy: 0.5869\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.9849 - accuracy: 0.5869\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9848 - accuracy: 0.5870\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 269ms/step - loss: 0.9846 - accuracy: 0.5870\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.9842 - accuracy: 0.5872\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 261ms/step - loss: 0.9835 - accuracy: 0.5872\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9835 - accuracy: 0.5872\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9817 - accuracy: 0.5874\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.9809 - accuracy: 0.5878\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 7s 273ms/step - loss: 0.9787 - accuracy: 0.5875\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.9758 - accuracy: 0.5882\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9717 - accuracy: 0.5890\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 269ms/step - loss: 0.9671 - accuracy: 0.5900\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9625 - accuracy: 0.5922\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9597 - accuracy: 0.5923\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 261ms/step - loss: 0.9534 - accuracy: 0.5946\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9480 - accuracy: 0.5968\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9399 - accuracy: 0.5999\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9336 - accuracy: 0.6023\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9307 - accuracy: 0.6039\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9491 - accuracy: 0.5972\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 268ms/step - loss: 0.9478 - accuracy: 0.5947\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.9373 - accuracy: 0.6003\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.9402 - accuracy: 0.5973\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9275 - accuracy: 0.6011\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.9252 - accuracy: 0.6035\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9351 - accuracy: 0.5979\n",
            "[0.9351116418838501, 0.5979166626930237]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEglKIfExKKI"
      },
      "source": [
        "epochs=40\r\n",
        "Embedding_size=100"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb44b54-6549-473d-e1e5-243ecfdae148"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 1.2234 - accuracy: 0.5639\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.9971 - accuracy: 0.5869\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.9784 - accuracy: 0.5869\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.9731 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.9710 - accuracy: 0.5869\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.9676 - accuracy: 0.5870\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.9623 - accuracy: 0.5870\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.9581 - accuracy: 0.5888\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.9514 - accuracy: 0.5924\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.9438 - accuracy: 0.5947\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.9347 - accuracy: 0.5951\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.9178 - accuracy: 0.6008\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8971 - accuracy: 0.6106\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8846 - accuracy: 0.6158\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.8725 - accuracy: 0.6211\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8574 - accuracy: 0.6281\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8432 - accuracy: 0.6340\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 2s 69ms/step - loss: 0.8331 - accuracy: 0.6412\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 2s 69ms/step - loss: 0.8369 - accuracy: 0.6410\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.8289 - accuracy: 0.6454\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 2s 69ms/step - loss: 0.8131 - accuracy: 0.6557\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.7991 - accuracy: 0.6653\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.7868 - accuracy: 0.6754\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 2s 69ms/step - loss: 0.7643 - accuracy: 0.6910\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.7523 - accuracy: 0.6961\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.7678 - accuracy: 0.6808\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.7405 - accuracy: 0.7037\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.7315 - accuracy: 0.7049\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.7155 - accuracy: 0.7185\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.7055 - accuracy: 0.7230\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.7101 - accuracy: 0.7177\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 2s 69ms/step - loss: 0.7275 - accuracy: 0.7081\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 2s 70ms/step - loss: 0.6940 - accuracy: 0.7322\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.6655 - accuracy: 0.7517\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.6549 - accuracy: 0.7562\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.6575 - accuracy: 0.7549\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.6851 - accuracy: 0.7372\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 2s 70ms/step - loss: 0.6905 - accuracy: 0.7314\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.6710 - accuracy: 0.7384\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 2s 69ms/step - loss: 0.6489 - accuracy: 0.7568\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.6739 - accuracy: 0.7477\n",
            "[0.6739051342010498, 0.7476666569709778]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4aadfbd-5d59-4f15-81bb-52681d3822e8"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 2s 72ms/step - loss: 1.1712 - accuracy: 0.4874\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.9701 - accuracy: 0.5870\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.9488 - accuracy: 0.5872\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 0.9332 - accuracy: 0.5882\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.9179 - accuracy: 0.5923\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.9065 - accuracy: 0.5975\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.8965 - accuracy: 0.6029\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.8868 - accuracy: 0.6092\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.8771 - accuracy: 0.6151\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.8676 - accuracy: 0.6187\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.8563 - accuracy: 0.6256\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.8462 - accuracy: 0.6325\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.8306 - accuracy: 0.6419\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.8149 - accuracy: 0.6487\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.7978 - accuracy: 0.6601\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.7772 - accuracy: 0.6702\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.7643 - accuracy: 0.6793\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.7511 - accuracy: 0.6842\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.7304 - accuracy: 0.6957\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.7114 - accuracy: 0.7067\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.7149 - accuracy: 0.7029\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.7033 - accuracy: 0.7128\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.6741 - accuracy: 0.7267\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.6521 - accuracy: 0.7362\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.6374 - accuracy: 0.7438\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.6213 - accuracy: 0.7514\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.6003 - accuracy: 0.7638\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.5789 - accuracy: 0.7745\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 0.5826 - accuracy: 0.7719\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.6140 - accuracy: 0.7484\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.5761 - accuracy: 0.7729\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.5739 - accuracy: 0.7722\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.5297 - accuracy: 0.7980\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.4924 - accuracy: 0.8176\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.4660 - accuracy: 0.8309\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.4481 - accuracy: 0.8389\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.4461 - accuracy: 0.8369\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.4490 - accuracy: 0.8346\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 2s 74ms/step - loss: 0.4414 - accuracy: 0.8378\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 2s 75ms/step - loss: 0.4902 - accuracy: 0.8118\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.4932 - accuracy: 0.8194\n",
            "[0.49316754937171936, 0.8194166421890259]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb65fd0-4a05-4db3-c992-28f19e68e7f1"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 1.1294 - accuracy: 0.5277\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.9892 - accuracy: 0.5862\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 2s 89ms/step - loss: 0.9829 - accuracy: 0.5867\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9803 - accuracy: 0.5866\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9776 - accuracy: 0.5867\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9774 - accuracy: 0.5872\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9736 - accuracy: 0.5868\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.9685 - accuracy: 0.5880\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9661 - accuracy: 0.5889\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9618 - accuracy: 0.5911\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9585 - accuracy: 0.5916\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9554 - accuracy: 0.5942\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.9523 - accuracy: 0.5964\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9490 - accuracy: 0.5988\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9456 - accuracy: 0.6003\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 2s 89ms/step - loss: 0.9416 - accuracy: 0.6016\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9381 - accuracy: 0.6037\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 2s 89ms/step - loss: 0.9351 - accuracy: 0.6066\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9308 - accuracy: 0.6076\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9262 - accuracy: 0.6116\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9247 - accuracy: 0.6125\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.9285 - accuracy: 0.6077\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 2s 89ms/step - loss: 0.9267 - accuracy: 0.6091\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.9165 - accuracy: 0.6161\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.9091 - accuracy: 0.6216\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 2s 90ms/step - loss: 0.9061 - accuracy: 0.6205\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 2s 90ms/step - loss: 0.8970 - accuracy: 0.6277\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 2s 89ms/step - loss: 0.8876 - accuracy: 0.6365\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 2s 90ms/step - loss: 0.8821 - accuracy: 0.6381\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.8780 - accuracy: 0.6421\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 2s 85ms/step - loss: 0.8915 - accuracy: 0.6344\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.8883 - accuracy: 0.6341\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.8755 - accuracy: 0.6430\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.8678 - accuracy: 0.6476\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.8636 - accuracy: 0.6497\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 2s 88ms/step - loss: 0.8645 - accuracy: 0.6511\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 2s 86ms/step - loss: 0.8750 - accuracy: 0.6442\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 2s 85ms/step - loss: 0.9049 - accuracy: 0.6230\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 2s 87ms/step - loss: 0.8838 - accuracy: 0.6394\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 2s 85ms/step - loss: 0.8785 - accuracy: 0.6443\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.8799 - accuracy: 0.6447\n",
            "[0.879911482334137, 0.6447499990463257]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdPYKennH7PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6363934-b7e4-4c9c-e902-ae5e62cd7afc"
      },
      "source": [
        "model_sg1 = tf.keras.Sequential()\r\n",
        "model_sg1.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_sg1.add(tf.keras.layers.LSTM(64))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg1.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg1.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg1.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table+=[history[1]]\r\n",
        "print(\"SG_LSTM\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " 2/24 [=>............................] - ETA: 1s - loss: 1.3840 - accuracy: 0.4100WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0463s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 1.2387 - accuracy: 0.5730\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 1.0254 - accuracy: 0.5867\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 1.0017 - accuracy: 0.5857\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9954 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9904 - accuracy: 0.5865\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9857 - accuracy: 0.5867\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9800 - accuracy: 0.5868\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9761 - accuracy: 0.5871\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9745 - accuracy: 0.5870\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9703 - accuracy: 0.5869\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9646 - accuracy: 0.5867\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9612 - accuracy: 0.5867\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9542 - accuracy: 0.5859\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9482 - accuracy: 0.5877\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9450 - accuracy: 0.5865\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9415 - accuracy: 0.5878\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.9340 - accuracy: 0.5878\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9248 - accuracy: 0.5904\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9139 - accuracy: 0.5932\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9036 - accuracy: 0.5978\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.8903 - accuracy: 0.6057\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.8854 - accuracy: 0.6053\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.8718 - accuracy: 0.6143\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.8663 - accuracy: 0.6191\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.8533 - accuracy: 0.6252\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.8442 - accuracy: 0.6299\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.8339 - accuracy: 0.6373\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.8279 - accuracy: 0.6392\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.8181 - accuracy: 0.6433\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.8151 - accuracy: 0.6436\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.8097 - accuracy: 0.6524\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.7909 - accuracy: 0.6609\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.7798 - accuracy: 0.6650\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7697 - accuracy: 0.6743\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.7600 - accuracy: 0.6808\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.7534 - accuracy: 0.6827\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.7658 - accuracy: 0.6816\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.7391 - accuracy: 0.6924\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.7183 - accuracy: 0.7038\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.7026 - accuracy: 0.7107\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.7233 - accuracy: 0.7009\n",
            "[0.7233056426048279, 0.7009166479110718]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261854f8-b5bb-4a41-e135-b8efe0dbdf13"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.7015833258628845, 0.6775000095367432, 0.6730833053588867]\n",
            "cbow [0.6918333172798157, 0.6694999933242798, 0.5979166626930237]\n",
            "glove [0.7476666569709778, 0.8194166421890259, 0.6447499990463257]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLVUTMaIFdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81d07b2-e8d3-4e19-8e97-76dc9ee330b0"
      },
      "source": [
        "print(result_table[9])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7009166479110718\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}