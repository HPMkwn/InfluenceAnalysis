{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllEkphrasisVecMod2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/AllEkphrasisVecMod2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9fee67-650e-4174-d00b-c81be501af6d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/train_clean.csv',names=['Tweet','Emotion'])\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/test_clean.csv',names=['Tweet','Emotion'])\n",
        "df_train = df_train.sample(frac=1)\n",
        "df_test = df_train.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zrtpGceAiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "59d52fb9-1f10-400f-ca0e-17874bf58432"
      },
      "source": [
        "df_train.head(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6909</th>\n",
              "      <td>back to work tomoo day 1 of 5 it will bee</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9298</th>\n",
              "      <td>how do i gain weight i lost pound and i want i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2365</th>\n",
              "      <td>justin_ro oh well ive never seen him do anyth ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5378</th>\n",
              "      <td>bookaliciouspam hey welcom to va lol it muggi ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10673</th>\n",
              "      <td>tri to open a docx file on my virtual system w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6192</th>\n",
              "      <td>on the monday so i wont be abl to be with you ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>crazy_cindi big hugg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4325</th>\n",
              "      <td>strike one three and I am out</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9119</th>\n",
              "      <td>caseywright ~acai emails~yes ton of them</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10777</th>\n",
              "      <td>is sad coz hyd theka do not have beer</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Tweet Emotion\n",
              "6909           back to work tomoo day 1 of 5 it will bee       2\n",
              "9298   how do i gain weight i lost pound and i want i...       2\n",
              "2365   justin_ro oh well ive never seen him do anyth ...       0\n",
              "5378   bookaliciouspam hey welcom to va lol it muggi ...       1\n",
              "10673  tri to open a docx file on my virtual system w...       0\n",
              "...                                                  ...     ...\n",
              "6192   on the monday so i wont be abl to be with you ...       2\n",
              "10001                               crazy_cindi big hugg       1\n",
              "4325                       strike one three and I am out       0\n",
              "9119            caseywright ~acai emails~yes ton of them       0\n",
              "10777              is sad coz hyd theka do not have beer       2\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K68sxL_LfLy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c02cfc7-8672-4ef4-b7e0-9a7cf642dfe9"
      },
      "source": [
        "df_test.Emotion.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1', '2', '0', '3', 'emotion'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9KFGTW2iP8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7045078b-f392-4185-8471-7be1c4838748"
      },
      "source": [
        "df_train.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tweet      12370\n",
              "Emotion        5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {'0':[1,0,0,0],'1':[0,1,0,0],'2':[0,0,1,0],'3':[0,0,0,1],'emotion':[1,0,0,0]}\n",
        "train_data_tweet = [x.lower().split() for x in df_train['Tweet']]\n",
        "train_data_cat = np.array([category_dict[x] for x in df_train['Emotion']])\n",
        "test_data_tweet = [x.lower().split() for x in df_test['Tweet']]\n",
        "test_data_cat = np.array([category_dict[x] for x in df_test['Emotion']])\n",
        "\n",
        "data_tweet = train_data_tweet + test_data_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb06746-569f-4f90-923c-b18d6140f182"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1304445, 1724630)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 500\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor=1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqvzy05D2iim"
      },
      "source": [
        "X_train = train_data_tweet\r\n",
        "X_test = test_data_tweet\r\n",
        "y_train = train_data_cat\r\n",
        "y_test = test_data_cat \r\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\r\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.get_vector(i[0]),i[1]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]*tweet[i][0])*mul_factor/np.sqrt(tweet[i][0].dot(tweet[i][0])),tweet[z][1])      \r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a96699a-0d96-4751-c4fd-eb2dad055c7a"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-bddb9a303a31>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 1.2359 - accuracy: 0.5727\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 1.0176 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9872 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9851 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9851 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9851 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9851 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9851 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9851 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 215ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 215ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 215ms/step - loss: 0.9853 - accuracy: 0.5854\n",
            "[0.9853453636169434, 0.5854166746139526]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHtKjzc1SA8",
        "outputId": "5e2835a1-ec72-47a0-dcad-f1662f3f0077"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 1.2241 - accuracy: 0.5748\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 1.0014 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 0.9846 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9845 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9844 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9837 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9836 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9834 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9832 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9830 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 214ms/step - loss: 0.9827 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 214ms/step - loss: 0.9824 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 215ms/step - loss: 0.9822 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 214ms/step - loss: 0.9819 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 213ms/step - loss: 0.9834 - accuracy: 0.5854\n",
            "[0.9833863377571106, 0.5854166746139526]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opfhhYPH1VoL",
        "outputId": "5589c647-b1a8-4576-bdbf-ebd35dff12e7"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 1.1136 - accuracy: 0.5690\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9894 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9855 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9855 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9854 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 210ms/step - loss: 0.9860 - accuracy: 0.5854\n",
            "[0.9859652519226074, 0.5854166746139526]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YumD1M9Y1Xxr",
        "outputId": "1f7437bd-4553-40cb-87da-696b81e16603"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 1.2545 - accuracy: 0.5716\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 1.0282 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9873 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9850 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9848 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9848 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9846 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9845 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9841 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9833 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9849 - accuracy: 0.5854\n",
            "[0.9849499464035034, 0.5854166746139526]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01yVMiF41ZtT",
        "outputId": "d9a83722-e44c-4a08-eb19-ee9c24067394"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 1.2400 - accuracy: 0.5736\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 1.0160 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9864 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9849 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9848 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9847 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9846 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9845 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9844 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9843 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9842 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9840 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9838 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9836 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9831 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9814 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9801 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9768 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9750 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9718 - accuracy: 0.5854\n",
            "[0.9718207716941833, 0.5854166746139526]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_G3Ff5r1bSD",
        "outputId": "de9afb5b-18e1-431c-fe0e-9e82e5e158b2"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 1.0740 - accuracy: 0.5727\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9858 - accuracy: 0.5863\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9853 - accuracy: 0.5863\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9852 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9856 - accuracy: 0.5854\n",
            "[0.9855551719665527, 0.5854166746139526]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEglKIfExKKI"
      },
      "source": [
        "epochs=40\r\n",
        "Embedding_size=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czoFmEwK1dWT",
        "outputId": "16589905-6e91-4c36-cbd4-20a4a545ddd6"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 1.2368 - accuracy: 0.5672\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.9978 - accuracy: 0.5863\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9791 - accuracy: 0.5863\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9732 - accuracy: 0.5863\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9694 - accuracy: 0.5863\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9647 - accuracy: 0.5916\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9561 - accuracy: 0.5939\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.9413 - accuracy: 0.5982\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9276 - accuracy: 0.6000\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9159 - accuracy: 0.6018\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9064 - accuracy: 0.6051\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.9015 - accuracy: 0.6098\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8978 - accuracy: 0.6110\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8888 - accuracy: 0.6137\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8791 - accuracy: 0.6198\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8726 - accuracy: 0.6228\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.8661 - accuracy: 0.6272\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8591 - accuracy: 0.6319\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.8518 - accuracy: 0.6378\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.8431 - accuracy: 0.6408\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.8340 - accuracy: 0.6487\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8256 - accuracy: 0.6543\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8168 - accuracy: 0.6598\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8090 - accuracy: 0.6633\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.8072 - accuracy: 0.6630\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.8045 - accuracy: 0.6658\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7882 - accuracy: 0.6743\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.7889 - accuracy: 0.6733\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7913 - accuracy: 0.6735\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.7768 - accuracy: 0.6821\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7648 - accuracy: 0.6892\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.7635 - accuracy: 0.6913\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.7857 - accuracy: 0.6786\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7562 - accuracy: 0.6924\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7423 - accuracy: 0.7023\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.7258 - accuracy: 0.7088\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7179 - accuracy: 0.7140\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.7307 - accuracy: 0.7078\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7409 - accuracy: 0.7026\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.7762 - accuracy: 0.6774\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.8213 - accuracy: 0.6628\n",
            "[0.8212751746177673, 0.6628333330154419]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKM9PEwm1fkb",
        "outputId": "5d87cb0e-5d61-4ad0-bc5f-b052835e9f0e"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 1.1427 - accuracy: 0.5304\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.9715 - accuracy: 0.5864\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.9541 - accuracy: 0.5881\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.9420 - accuracy: 0.5893\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.9280 - accuracy: 0.5942\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.9165 - accuracy: 0.5979\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.9070 - accuracy: 0.6034\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.9003 - accuracy: 0.6067\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.8940 - accuracy: 0.6100\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8875 - accuracy: 0.6108\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8805 - accuracy: 0.6154\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8731 - accuracy: 0.6208\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8650 - accuracy: 0.6277\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8561 - accuracy: 0.6348\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.8475 - accuracy: 0.6393\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8381 - accuracy: 0.6454\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8286 - accuracy: 0.6510\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8179 - accuracy: 0.6550\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.8056 - accuracy: 0.6602\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.7909 - accuracy: 0.6706\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.7767 - accuracy: 0.6777\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.7620 - accuracy: 0.6859\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.7517 - accuracy: 0.6898\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.7430 - accuracy: 0.6939\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.7302 - accuracy: 0.7004\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.7114 - accuracy: 0.7132\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.7149 - accuracy: 0.7038\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6968 - accuracy: 0.7146\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6802 - accuracy: 0.7253\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6615 - accuracy: 0.7353\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6483 - accuracy: 0.7391\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6534 - accuracy: 0.7348\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6710 - accuracy: 0.7236\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6578 - accuracy: 0.7293\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6227 - accuracy: 0.7480\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 1s 42ms/step - loss: 0.6066 - accuracy: 0.7561\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6090 - accuracy: 0.7533\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6221 - accuracy: 0.7477\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 1s 43ms/step - loss: 0.6451 - accuracy: 0.7327\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 1s 44ms/step - loss: 0.6126 - accuracy: 0.7537\n",
            "24/24 [==============================] - 1s 41ms/step - loss: 0.6262 - accuracy: 0.7354\n",
            "[0.6261599659919739, 0.7354166507720947]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQBa5s4s1hNU",
        "outputId": "ea1f1b19-0049-46f0-919e-56dd33ecd37c"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 1.0919 - accuracy: 0.5319\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9893 - accuracy: 0.5860\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9882 - accuracy: 0.5860\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9879 - accuracy: 0.5860\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 1s 56ms/step - loss: 0.9879 - accuracy: 0.5860\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9864 - accuracy: 0.5861\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9834 - accuracy: 0.5860\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9817 - accuracy: 0.5864\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9700 - accuracy: 0.5895\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9619 - accuracy: 0.5944\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9577 - accuracy: 0.5970\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9538 - accuracy: 0.5980\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9509 - accuracy: 0.5990\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9471 - accuracy: 0.6011\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9435 - accuracy: 0.6033\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9412 - accuracy: 0.6049\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9392 - accuracy: 0.6072\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9398 - accuracy: 0.6072\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9403 - accuracy: 0.6058\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9330 - accuracy: 0.6084\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9267 - accuracy: 0.6143\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9211 - accuracy: 0.6193\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9145 - accuracy: 0.6225\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.9143 - accuracy: 0.6212\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9153 - accuracy: 0.6238\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9167 - accuracy: 0.6239\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.9094 - accuracy: 0.6284\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9096 - accuracy: 0.6279\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 1s 56ms/step - loss: 0.9089 - accuracy: 0.6282\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8998 - accuracy: 0.6373\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.8908 - accuracy: 0.6402\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8845 - accuracy: 0.6447\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8767 - accuracy: 0.6506\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8707 - accuracy: 0.6543\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8732 - accuracy: 0.6535\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.8776 - accuracy: 0.6533\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8902 - accuracy: 0.6436\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.9074 - accuracy: 0.6349\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.8782 - accuracy: 0.6496\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.8708 - accuracy: 0.6523\n",
            "24/24 [==============================] - 1s 40ms/step - loss: 0.8545 - accuracy: 0.6630\n",
            "[0.8545003533363342, 0.6629999876022339]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdPYKennH7PF",
        "outputId": "a179ef37-9e2c-4828-e88f-eaf1104a741c"
      },
      "source": [
        "model_sg1 = tf.keras.Sequential()\r\n",
        "model_sg1.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_sg1.add(tf.keras.layers.LSTM(64))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg1.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history = model_sg1.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg1.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table+=[history[1]]\r\n",
        "print(\"SG_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 1.2698 - accuracy: 0.5642\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 1.0253 - accuracy: 0.5823\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 1.0050 - accuracy: 0.5832\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 1.0022 - accuracy: 0.5859\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 1.0001 - accuracy: 0.5857\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9964 - accuracy: 0.5862\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9973 - accuracy: 0.5863\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9949 - accuracy: 0.5863\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9930 - accuracy: 0.5863\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9928 - accuracy: 0.5863\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 0.9919 - accuracy: 0.5863\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9911 - accuracy: 0.5863\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9912 - accuracy: 0.5863\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9910 - accuracy: 0.5863\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9889 - accuracy: 0.5863\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9882 - accuracy: 0.5863\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9902 - accuracy: 0.5863\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9891 - accuracy: 0.5863\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 5s 214ms/step - loss: 0.9888 - accuracy: 0.5863\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9887 - accuracy: 0.5863\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 5s 213ms/step - loss: 0.9886 - accuracy: 0.5863\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 5s 213ms/step - loss: 0.9883 - accuracy: 0.5863\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 5s 211ms/step - loss: 0.9893 - accuracy: 0.5863\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 5s 213ms/step - loss: 0.9857 - accuracy: 0.5863\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 5s 216ms/step - loss: 0.9858 - accuracy: 0.5863\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 5s 213ms/step - loss: 0.9839 - accuracy: 0.5863\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 5s 211ms/step - loss: 0.9835 - accuracy: 0.5863\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 5s 211ms/step - loss: 0.9804 - accuracy: 0.5863\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 5s 212ms/step - loss: 0.9807 - accuracy: 0.5863\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 5s 212ms/step - loss: 0.9800 - accuracy: 0.5863\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 5s 211ms/step - loss: 0.9805 - accuracy: 0.5863\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 5s 213ms/step - loss: 0.9785 - accuracy: 0.5863\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 5s 211ms/step - loss: 0.9780 - accuracy: 0.5863\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 5s 210ms/step - loss: 0.9765 - accuracy: 0.5863\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 5s 212ms/step - loss: 0.9776 - accuracy: 0.5863\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 5s 214ms/step - loss: 0.9773 - accuracy: 0.5863\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 5s 214ms/step - loss: 0.9785 - accuracy: 0.5863\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 5s 210ms/step - loss: 0.9767 - accuracy: 0.5863\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 5s 212ms/step - loss: 0.9783 - accuracy: 0.5863\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 5s 210ms/step - loss: 0.9757 - accuracy: 0.5863\n",
            "24/24 [==============================] - 5s 209ms/step - loss: 0.9717 - accuracy: 0.5854\n",
            "[0.9717304706573486, 0.5854166746139526]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZqN_SaM3hs9",
        "outputId": "1cf2ebf6-e4e6-4b07-8de8-c86c39fffdad"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.5854166746139526, 0.5854166746139526, 0.5854166746139526]\n",
            "cbow [0.5854166746139526, 0.5854166746139526, 0.5854166746139526]\n",
            "glove [0.6628333330154419, 0.7354166507720947, 0.6629999876022339]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRLVUTMaIFdi",
        "outputId": "8ca6dd73-0d50-4a2f-d006-ac70b596cbb1"
      },
      "source": [
        "print(result_table[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5854166746139526\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}