{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AllEkphrasisvec4negpos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrymkwn/InfluenceAnalysis/blob/master/Copy_of_AllEkphrasisvec4negpos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuKKcIUmM27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c9271f-7e53-464d-8d5f-78f6b28e7944"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/train_clean.csv',names=['Tweet','Emotion'])\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/InfluenceAnalysis/Newdata/test_clean.csv',names=['Tweet','Emotion'])\n",
        "df_train = df_train.sample(frac=1)\n",
        "df_test = df_train.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zrtpGceAiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "020bf339-c092-409b-9e4c-d9c1c682983c"
      },
      "source": [
        "df_train.head(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2208</th>\n",
              "      <td>nicolepenney that suck I am go to be do summer...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9155</th>\n",
              "      <td>transit time is over tonight i work night shif...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11920</th>\n",
              "      <td>anoth friday night in man be skint is rubbish</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7182</th>\n",
              "      <td>saschaillyvich i guess that mean you did not b...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>amyype lol i know i onli get to read it someti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6236</th>\n",
              "      <td>realli want to go to the net</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3581</th>\n",
              "      <td>tri to draw some mangaanim for our new websit ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8196</th>\n",
              "      <td>just watch the quotfin breakquot final prison ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6299</th>\n",
              "      <td>nobili i feel your pain mine is the same way</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7251</th>\n",
              "      <td>i have not log onto mintocom in like 3 month a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Tweet Emotion\n",
              "2208   nicolepenney that suck I am go to be do summer...       2\n",
              "9155   transit time is over tonight i work night shif...       0\n",
              "11920      anoth friday night in man be skint is rubbish       2\n",
              "7182   saschaillyvich i guess that mean you did not b...       2\n",
              "1817   amyype lol i know i onli get to read it someti...       1\n",
              "...                                                  ...     ...\n",
              "6236                        realli want to go to the net       1\n",
              "3581   tri to draw some mangaanim for our new websit ...       2\n",
              "8196   just watch the quotfin breakquot final prison ...       1\n",
              "6299        nobili i feel your pain mine is the same way       2\n",
              "7251   i have not log onto mintocom in like 3 month a...       2\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K68sxL_LfLy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb3370f-0195-4602-de09-f6c59aadb996"
      },
      "source": [
        "df_test.Emotion.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', '2', '1', '3', 'emotion'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9KFGTW2iP8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569b1835-7ebf-44aa-b8c9-b46e2ca15ba7"
      },
      "source": [
        "df_train.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tweet      12370\n",
              "Emotion        5\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yV6_xYSnZHt"
      },
      "source": [
        "category_dict = {'0':[1,0,0,0],'1':[0,1,0,0],'2':[0,0,1,0],'3':[0,0,0,1],'emotion':[1,0,0,0]}\n",
        "train_data_tweet = [x.lower().split() for x in df_train['Tweet']]\n",
        "train_data_cat = np.array([category_dict[x] for x in df_train['Emotion']])\n",
        "test_data_tweet = [x.lower().split() for x in df_test['Tweet']]\n",
        "test_data_cat = np.array([category_dict[x] for x in df_test['Emotion']])\n",
        "\n",
        "data_tweet = train_data_tweet + test_data_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIV-pI5C44j"
      },
      "source": [
        "# Parameters\n",
        "Min_count = 0\n",
        "Embedding_size = 100\n",
        "Window_size = 5\n",
        "Negative_sampling = 00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUSBfiE0nTVv"
      },
      "source": [
        "w2v_sg = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=1)\n",
        "w2v_sg.build_vocab(data_tweet)\n",
        "w2v_sg.train(data_tweet, total_examples=w2v_sg.corpus_count, epochs=5)\n",
        "w2v_cbow = Word2Vec(min_count=Min_count,\n",
        "                     window=Window_size,\n",
        "                     size=Embedding_size,\n",
        "                     negative=Negative_sampling,sg=0)\n",
        "w2v_cbow.build_vocab(data_tweet)\n",
        "w2v_cbow.train(data_tweet, total_examples=w2v_cbow.corpus_count, epochs=5)\n",
        "w2v_sg.wv.init_sims(True)\n",
        "w2v_cbow.wv.init_sims(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzquoxb6FQJ"
      },
      "source": [
        "vocab_sg = w2v_sg.wv.vocab\n",
        "vocab_sg = [x for x in vocab_sg]\n",
        "vocab_cbow = w2v_cbow.wv.vocab\n",
        "vocab_cbow = [x for x in vocab_cbow] \n",
        "vocab_glove = {}\n",
        "with open(\"/content/drive/My Drive/InfluenceAnalysis/glove/glove.twitter.27B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      vocab_glove[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ksBvx9Z_ws"
      },
      "source": [
        "def save_dict_to_file(dic):\r\n",
        "    f = open(\"/content/drive/My Drive/InfluenceAnalysis/vocab1.txt\",'w')\r\n",
        "    for i in dic:\r\n",
        "      f.write(str(i)+\"\\t\")\r\n",
        "      f.write(str(w2v_sg.wv.get_vector(i)))\r\n",
        "      f.write(\"\\n\")\r\n",
        "    f.close()\r\n",
        "save_dict_to_file(vocab_sg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdgSI9FSmw60"
      },
      "source": [
        "batch_size = 500\r\n",
        "Max_input_size = max([len(x) for x in data_tweet])\r\n",
        "mul_factor=1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z65iXVhlpeDX"
      },
      "source": [
        "positive = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/positive-words.csv')\r\n",
        "positive = list(positive['0'][1:])\r\n",
        "negative = pd.read_csv('/content/drive/My Drive/InfluenceAnalysis/CodeMix/negative-words.csv')\r\n",
        "negative = list(negative['0'][2:])\r\n",
        "pn_dict = {}\r\n",
        "for i in range(len(positive)):\r\n",
        "  pn_dict[positive[i]] = 1\r\n",
        "\r\n",
        "for i in range(len(negative)):\r\n",
        "  pn_dict[negative[i]] = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqvzy05D2iim"
      },
      "source": [
        "X_train = train_data_tweet\r\n",
        "X_test = test_data_tweet\r\n",
        "y_train = train_data_cat\r\n",
        "y_test = test_data_cat \r\n",
        "X_train = [nltk.pos_tag(x) for x in X_train]\r\n",
        "X_test = [nltk.pos_tag(x) for x in X_test]\r\n",
        "X_train = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_train]\r\n",
        "X_test = [[(i[0],i[1],pn_dict.get(i[0],1)) for i in x] for x in X_test]\r\n",
        "result_table = [0,0,0,0,0,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIGpYOBRx-kf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwAlW-wmZxv"
      },
      "source": [
        "def vec_gen(w2v,vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(w2v.wv.word_vec(i[0]),i[1],i[2]) for i in x if i[0] in vocab]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          tweet[i]=((tweet[i][0]*tweet[i][2]),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)          \r\n",
        "      \r\n",
        "def glove_gen(vocab,data_tweet,data_cat,batch_size,Max_input_size,Embedding_size,mul_factor):\r\n",
        "  while True:\r\n",
        "    for k in range(int(len(data_tweet)/batch_size)):\r\n",
        "      res=[]\r\n",
        "      for x in data_tweet[k*(batch_size):(k+1)*(batch_size)]:\r\n",
        "        tweet = [(vocab[i[0]],i[1],i[2]) for i in x if i[0] in vocab.keys()]\r\n",
        "        l=len(tweet)\r\n",
        "        for i in range(l):\r\n",
        "          tweet[i]=((tweet[i][0]*tweet[i][2]),tweet[i][1],tweet[i][2])\r\n",
        "        for i in range(l):\r\n",
        "          if tweet[i][1][:2]=='JJ':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='NN':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "          if tweet[i][1][:2]=='RB':\r\n",
        "            for z in range(max(i-2,0),min(i+3,l)):\r\n",
        "              if tweet[z][1][:2]=='VB':\r\n",
        "                tweet[z]=((tweet[z][0]+tweet[i][0])*mul_factor,tweet[z][1])\r\n",
        "        res+=[np.array([x[0] for x in tweet])]\r\n",
        "      temp = np.array([np.pad(z.flatten(),(0,Max_input_size*Embedding_size-len(z.flatten()))).reshape(Max_input_size,Embedding_size) for z in res])\r\n",
        "      tempres = data_cat[k*(batch_size):(k+1)*(batch_size)]\r\n",
        "      yield (temp,tempres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0leTuTOb3naL"
      },
      "source": [
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GfgXfAFlD1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cab5b7-fda0-4379-fcc5-fceade798204"
      },
      "source": [
        "model_sg = tf.keras.Sequential()\n",
        "model_sg.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\n",
        "model_sg.add(tf.keras.layers.LSTM(50))\n",
        "model_sg.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model_sg.compile(loss='categorical_crossentropy',optimizer='adam', \n",
        "                           metrics=['accuracy'])\n",
        "history_sg_lstm = model_sg.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\n",
        "history = model_sg.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\n",
        "print(history)\n",
        "\n",
        "result_table[0]=history[1]\n",
        "print(\"SG_LSTM\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 2/24 [=>............................] - ETA: 2s - loss: 1.3834 - accuracy: 0.4110WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0273s vs `on_train_batch_end` time: 0.1251s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 1.2096 - accuracy: 0.5680\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9864 - accuracy: 0.5857\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9834 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9824 - accuracy: 0.5857\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9798 - accuracy: 0.5857\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9759 - accuracy: 0.5858\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9716 - accuracy: 0.5857\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9690 - accuracy: 0.5857\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9653 - accuracy: 0.5857\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9637 - accuracy: 0.5857\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9606 - accuracy: 0.5857\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9578 - accuracy: 0.5857\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9541 - accuracy: 0.5855\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9508 - accuracy: 0.5853\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9475 - accuracy: 0.5861\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9435 - accuracy: 0.5868\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9384 - accuracy: 0.5884\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9323 - accuracy: 0.5917\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9264 - accuracy: 0.5939\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9196 - accuracy: 0.5966\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 5s 217ms/step - loss: 0.9105 - accuracy: 0.6012\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9025 - accuracy: 0.6061\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.8945 - accuracy: 0.6130\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.8856 - accuracy: 0.6187\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.8781 - accuracy: 0.6218\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.8713 - accuracy: 0.6255\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 0.8641 - accuracy: 0.6296\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.8569 - accuracy: 0.6338\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.8499 - accuracy: 0.6392\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.8438 - accuracy: 0.6447\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8393 - accuracy: 0.6467\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.8337 - accuracy: 0.6507\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8269 - accuracy: 0.6541\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.8265 - accuracy: 0.6528\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.8113 - accuracy: 0.6632\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.7967 - accuracy: 0.6700\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.7901 - accuracy: 0.6780\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7826 - accuracy: 0.6838\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.7775 - accuracy: 0.6860\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.7770 - accuracy: 0.6864\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7879 - accuracy: 0.6763\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.7723 - accuracy: 0.6876\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7595 - accuracy: 0.6969\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.7474 - accuracy: 0.7042\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7381 - accuracy: 0.7097\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.7299 - accuracy: 0.7151\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.7244 - accuracy: 0.7171\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.7198 - accuracy: 0.7208\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.7182 - accuracy: 0.7217\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7334 - accuracy: 0.7120\n",
            " 2/24 [=>............................] - ETA: 4s - loss: 0.7502 - accuracy: 0.7090WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0818s vs `on_test_batch_end` time: 0.2080s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7383 - accuracy: 0.7118\n",
            "[0.738257646560669, 0.7118333578109741]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJHtKjzc1SA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ad6ec0-44a7-498e-ad38-a21d697b62e1"
      },
      "source": [
        "model_sg_bi = tf.keras.Sequential()\r\n",
        "model_sg_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_sg_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_bi = model_sg_bi.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_bi.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[1]=history[1]\r\n",
        "print(\"SG_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 2/24 [=>............................] - ETA: 0s - loss: 1.3866 - accuracy: 0.2650WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0294s vs `on_train_batch_end` time: 0.1107s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 218ms/step - loss: 1.2231 - accuracy: 0.5327\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9891 - accuracy: 0.5857\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9759 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9691 - accuracy: 0.5857\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9632 - accuracy: 0.5857\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9580 - accuracy: 0.5856\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.9538 - accuracy: 0.5862\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9507 - accuracy: 0.5861\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.9483 - accuracy: 0.5857\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9464 - accuracy: 0.5847\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9435 - accuracy: 0.5857\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9384 - accuracy: 0.5867\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9348 - accuracy: 0.5883\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9316 - accuracy: 0.5889\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9281 - accuracy: 0.5903\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9245 - accuracy: 0.5901\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9208 - accuracy: 0.5907\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9167 - accuracy: 0.5923\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9124 - accuracy: 0.5927\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9076 - accuracy: 0.5951\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.9025 - accuracy: 0.5986\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.8971 - accuracy: 0.6006\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.8912 - accuracy: 0.6045\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8843 - accuracy: 0.6070\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8780 - accuracy: 0.6075\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8715 - accuracy: 0.6122\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8657 - accuracy: 0.6184\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.8551 - accuracy: 0.6236\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8444 - accuracy: 0.6298\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8383 - accuracy: 0.6334\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.8330 - accuracy: 0.6388\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8323 - accuracy: 0.6380\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.8357 - accuracy: 0.6340\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.8136 - accuracy: 0.6496\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.7968 - accuracy: 0.6593\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.7871 - accuracy: 0.6634\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.7779 - accuracy: 0.6662\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.7733 - accuracy: 0.6717\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.7842 - accuracy: 0.6642\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7787 - accuracy: 0.6674\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.7493 - accuracy: 0.6828\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.7359 - accuracy: 0.6899\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.7214 - accuracy: 0.6973\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.7098 - accuracy: 0.7059\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.6994 - accuracy: 0.7122\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.6963 - accuracy: 0.7147\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.7000 - accuracy: 0.7111\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.7058 - accuracy: 0.7067\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 5s 220ms/step - loss: 0.6867 - accuracy: 0.7158\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.6712 - accuracy: 0.7269\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.7128 - accuracy: 0.7118\n",
            "[0.7128136157989502, 0.7118333578109741]\n",
            "SG_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opfhhYPH1VoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629f7bbb-35f9-4f40-d869-ffd4d72479d1"
      },
      "source": [
        "model_sg_rnn = tf.keras.Sequential()\r\n",
        "model_sg_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_sg_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_sg_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_rnn = model_sg_rnn.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg_rnn.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[2]=history[1]\r\n",
        "print(\"SG_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 1.1015 - accuracy: 0.5351\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9908 - accuracy: 0.5856\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9871 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9862 - accuracy: 0.5857\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9858 - accuracy: 0.5857\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9857 - accuracy: 0.5857\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9857 - accuracy: 0.5857\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9856 - accuracy: 0.5857\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.9857 - accuracy: 0.5857\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9857 - accuracy: 0.5857\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9857 - accuracy: 0.5857\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9858 - accuracy: 0.5857\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9858 - accuracy: 0.5857\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9859 - accuracy: 0.5857\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9860 - accuracy: 0.5857\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9861 - accuracy: 0.5857\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9862 - accuracy: 0.5857\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9863 - accuracy: 0.5857\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9864 - accuracy: 0.5857\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9863 - accuracy: 0.5857\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9861 - accuracy: 0.5857\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9856 - accuracy: 0.5857\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9849 - accuracy: 0.5857\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9835 - accuracy: 0.5857\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9814 - accuracy: 0.5862\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.9782 - accuracy: 0.5863\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9741 - accuracy: 0.5869\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9683 - accuracy: 0.5890\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9614 - accuracy: 0.5909\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9537 - accuracy: 0.5944\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9462 - accuracy: 0.5964\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9391 - accuracy: 0.5982\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9355 - accuracy: 0.6008\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9285 - accuracy: 0.5982\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9174 - accuracy: 0.6034\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9023 - accuracy: 0.6145\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 6s 242ms/step - loss: 0.8942 - accuracy: 0.6214\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.8947 - accuracy: 0.6165\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9019 - accuracy: 0.6078\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.8858 - accuracy: 0.6171\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.8718 - accuracy: 0.6267\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.8666 - accuracy: 0.6288\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.8856 - accuracy: 0.6191\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.8678 - accuracy: 0.6270\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.8474 - accuracy: 0.6356\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.8365 - accuracy: 0.6410\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.8297 - accuracy: 0.6474\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 6s 242ms/step - loss: 0.8289 - accuracy: 0.6511\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.8393 - accuracy: 0.6423\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.8368 - accuracy: 0.6468\n",
            " 2/24 [=>............................] - ETA: 3s - loss: 0.8248 - accuracy: 0.6570WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0304s vs `on_test_batch_end` time: 0.0650s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.8385 - accuracy: 0.6472\n",
            "[0.8384850025177002, 0.6471666693687439]\n",
            "SG_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YumD1M9Y1Xxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ce4ac9-3341-41a5-d39b-2a6131cd2b00"
      },
      "source": [
        "model_cbow = tf.keras.Sequential()\r\n",
        "model_cbow.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow.add(tf.keras.layers.LSTM(50))\r\n",
        "model_cbow.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_lstm = model_cbow.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[3]=history[1]\r\n",
        "print(\"CBOW_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 1.2431 - accuracy: 0.5688\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 1.0075 - accuracy: 0.5857\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9842 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9803 - accuracy: 0.5857\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9760 - accuracy: 0.5857\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9727 - accuracy: 0.5857\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9675 - accuracy: 0.5857\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9647 - accuracy: 0.5861\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9621 - accuracy: 0.5857\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9593 - accuracy: 0.5853\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.9561 - accuracy: 0.5852\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 5s 219ms/step - loss: 0.9526 - accuracy: 0.5853\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9484 - accuracy: 0.5865\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9437 - accuracy: 0.5855\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.9380 - accuracy: 0.5863\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9333 - accuracy: 0.5887\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 5s 223ms/step - loss: 0.9280 - accuracy: 0.5882\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9204 - accuracy: 0.5902\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9135 - accuracy: 0.5942\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.9063 - accuracy: 0.5971\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.8974 - accuracy: 0.6009\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8883 - accuracy: 0.6071\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8807 - accuracy: 0.6162\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8716 - accuracy: 0.6221\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8639 - accuracy: 0.6283\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8548 - accuracy: 0.6314\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.8474 - accuracy: 0.6362\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8396 - accuracy: 0.6417\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8319 - accuracy: 0.6449\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 5s 221ms/step - loss: 0.8243 - accuracy: 0.6507\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 5s 222ms/step - loss: 0.8167 - accuracy: 0.6551\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.8089 - accuracy: 0.6593\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.8010 - accuracy: 0.6662\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.7937 - accuracy: 0.6697\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.7878 - accuracy: 0.6733\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.7845 - accuracy: 0.6772\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.7852 - accuracy: 0.6743\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.8025 - accuracy: 0.6672\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.7880 - accuracy: 0.6744\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.7730 - accuracy: 0.6846\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.7650 - accuracy: 0.6897\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.7601 - accuracy: 0.6910\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.7567 - accuracy: 0.6951\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.7545 - accuracy: 0.6963\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.7541 - accuracy: 0.6973\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.7548 - accuracy: 0.6917\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.7467 - accuracy: 0.6979\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.7502 - accuracy: 0.6963\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.7638 - accuracy: 0.6873\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.7341 - accuracy: 0.7048\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.7417 - accuracy: 0.7007\n",
            "[0.7416696548461914, 0.7007499933242798]\n",
            "CBOW_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yVMiF41ZtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c6a68e-8b52-41d1-ba60-42b3d6e02f1a"
      },
      "source": [
        "model_cbow_bi = tf.keras.Sequential()\r\n",
        "model_cbow_bi.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_cbow_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_bi = model_cbow_bi.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_bi.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[4]=history[1]\r\n",
        "print(\"CBOW_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 2/24 [=>............................] - ETA: 1s - loss: 1.3818 - accuracy: 0.3210WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0372s vs `on_train_batch_end` time: 0.1183s). Check your callbacks.\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 1.2255 - accuracy: 0.5424\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9982 - accuracy: 0.5857\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9761 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9706 - accuracy: 0.5857\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9626 - accuracy: 0.5860\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9560 - accuracy: 0.5852\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9516 - accuracy: 0.5853\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9477 - accuracy: 0.5857\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9433 - accuracy: 0.5863\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9394 - accuracy: 0.5870\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9361 - accuracy: 0.5877\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9325 - accuracy: 0.5878\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9284 - accuracy: 0.5896\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.9234 - accuracy: 0.5913\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.9170 - accuracy: 0.5949\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.9096 - accuracy: 0.5974\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 6s 229ms/step - loss: 0.9017 - accuracy: 0.6016\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.8931 - accuracy: 0.6084\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.8823 - accuracy: 0.6133\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.8712 - accuracy: 0.6209\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.8620 - accuracy: 0.6248\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.8546 - accuracy: 0.6275\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.8474 - accuracy: 0.6336\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.8419 - accuracy: 0.6357\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.8273 - accuracy: 0.6453\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.8138 - accuracy: 0.6526\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.8023 - accuracy: 0.6601\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.7919 - accuracy: 0.6663\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.7823 - accuracy: 0.6708\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.7715 - accuracy: 0.6775\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.7609 - accuracy: 0.6827\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.7507 - accuracy: 0.6882\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.7415 - accuracy: 0.6930\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.7343 - accuracy: 0.6998\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.7341 - accuracy: 0.6962\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.7429 - accuracy: 0.6898\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.7287 - accuracy: 0.6976\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.7226 - accuracy: 0.7029\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.7124 - accuracy: 0.7065\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.7043 - accuracy: 0.7136\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.6943 - accuracy: 0.7210\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 5s 226ms/step - loss: 0.6861 - accuracy: 0.7229\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 5s 224ms/step - loss: 0.6768 - accuracy: 0.7268\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.6696 - accuracy: 0.7306\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.6656 - accuracy: 0.7303\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.6631 - accuracy: 0.7270\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 5s 227ms/step - loss: 0.6610 - accuracy: 0.7311\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.6602 - accuracy: 0.7323\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 5s 228ms/step - loss: 0.6753 - accuracy: 0.7216\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.6815 - accuracy: 0.7166\n",
            "24/24 [==============================] - 6s 230ms/step - loss: 0.6897 - accuracy: 0.7209\n",
            "[0.6896538734436035, 0.7209166884422302]\n",
            "CBOW_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_G3Ff5r1bSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc240cbc-ee34-4461-eee9-ce1b5a12dac2"
      },
      "source": [
        "model_cbow_rnn = tf.keras.Sequential()\r\n",
        "model_cbow_rnn.add(tf.keras.Input(shape=(Max_input_size,Embedding_size)))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_cbow_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_cbow_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_cbow_rnn = model_cbow_rnn.fit_generator(generator = vec_gen(w2v_cbow,vocab_cbow,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_cbow_rnn.evaluate(x = vec_gen(w2v_cbow,vocab_cbow,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[5]=history[1]\r\n",
        "print(\"CBOW_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 1.1320 - accuracy: 0.5229\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9923 - accuracy: 0.5852\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.9869 - accuracy: 0.5856\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9861 - accuracy: 0.5856\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 6s 242ms/step - loss: 0.9858 - accuracy: 0.5856\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9854 - accuracy: 0.5856\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9856 - accuracy: 0.5856\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9851 - accuracy: 0.5857\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9847 - accuracy: 0.5858\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9849 - accuracy: 0.5857\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9848 - accuracy: 0.5858\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9848 - accuracy: 0.5858\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9847 - accuracy: 0.5858\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9846 - accuracy: 0.5858\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9848 - accuracy: 0.5858\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9849 - accuracy: 0.5858\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9847 - accuracy: 0.5858\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9847 - accuracy: 0.5857\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9841 - accuracy: 0.5857\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9820 - accuracy: 0.5857\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 6s 233ms/step - loss: 0.9830 - accuracy: 0.5857\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9794 - accuracy: 0.5858\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9766 - accuracy: 0.5857\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.9731 - accuracy: 0.5858\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9709 - accuracy: 0.5857\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9687 - accuracy: 0.5855\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 6s 254ms/step - loss: 0.9669 - accuracy: 0.5856\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.9650 - accuracy: 0.5857\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.9631 - accuracy: 0.5855\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9611 - accuracy: 0.5845\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9593 - accuracy: 0.5845\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9579 - accuracy: 0.5846\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9563 - accuracy: 0.5854\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9546 - accuracy: 0.5858\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 6s 234ms/step - loss: 0.9528 - accuracy: 0.5863\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9515 - accuracy: 0.5856\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 6s 231ms/step - loss: 0.9507 - accuracy: 0.5876\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 6s 236ms/step - loss: 0.9492 - accuracy: 0.5867\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9468 - accuracy: 0.5879\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 6s 239ms/step - loss: 0.9440 - accuracy: 0.5886\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 6s 238ms/step - loss: 0.9425 - accuracy: 0.5890\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9426 - accuracy: 0.5908\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 6s 242ms/step - loss: 0.9395 - accuracy: 0.5907\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 5s 229ms/step - loss: 0.9395 - accuracy: 0.5907\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9413 - accuracy: 0.5897\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 6s 232ms/step - loss: 0.9389 - accuracy: 0.5923\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9318 - accuracy: 0.5954\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 6s 237ms/step - loss: 0.9261 - accuracy: 0.5962\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9203 - accuracy: 0.5973\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 6s 235ms/step - loss: 0.9175 - accuracy: 0.5962\n",
            " 2/24 [=>............................] - ETA: 3s - loss: 0.9038 - accuracy: 0.6030WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0959s vs `on_test_batch_end` time: 0.1575s). Check your callbacks.\n",
            "24/24 [==============================] - 5s 225ms/step - loss: 0.9153 - accuracy: 0.5952\n",
            "[0.9153195023536682, 0.5951666831970215]\n",
            "CBOW_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEglKIfExKKI"
      },
      "source": [
        "epochs=50\r\n",
        "Embedding_size=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czoFmEwK1dWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7297cb1-971c-4ea7-9af0-5750899b3582"
      },
      "source": [
        "model_glove = tf.keras.Sequential()\r\n",
        "model_glove.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove.add(tf.keras.layers.LSTM(50))\r\n",
        "model_glove.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_lstm = model_glove.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[6]=history[1]\r\n",
        "print(\"GLOVE_LSTM\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 1.1943 - accuracy: 0.5655\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.9865 - accuracy: 0.5858\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.9793 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.9737 - accuracy: 0.5858\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.9708 - accuracy: 0.5858\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.9671 - accuracy: 0.5858\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.9597 - accuracy: 0.5859\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.9504 - accuracy: 0.5860\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.9435 - accuracy: 0.5876\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.9363 - accuracy: 0.5914\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.9277 - accuracy: 0.5972\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.9173 - accuracy: 0.6025\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.9044 - accuracy: 0.6105\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.8897 - accuracy: 0.6194\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.8744 - accuracy: 0.6283\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.8587 - accuracy: 0.6350\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.8418 - accuracy: 0.6447\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.8280 - accuracy: 0.6527\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.8179 - accuracy: 0.6606\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.8149 - accuracy: 0.6633\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.8198 - accuracy: 0.6629\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.8161 - accuracy: 0.6585\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.7967 - accuracy: 0.6687\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.7750 - accuracy: 0.6840\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.7559 - accuracy: 0.6970\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.7404 - accuracy: 0.7092\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.7265 - accuracy: 0.7180\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.7182 - accuracy: 0.7231\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.7217 - accuracy: 0.7186\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.7227 - accuracy: 0.7157\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.7433 - accuracy: 0.7017\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.7231 - accuracy: 0.7171\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.7351 - accuracy: 0.7114\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.7421 - accuracy: 0.6969\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6894 - accuracy: 0.7347\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6790 - accuracy: 0.7417\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.6693 - accuracy: 0.7462\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.6934 - accuracy: 0.7283\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6705 - accuracy: 0.7417\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.6565 - accuracy: 0.7544\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6532 - accuracy: 0.7558\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.6700 - accuracy: 0.7445\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6859 - accuracy: 0.7347\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6461 - accuracy: 0.7589\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.6345 - accuracy: 0.7639\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.6147 - accuracy: 0.7779\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.6019 - accuracy: 0.7832\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.5952 - accuracy: 0.7891\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.5978 - accuracy: 0.7858\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.5921 - accuracy: 0.7843\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.6190 - accuracy: 0.7717\n",
            "[0.6189945340156555, 0.7717499732971191]\n",
            "GLOVE_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKM9PEwm1fkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0e9593-0875-43be-c5ed-f69e7ea1790e"
      },
      "source": [
        "model_glove_bi = tf.keras.Sequential()\r\n",
        "model_glove_bi.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_bi.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50),merge_mode='concat'))\r\n",
        "model_glove_bi.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_bi.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_bi = model_glove_bi.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_bi.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[7]=history[1]\r\n",
        "print(\"GLOVE_BI\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 1.0689 - accuracy: 0.5795\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.9598 - accuracy: 0.5859\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.9397 - accuracy: 0.5873\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.9243 - accuracy: 0.5897\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.9102 - accuracy: 0.5952\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.8975 - accuracy: 0.6053\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.8857 - accuracy: 0.6080\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.8743 - accuracy: 0.6155\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.8629 - accuracy: 0.6207\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.8521 - accuracy: 0.6287\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.8394 - accuracy: 0.6332\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.8243 - accuracy: 0.6426\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.8069 - accuracy: 0.6519\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.7907 - accuracy: 0.6614\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.7733 - accuracy: 0.6737\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.7560 - accuracy: 0.6823\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.7527 - accuracy: 0.6852\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.7415 - accuracy: 0.6924\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.7261 - accuracy: 0.6967\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.6959 - accuracy: 0.7143\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.6757 - accuracy: 0.7262\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.6567 - accuracy: 0.7377\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.6426 - accuracy: 0.7458\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 1s 52ms/step - loss: 0.6446 - accuracy: 0.7410\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 1s 53ms/step - loss: 0.6564 - accuracy: 0.7364\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.6229 - accuracy: 0.7511\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 1s 52ms/step - loss: 0.5940 - accuracy: 0.7677\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.5677 - accuracy: 0.7815\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 1s 54ms/step - loss: 0.5468 - accuracy: 0.7920\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.5273 - accuracy: 0.8008\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.5214 - accuracy: 0.7996\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.5252 - accuracy: 0.7977\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.5888 - accuracy: 0.7623\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.6052 - accuracy: 0.7532\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.5238 - accuracy: 0.7957\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.4926 - accuracy: 0.8070\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 1s 51ms/step - loss: 0.4906 - accuracy: 0.8108\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.4750 - accuracy: 0.8162\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.4637 - accuracy: 0.8257\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.4232 - accuracy: 0.8469\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.3888 - accuracy: 0.8638\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 1s 50ms/step - loss: 0.3675 - accuracy: 0.8726\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.3582 - accuracy: 0.8751\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 1s 48ms/step - loss: 0.3478 - accuracy: 0.8783\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 1s 52ms/step - loss: 0.3779 - accuracy: 0.8586\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 1s 52ms/step - loss: 0.3991 - accuracy: 0.8537\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 1s 47ms/step - loss: 0.4146 - accuracy: 0.8418\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 1s 55ms/step - loss: 0.3797 - accuracy: 0.8588\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.3857 - accuracy: 0.8536\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 1s 49ms/step - loss: 0.3770 - accuracy: 0.8585\n",
            "24/24 [==============================] - 1s 45ms/step - loss: 0.3995 - accuracy: 0.8593\n",
            "[0.3994828760623932, 0.859333336353302]\n",
            "GLOVE_BI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQBa5s4s1hNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c703e7b-1516-432f-c19f-a3365990dd63"
      },
      "source": [
        "model_glove_rnn = tf.keras.Sequential()\r\n",
        "model_glove_rnn.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_glove_rnn.add(tf.keras.layers.SimpleRNN(50))\r\n",
        "model_glove_rnn.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_glove_rnn.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_glove_rnn = model_glove_rnn.fit_generator(generator = glove_gen(vocab_glove,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_glove_rnn.evaluate(x = glove_gen(vocab_glove,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table[8]=history[1]\r\n",
        "print(\"GLOVE_RNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "24/24 [==============================] - 2s 64ms/step - loss: 1.1201 - accuracy: 0.5362\n",
            "Epoch 2/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.9909 - accuracy: 0.5853\n",
            "Epoch 3/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.9862 - accuracy: 0.5857\n",
            "Epoch 4/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.9851 - accuracy: 0.5857\n",
            "Epoch 5/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.9844 - accuracy: 0.5857\n",
            "Epoch 6/50\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.9811 - accuracy: 0.5859\n",
            "Epoch 7/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.9765 - accuracy: 0.5856\n",
            "Epoch 8/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.9735 - accuracy: 0.5867\n",
            "Epoch 9/50\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.9693 - accuracy: 0.5890\n",
            "Epoch 10/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.9656 - accuracy: 0.5914\n",
            "Epoch 11/50\n",
            "24/24 [==============================] - 1s 59ms/step - loss: 0.9630 - accuracy: 0.5924\n",
            "Epoch 12/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.9581 - accuracy: 0.5927\n",
            "Epoch 13/50\n",
            "24/24 [==============================] - 2s 73ms/step - loss: 0.9546 - accuracy: 0.5959\n",
            "Epoch 14/50\n",
            "24/24 [==============================] - 2s 64ms/step - loss: 0.9497 - accuracy: 0.5992\n",
            "Epoch 15/50\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.9456 - accuracy: 0.6025\n",
            "Epoch 16/50\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.9387 - accuracy: 0.6051\n",
            "Epoch 17/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.9359 - accuracy: 0.6049\n",
            "Epoch 18/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.9283 - accuracy: 0.6096\n",
            "Epoch 19/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.9204 - accuracy: 0.6134\n",
            "Epoch 20/50\n",
            "24/24 [==============================] - 2s 70ms/step - loss: 0.9110 - accuracy: 0.6187\n",
            "Epoch 21/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.9066 - accuracy: 0.6216\n",
            "Epoch 22/50\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.9067 - accuracy: 0.6200\n",
            "Epoch 23/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.9057 - accuracy: 0.6189\n",
            "Epoch 24/50\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.8900 - accuracy: 0.6310\n",
            "Epoch 25/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.8864 - accuracy: 0.6359\n",
            "Epoch 26/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.8876 - accuracy: 0.6299\n",
            "Epoch 27/50\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8911 - accuracy: 0.6312\n",
            "Epoch 28/50\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.9046 - accuracy: 0.6215\n",
            "Epoch 29/50\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.8770 - accuracy: 0.6409\n",
            "Epoch 30/50\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.8784 - accuracy: 0.6321\n",
            "Epoch 31/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.8762 - accuracy: 0.6369\n",
            "Epoch 32/50\n",
            "24/24 [==============================] - 2s 68ms/step - loss: 0.8567 - accuracy: 0.6507\n",
            "Epoch 33/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.8410 - accuracy: 0.6604\n",
            "Epoch 34/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.8298 - accuracy: 0.6668\n",
            "Epoch 35/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.8265 - accuracy: 0.6673\n",
            "Epoch 36/50\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.8208 - accuracy: 0.6681\n",
            "Epoch 37/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.8132 - accuracy: 0.6739\n",
            "Epoch 38/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.8089 - accuracy: 0.6774\n",
            "Epoch 39/50\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8039 - accuracy: 0.6788\n",
            "Epoch 40/50\n",
            "24/24 [==============================] - 2s 64ms/step - loss: 0.7941 - accuracy: 0.6842\n",
            "Epoch 41/50\n",
            "24/24 [==============================] - 1s 61ms/step - loss: 0.7910 - accuracy: 0.6823\n",
            "Epoch 42/50\n",
            "24/24 [==============================] - 1s 62ms/step - loss: 0.7868 - accuracy: 0.6858\n",
            "Epoch 43/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.7849 - accuracy: 0.6841\n",
            "Epoch 44/50\n",
            "24/24 [==============================] - 2s 65ms/step - loss: 0.7790 - accuracy: 0.6883\n",
            "Epoch 45/50\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.8170 - accuracy: 0.6635\n",
            "Epoch 46/50\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.8151 - accuracy: 0.6645\n",
            "Epoch 47/50\n",
            "24/24 [==============================] - 2s 67ms/step - loss: 0.8055 - accuracy: 0.6736\n",
            "Epoch 48/50\n",
            "24/24 [==============================] - 1s 60ms/step - loss: 0.7942 - accuracy: 0.6781\n",
            "Epoch 49/50\n",
            "24/24 [==============================] - 2s 63ms/step - loss: 0.7917 - accuracy: 0.6814\n",
            "Epoch 50/50\n",
            "24/24 [==============================] - 2s 66ms/step - loss: 0.7894 - accuracy: 0.6784\n",
            "24/24 [==============================] - 1s 46ms/step - loss: 0.8005 - accuracy: 0.6795\n",
            "[0.8004835844039917, 0.6794999837875366]\n",
            "GLOVE_RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdPYKennH7PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6363934-b7e4-4c9c-e902-ae5e62cd7afc"
      },
      "source": [
        "model_sg1 = tf.keras.Sequential()\r\n",
        "model_sg1.add(tf.keras.Input(shape=(Max_input_size,100)))\r\n",
        "model_sg1.add(tf.keras.layers.LSTM(64))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(64,activation='relu'))\r\n",
        "model_sg1.add(tf.keras.layers.Dropout(0.2))\r\n",
        "model_sg1.add(tf.keras.layers.Dense(4, activation='softmax'))\r\n",
        "model_sg1.compile(loss='categorical_crossentropy',optimizer='adam', \r\n",
        "                           metrics=['accuracy'])\r\n",
        "history_sg_lstm = model_sg1.fit_generator(generator = vec_gen(w2v_sg,vocab_sg,X_train,y_train,batch_size,Max_input_size,Embedding_size,mul_factor),steps_per_epoch=int(len(X_train)/batch_size),epochs=epochs)\r\n",
        "history = model_sg1.evaluate(x = vec_gen(w2v_sg,vocab_sg,X_test,y_test,batch_size,Max_input_size,Embedding_size,mul_factor),steps=int(len(y_test)/batch_size))\r\n",
        "print(history)\r\n",
        "\r\n",
        "result_table+=[history[1]]\r\n",
        "print(\"SG_LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            " 2/24 [=>............................] - ETA: 1s - loss: 1.3840 - accuracy: 0.4100WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0463s vs `on_train_batch_end` time: 0.1263s). Check your callbacks.\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 1.2387 - accuracy: 0.5730\n",
            "Epoch 2/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 1.0254 - accuracy: 0.5867\n",
            "Epoch 3/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 1.0017 - accuracy: 0.5857\n",
            "Epoch 4/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9954 - accuracy: 0.5869\n",
            "Epoch 5/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9904 - accuracy: 0.5865\n",
            "Epoch 6/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9857 - accuracy: 0.5867\n",
            "Epoch 7/40\n",
            "24/24 [==============================] - 6s 253ms/step - loss: 0.9800 - accuracy: 0.5868\n",
            "Epoch 8/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9761 - accuracy: 0.5871\n",
            "Epoch 9/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9745 - accuracy: 0.5870\n",
            "Epoch 10/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9703 - accuracy: 0.5869\n",
            "Epoch 11/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.9646 - accuracy: 0.5867\n",
            "Epoch 12/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9612 - accuracy: 0.5867\n",
            "Epoch 13/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9542 - accuracy: 0.5859\n",
            "Epoch 14/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.9482 - accuracy: 0.5877\n",
            "Epoch 15/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.9450 - accuracy: 0.5865\n",
            "Epoch 16/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9415 - accuracy: 0.5878\n",
            "Epoch 17/40\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.9340 - accuracy: 0.5878\n",
            "Epoch 18/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.9248 - accuracy: 0.5904\n",
            "Epoch 19/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.9139 - accuracy: 0.5932\n",
            "Epoch 20/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.9036 - accuracy: 0.5978\n",
            "Epoch 21/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.8903 - accuracy: 0.6057\n",
            "Epoch 22/40\n",
            "24/24 [==============================] - 6s 255ms/step - loss: 0.8854 - accuracy: 0.6053\n",
            "Epoch 23/40\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.8718 - accuracy: 0.6143\n",
            "Epoch 24/40\n",
            "24/24 [==============================] - 6s 258ms/step - loss: 0.8663 - accuracy: 0.6191\n",
            "Epoch 25/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.8533 - accuracy: 0.6252\n",
            "Epoch 26/40\n",
            "24/24 [==============================] - 6s 243ms/step - loss: 0.8442 - accuracy: 0.6299\n",
            "Epoch 27/40\n",
            "24/24 [==============================] - 6s 240ms/step - loss: 0.8339 - accuracy: 0.6373\n",
            "Epoch 28/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.8279 - accuracy: 0.6392\n",
            "Epoch 29/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.8181 - accuracy: 0.6433\n",
            "Epoch 30/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.8151 - accuracy: 0.6436\n",
            "Epoch 31/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.8097 - accuracy: 0.6524\n",
            "Epoch 32/40\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.7909 - accuracy: 0.6609\n",
            "Epoch 33/40\n",
            "24/24 [==============================] - 6s 251ms/step - loss: 0.7798 - accuracy: 0.6650\n",
            "Epoch 34/40\n",
            "24/24 [==============================] - 6s 249ms/step - loss: 0.7697 - accuracy: 0.6743\n",
            "Epoch 35/40\n",
            "24/24 [==============================] - 6s 248ms/step - loss: 0.7600 - accuracy: 0.6808\n",
            "Epoch 36/40\n",
            "24/24 [==============================] - 6s 244ms/step - loss: 0.7534 - accuracy: 0.6827\n",
            "Epoch 37/40\n",
            "24/24 [==============================] - 6s 241ms/step - loss: 0.7658 - accuracy: 0.6816\n",
            "Epoch 38/40\n",
            "24/24 [==============================] - 6s 246ms/step - loss: 0.7391 - accuracy: 0.6924\n",
            "Epoch 39/40\n",
            "24/24 [==============================] - 6s 250ms/step - loss: 0.7183 - accuracy: 0.7038\n",
            "Epoch 40/40\n",
            "24/24 [==============================] - 6s 247ms/step - loss: 0.7026 - accuracy: 0.7107\n",
            "24/24 [==============================] - 6s 245ms/step - loss: 0.7233 - accuracy: 0.7009\n",
            "[0.7233056426048279, 0.7009166479110718]\n",
            "SG_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZqN_SaM3hs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e2a5e4-2f25-4ea6-f910-3d1614b735b3"
      },
      "source": [
        "print(\"\\tLstm\\t\\t\\tBiLstm\\t\\t\\tRnn\")\r\n",
        "print(\"sg\",result_table[:3])\r\n",
        "print(\"cbow\",result_table[3:6])\r\n",
        "print(\"glove\",result_table[6:9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLstm\t\t\tBiLstm\t\t\tRnn\n",
            "sg [0.7118333578109741, 0.7118333578109741, 0.6471666693687439]\n",
            "cbow [0.7007499933242798, 0.7209166884422302, 0.5951666831970215]\n",
            "glove [0.7717499732971191, 0.859333336353302, 0.6794999837875366]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw2y3rm1f_l_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a2a8e9c5-4250-4aef-ff82-f53119522312"
      },
      "source": [
        "plt.title(\"Skip-Gram + Bi-LSTM accuracy and loss vs epochs Graph\")\r\n",
        "plt.plot(history_sg_bi.history['loss'],c='b',label='loss')\r\n",
        "plt.plot(history_sg_bi.history['accuracy'],c='r',label='accuracy')\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "# plt.yticks()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLVUTMaIFdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "ea2b1dc1-d576-4715-bca2-49a7e40f1957"
      },
      "source": [
        "print(result_table[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-25e0a4fc35b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}